PEMS04
Trainset:	x-(10181, 12, 307, 2)	y-(10181, 12, 307, 1)
Valset:  	x-(3394, 12, 307, 2)  	y-(3394, 12, 307, 1)
Testset:	x-(3394, 12, 307, 2)	y-(3394, 12, 307, 1)

--------- GWNET ---------
{
    "num_nodes": 307,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        175
    ],
    "early_stop": 15,
    "clip_grad": false,
    "batch_size": 64,
    "max_epochs": 500,
    "use_cl": true,
    "cl_step_size": 2500,
    "pass_device": true,
    "model_args": {
        "num_nodes": 307,
        "in_dim": 2,
        "out_dim": 12,
        "adj_path": "../data/PEMS04/adj_PEMS04_distance.pkl",
        "adj_type": "doubletransition",
        "device": "cuda:0",
        "dropout": 0.3,
        "gcn_bool": true,
        "addaptadj": true,
        "aptinit": null
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
GWNET                                    --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─ModuleList: 1-3                        --                        --
├─ModuleList: 1-4                        --                        --
├─ModuleList: 1-5                        --                        --
├─ModuleList: 1-6                        --                        --
├─Conv2d: 1-7                            [64, 32, 307, 13]         96
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-1                       [64, 32, 307, 12]         2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-2                       [64, 32, 307, 12]         2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-3                       [64, 256, 307, 12]        8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-4                          [64, 32, 307, 12]         --
│    │    └─nconv: 3-1                   [64, 32, 307, 12]         --
│    │    └─nconv: 3-2                   [64, 32, 307, 12]         --
│    │    └─nconv: 3-3                   [64, 32, 307, 12]         --
│    │    └─nconv: 3-4                   [64, 32, 307, 12]         --
│    │    └─nconv: 3-5                   [64, 32, 307, 12]         --
│    │    └─nconv: 3-6                   [64, 32, 307, 12]         --
│    │    └─linear: 3-7                  [64, 32, 307, 12]         7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-5                  [64, 32, 307, 12]         64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-6                       [64, 32, 307, 10]         2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-7                       [64, 32, 307, 10]         2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-8                       [64, 256, 307, 10]        8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-9                          [64, 32, 307, 10]         --
│    │    └─nconv: 3-8                   [64, 32, 307, 10]         --
│    │    └─nconv: 3-9                   [64, 32, 307, 10]         --
│    │    └─nconv: 3-10                  [64, 32, 307, 10]         --
│    │    └─nconv: 3-11                  [64, 32, 307, 10]         --
│    │    └─nconv: 3-12                  [64, 32, 307, 10]         --
│    │    └─nconv: 3-13                  [64, 32, 307, 10]         --
│    │    └─linear: 3-14                 [64, 32, 307, 10]         7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-10                 [64, 32, 307, 10]         64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-11                      [64, 32, 307, 9]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-12                      [64, 32, 307, 9]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-13                      [64, 256, 307, 9]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-14                         [64, 32, 307, 9]          --
│    │    └─nconv: 3-15                  [64, 32, 307, 9]          --
│    │    └─nconv: 3-16                  [64, 32, 307, 9]          --
│    │    └─nconv: 3-17                  [64, 32, 307, 9]          --
│    │    └─nconv: 3-18                  [64, 32, 307, 9]          --
│    │    └─nconv: 3-19                  [64, 32, 307, 9]          --
│    │    └─nconv: 3-20                  [64, 32, 307, 9]          --
│    │    └─linear: 3-21                 [64, 32, 307, 9]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-15                 [64, 32, 307, 9]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-16                      [64, 32, 307, 7]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-17                      [64, 32, 307, 7]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-18                      [64, 256, 307, 7]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-19                         [64, 32, 307, 7]          --
│    │    └─nconv: 3-22                  [64, 32, 307, 7]          --
│    │    └─nconv: 3-23                  [64, 32, 307, 7]          --
│    │    └─nconv: 3-24                  [64, 32, 307, 7]          --
│    │    └─nconv: 3-25                  [64, 32, 307, 7]          --
│    │    └─nconv: 3-26                  [64, 32, 307, 7]          --
│    │    └─nconv: 3-27                  [64, 32, 307, 7]          --
│    │    └─linear: 3-28                 [64, 32, 307, 7]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-20                 [64, 32, 307, 7]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-21                      [64, 32, 307, 6]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-22                      [64, 32, 307, 6]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-23                      [64, 256, 307, 6]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-24                         [64, 32, 307, 6]          --
│    │    └─nconv: 3-29                  [64, 32, 307, 6]          --
│    │    └─nconv: 3-30                  [64, 32, 307, 6]          --
│    │    └─nconv: 3-31                  [64, 32, 307, 6]          --
│    │    └─nconv: 3-32                  [64, 32, 307, 6]          --
│    │    └─nconv: 3-33                  [64, 32, 307, 6]          --
│    │    └─nconv: 3-34                  [64, 32, 307, 6]          --
│    │    └─linear: 3-35                 [64, 32, 307, 6]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-25                 [64, 32, 307, 6]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-26                      [64, 32, 307, 4]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-27                      [64, 32, 307, 4]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-28                      [64, 256, 307, 4]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-29                         [64, 32, 307, 4]          --
│    │    └─nconv: 3-36                  [64, 32, 307, 4]          --
│    │    └─nconv: 3-37                  [64, 32, 307, 4]          --
│    │    └─nconv: 3-38                  [64, 32, 307, 4]          --
│    │    └─nconv: 3-39                  [64, 32, 307, 4]          --
│    │    └─nconv: 3-40                  [64, 32, 307, 4]          --
│    │    └─nconv: 3-41                  [64, 32, 307, 4]          --
│    │    └─linear: 3-42                 [64, 32, 307, 4]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-30                 [64, 32, 307, 4]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-31                      [64, 32, 307, 3]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-32                      [64, 32, 307, 3]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-33                      [64, 256, 307, 3]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-34                         [64, 32, 307, 3]          --
│    │    └─nconv: 3-43                  [64, 32, 307, 3]          --
│    │    └─nconv: 3-44                  [64, 32, 307, 3]          --
│    │    └─nconv: 3-45                  [64, 32, 307, 3]          --
│    │    └─nconv: 3-46                  [64, 32, 307, 3]          --
│    │    └─nconv: 3-47                  [64, 32, 307, 3]          --
│    │    └─nconv: 3-48                  [64, 32, 307, 3]          --
│    │    └─linear: 3-49                 [64, 32, 307, 3]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-35                 [64, 32, 307, 3]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-36                      [64, 32, 307, 1]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-37                      [64, 32, 307, 1]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-38                      [64, 256, 307, 1]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-39                         [64, 32, 307, 1]          --
│    │    └─nconv: 3-50                  [64, 32, 307, 1]          --
│    │    └─nconv: 3-51                  [64, 32, 307, 1]          --
│    │    └─nconv: 3-52                  [64, 32, 307, 1]          --
│    │    └─nconv: 3-53                  [64, 32, 307, 1]          --
│    │    └─nconv: 3-54                  [64, 32, 307, 1]          --
│    │    └─nconv: 3-55                  [64, 32, 307, 1]          --
│    │    └─linear: 3-56                 [64, 32, 307, 1]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-40                 [64, 32, 307, 1]          64
├─Conv2d: 1-8                            [64, 512, 307, 1]         131,584
├─Conv2d: 1-9                            [64, 12, 307, 1]          6,156
==========================================================================================
Total params: 296,812
Trainable params: 296,812
Non-trainable params: 0
Total mult-adds (G): 22.97
==========================================================================================
Input size (MB): 1.89
Forward/backward pass size (MB): 3286.40
Params size (MB): 1.19
Estimated Total Size (MB): 3289.48
==========================================================================================

Loss: HuberLoss

CL target length = 1
2023-04-08 21:44:35.935330 Epoch 1  	Train Loss = 24.52359 Val Loss = 123.75906
2023-04-08 21:45:02.828017 Epoch 2  	Train Loss = 19.57518 Val Loss = 123.41991
2023-04-08 21:45:29.864433 Epoch 3  	Train Loss = 18.35551 Val Loss = 123.41604
2023-04-08 21:45:57.013150 Epoch 4  	Train Loss = 17.66025 Val Loss = 123.30742
2023-04-08 21:46:24.243191 Epoch 5  	Train Loss = 17.36121 Val Loss = 123.34683
2023-04-08 21:46:51.534459 Epoch 6  	Train Loss = 17.35729 Val Loss = 123.30714
2023-04-08 21:47:18.944182 Epoch 7  	Train Loss = 17.16812 Val Loss = 123.29692
2023-04-08 21:47:46.435336 Epoch 8  	Train Loss = 17.07895 Val Loss = 123.28217
2023-04-08 21:48:13.966282 Epoch 9  	Train Loss = 16.95908 Val Loss = 123.31013
2023-04-08 21:48:41.561865 Epoch 10  	Train Loss = 16.99244 Val Loss = 123.33320
2023-04-08 21:49:09.184190 Epoch 11  	Train Loss = 16.84854 Val Loss = 123.43505
2023-04-08 21:49:36.779955 Epoch 12  	Train Loss = 16.90196 Val Loss = 123.27058
2023-04-08 21:50:04.398339 Epoch 13  	Train Loss = 16.78475 Val Loss = 123.26548
2023-04-08 21:50:32.023454 Epoch 14  	Train Loss = 16.70387 Val Loss = 123.27411
2023-04-08 21:50:59.648600 Epoch 15  	Train Loss = 16.70058 Val Loss = 123.43490
CL target length = 2
2023-04-08 21:51:27.298540 Epoch 16  	Train Loss = 18.92560 Val Loss = 113.74557
2023-04-08 21:51:54.966113 Epoch 17  	Train Loss = 17.36594 Val Loss = 113.75878
2023-04-08 21:52:22.629847 Epoch 18  	Train Loss = 17.35883 Val Loss = 113.88632
2023-04-08 21:52:50.310902 Epoch 19  	Train Loss = 17.28309 Val Loss = 113.70993
2023-04-08 21:53:18.045688 Epoch 20  	Train Loss = 17.20191 Val Loss = 113.65815
2023-04-08 21:53:45.768297 Epoch 21  	Train Loss = 17.08821 Val Loss = 113.70160
2023-04-08 21:54:13.496761 Epoch 22  	Train Loss = 17.05693 Val Loss = 113.68251
2023-04-08 21:54:41.244760 Epoch 23  	Train Loss = 17.04756 Val Loss = 113.78796
2023-04-08 21:55:08.996525 Epoch 24  	Train Loss = 17.05764 Val Loss = 113.77337
2023-04-08 21:55:36.769440 Epoch 25  	Train Loss = 16.98474 Val Loss = 113.74282
2023-04-08 21:56:04.546449 Epoch 26  	Train Loss = 17.01367 Val Loss = 113.79660
2023-04-08 21:56:32.328491 Epoch 27  	Train Loss = 16.98473 Val Loss = 113.61789
2023-04-08 21:57:00.136612 Epoch 28  	Train Loss = 16.89652 Val Loss = 113.67418
2023-04-08 21:57:27.962056 Epoch 29  	Train Loss = 16.88855 Val Loss = 113.61986
2023-04-08 21:57:55.780944 Epoch 30  	Train Loss = 16.82589 Val Loss = 113.64688
2023-04-08 21:58:23.679458 Epoch 31  	Train Loss = 16.75027 Val Loss = 113.68681
CL target length = 3
2023-04-08 21:58:51.584928 Epoch 32  	Train Loss = 18.63152 Val Loss = 104.09209
2023-04-08 21:59:19.435029 Epoch 33  	Train Loss = 17.32430 Val Loss = 104.49865
2023-04-08 21:59:47.167826 Epoch 34  	Train Loss = 17.35286 Val Loss = 104.07069
2023-04-08 22:00:14.822567 Epoch 35  	Train Loss = 17.23810 Val Loss = 104.06096
2023-04-08 22:00:42.459635 Epoch 36  	Train Loss = 17.29981 Val Loss = 104.14054
2023-04-08 22:01:10.115232 Epoch 37  	Train Loss = 17.19574 Val Loss = 104.23184
2023-04-08 22:01:37.775775 Epoch 38  	Train Loss = 17.12473 Val Loss = 104.07916
2023-04-08 22:02:05.414448 Epoch 39  	Train Loss = 17.10978 Val Loss = 104.19834
2023-04-08 22:02:33.060814 Epoch 40  	Train Loss = 17.18251 Val Loss = 104.04184
2023-04-08 22:03:00.692596 Epoch 41  	Train Loss = 17.11221 Val Loss = 104.31049
2023-04-08 22:03:28.332979 Epoch 42  	Train Loss = 17.03110 Val Loss = 104.39380
2023-04-08 22:03:55.993276 Epoch 43  	Train Loss = 17.08313 Val Loss = 104.11644
2023-04-08 22:04:23.645026 Epoch 44  	Train Loss = 16.96702 Val Loss = 104.01435
2023-04-08 22:04:51.307227 Epoch 45  	Train Loss = 16.94167 Val Loss = 104.06842
2023-04-08 22:05:18.977314 Epoch 46  	Train Loss = 16.94423 Val Loss = 104.25386
CL target length = 4
2023-04-08 22:05:46.648827 Epoch 47  	Train Loss = 18.02469 Val Loss = 95.39540
2023-04-08 22:06:14.333894 Epoch 48  	Train Loss = 17.50214 Val Loss = 94.62451
2023-04-08 22:06:42.088259 Epoch 49  	Train Loss = 17.33506 Val Loss = 94.46760
2023-04-08 22:07:09.813875 Epoch 50  	Train Loss = 17.26327 Val Loss = 94.84205
2023-04-08 22:07:37.542969 Epoch 51  	Train Loss = 17.21059 Val Loss = 94.48853
2023-04-08 22:08:05.279250 Epoch 52  	Train Loss = 17.16985 Val Loss = 94.50934
2023-04-08 22:08:33.017722 Epoch 53  	Train Loss = 17.18451 Val Loss = 94.63702
2023-04-08 22:09:00.791795 Epoch 54  	Train Loss = 17.24515 Val Loss = 94.46390
2023-04-08 22:09:28.581882 Epoch 55  	Train Loss = 17.13010 Val Loss = 94.66394
2023-04-08 22:09:56.398929 Epoch 56  	Train Loss = 17.07520 Val Loss = 94.79245
2023-04-08 22:10:24.220219 Epoch 57  	Train Loss = 17.05107 Val Loss = 94.44836
2023-04-08 22:10:52.044763 Epoch 58  	Train Loss = 16.99542 Val Loss = 94.53007
2023-04-08 22:11:19.878546 Epoch 59  	Train Loss = 17.06046 Val Loss = 94.48518
2023-04-08 22:11:47.706670 Epoch 60  	Train Loss = 16.99928 Val Loss = 94.64103
2023-04-08 22:12:15.544629 Epoch 61  	Train Loss = 16.99769 Val Loss = 94.75204
2023-04-08 22:12:43.374689 Epoch 62  	Train Loss = 17.05012 Val Loss = 94.36974
CL target length = 5
2023-04-08 22:13:11.197992 Epoch 63  	Train Loss = 18.08398 Val Loss = 85.07429
2023-04-08 22:13:39.000128 Epoch 64  	Train Loss = 17.28984 Val Loss = 84.97008
2023-04-08 22:14:06.720987 Epoch 65  	Train Loss = 17.26862 Val Loss = 84.90930
2023-04-08 22:14:34.399919 Epoch 66  	Train Loss = 17.19925 Val Loss = 84.95493
2023-04-08 22:15:02.017738 Epoch 67  	Train Loss = 17.16164 Val Loss = 85.21678
2023-04-08 22:15:29.582277 Epoch 68  	Train Loss = 17.21761 Val Loss = 84.85791
2023-04-08 22:15:57.175742 Epoch 69  	Train Loss = 17.13706 Val Loss = 85.67829
2023-04-08 22:16:24.797956 Epoch 70  	Train Loss = 17.18327 Val Loss = 84.96202
2023-04-08 22:16:52.417703 Epoch 71  	Train Loss = 17.02282 Val Loss = 85.16142
2023-04-08 22:17:20.040698 Epoch 72  	Train Loss = 17.11537 Val Loss = 85.04758
2023-04-08 22:17:47.670374 Epoch 73  	Train Loss = 17.09417 Val Loss = 84.92419
2023-04-08 22:18:15.319109 Epoch 74  	Train Loss = 17.05820 Val Loss = 84.98515
2023-04-08 22:18:42.946572 Epoch 75  	Train Loss = 16.97586 Val Loss = 84.84136
2023-04-08 22:19:10.593538 Epoch 76  	Train Loss = 16.95753 Val Loss = 84.82626
2023-04-08 22:19:38.260480 Epoch 77  	Train Loss = 17.04035 Val Loss = 84.91711
2023-04-08 22:20:05.935051 Epoch 78  	Train Loss = 16.94445 Val Loss = 85.06826
CL target length = 6
2023-04-08 22:20:33.621160 Epoch 79  	Train Loss = 18.02697 Val Loss = 75.56486
2023-04-08 22:21:01.328726 Epoch 80  	Train Loss = 17.27794 Val Loss = 75.26706
2023-04-08 22:21:29.050786 Epoch 81  	Train Loss = 17.19498 Val Loss = 75.46657
2023-04-08 22:21:56.782353 Epoch 82  	Train Loss = 17.20034 Val Loss = 75.44595
2023-04-08 22:22:24.519687 Epoch 83  	Train Loss = 17.23868 Val Loss = 75.35734
2023-04-08 22:22:52.246874 Epoch 84  	Train Loss = 17.10575 Val Loss = 75.46794
2023-04-08 22:23:19.998052 Epoch 85  	Train Loss = 17.19110 Val Loss = 75.39484
2023-04-08 22:23:47.756992 Epoch 86  	Train Loss = 17.09278 Val Loss = 75.28975
2023-04-08 22:24:15.516442 Epoch 87  	Train Loss = 17.03813 Val Loss = 75.44166
2023-04-08 22:24:43.258230 Epoch 88  	Train Loss = 17.12572 Val Loss = 75.41306
2023-04-08 22:25:11.006341 Epoch 89  	Train Loss = 17.01989 Val Loss = 76.18320
2023-04-08 22:25:38.748530 Epoch 90  	Train Loss = 17.09597 Val Loss = 75.31008
2023-04-08 22:26:06.474371 Epoch 91  	Train Loss = 17.03184 Val Loss = 75.19803
2023-04-08 22:26:34.204143 Epoch 92  	Train Loss = 17.02082 Val Loss = 75.34678
2023-04-08 22:27:01.918330 Epoch 93  	Train Loss = 16.97033 Val Loss = 75.43821
CL target length = 7
2023-04-08 22:27:29.635391 Epoch 94  	Train Loss = 17.72128 Val Loss = 66.34020
2023-04-08 22:27:57.335915 Epoch 95  	Train Loss = 17.25654 Val Loss = 65.91581
2023-04-08 22:28:25.037296 Epoch 96  	Train Loss = 17.21101 Val Loss = 65.97474
2023-04-08 22:28:52.720940 Epoch 97  	Train Loss = 17.17466 Val Loss = 65.85541
2023-04-08 22:29:20.391774 Epoch 98  	Train Loss = 17.19338 Val Loss = 66.39178
2023-04-08 22:29:48.060064 Epoch 99  	Train Loss = 17.18522 Val Loss = 65.72985
2023-04-08 22:30:15.736598 Epoch 100  	Train Loss = 17.12595 Val Loss = 65.80374
2023-04-08 22:30:43.405610 Epoch 101  	Train Loss = 17.09946 Val Loss = 65.96416
2023-04-08 22:31:11.059339 Epoch 102  	Train Loss = 17.13458 Val Loss = 65.88270
2023-04-08 22:31:38.714776 Epoch 103  	Train Loss = 17.05681 Val Loss = 66.84912
2023-04-08 22:32:06.403528 Epoch 104  	Train Loss = 17.12024 Val Loss = 65.93174
2023-04-08 22:32:34.073561 Epoch 105  	Train Loss = 17.04712 Val Loss = 65.69300
2023-04-08 22:33:01.794836 Epoch 106  	Train Loss = 17.05450 Val Loss = 66.06786
2023-04-08 22:33:29.525330 Epoch 107  	Train Loss = 17.13172 Val Loss = 66.26954
2023-04-08 22:33:57.275331 Epoch 108  	Train Loss = 17.05168 Val Loss = 66.04441
2023-04-08 22:34:25.008325 Epoch 109  	Train Loss = 17.15248 Val Loss = 66.03751
CL target length = 8
2023-04-08 22:34:52.766894 Epoch 110  	Train Loss = 17.76692 Val Loss = 56.24099
2023-04-08 22:35:20.540768 Epoch 111  	Train Loss = 17.21930 Val Loss = 56.49824
2023-04-08 22:35:48.307995 Epoch 112  	Train Loss = 17.20040 Val Loss = 56.26908
2023-04-08 22:36:16.078788 Epoch 113  	Train Loss = 17.18205 Val Loss = 57.22666
2023-04-08 22:36:43.860408 Epoch 114  	Train Loss = 17.32251 Val Loss = 57.95461
2023-04-08 22:37:11.640247 Epoch 115  	Train Loss = 17.21548 Val Loss = 56.23669
2023-04-08 22:37:39.395979 Epoch 116  	Train Loss = 17.07266 Val Loss = 56.16778
2023-04-08 22:38:07.136412 Epoch 117  	Train Loss = 17.16852 Val Loss = 56.15304
2023-04-08 22:38:34.853593 Epoch 118  	Train Loss = 17.07085 Val Loss = 56.23499
2023-04-08 22:39:02.590793 Epoch 119  	Train Loss = 17.12881 Val Loss = 56.51247
2023-04-08 22:39:30.338078 Epoch 120  	Train Loss = 17.12725 Val Loss = 56.17792
2023-04-08 22:39:58.143335 Epoch 121  	Train Loss = 17.11721 Val Loss = 56.34191
2023-04-08 22:40:25.953817 Epoch 122  	Train Loss = 17.11047 Val Loss = 56.28666
2023-04-08 22:40:53.715160 Epoch 123  	Train Loss = 17.09768 Val Loss = 57.27093
2023-04-08 22:41:21.480975 Epoch 124  	Train Loss = 17.18215 Val Loss = 56.21187
CL target length = 9
2023-04-08 22:41:49.247400 Epoch 125  	Train Loss = 17.14187 Val Loss = 55.32951
2023-04-08 22:42:16.999139 Epoch 126  	Train Loss = 17.70443 Val Loss = 46.91771
2023-04-08 22:42:44.740236 Epoch 127  	Train Loss = 17.28602 Val Loss = 47.66120
2023-04-08 22:43:12.477185 Epoch 128  	Train Loss = 17.35889 Val Loss = 46.63378
2023-04-08 22:43:40.210387 Epoch 129  	Train Loss = 17.22417 Val Loss = 46.74767
2023-04-08 22:44:07.941652 Epoch 130  	Train Loss = 17.21476 Val Loss = 46.95214
2023-04-08 22:44:35.673797 Epoch 131  	Train Loss = 17.21781 Val Loss = 46.72591
2023-04-08 22:45:03.388602 Epoch 132  	Train Loss = 17.20572 Val Loss = 46.90419
2023-04-08 22:45:31.089259 Epoch 133  	Train Loss = 17.16763 Val Loss = 47.08817
2023-04-08 22:45:58.784540 Epoch 134  	Train Loss = 17.12135 Val Loss = 48.07593
2023-04-08 22:46:26.506513 Epoch 135  	Train Loss = 17.19852 Val Loss = 46.87261
2023-04-08 22:46:54.219485 Epoch 136  	Train Loss = 17.13320 Val Loss = 46.93372
2023-04-08 22:47:21.929263 Epoch 137  	Train Loss = 17.11918 Val Loss = 46.70669
2023-04-08 22:47:49.650692 Epoch 138  	Train Loss = 17.08607 Val Loss = 47.55187
2023-04-08 22:48:17.380812 Epoch 139  	Train Loss = 17.16275 Val Loss = 47.27264
2023-04-08 22:48:45.111322 Epoch 140  	Train Loss = 17.14050 Val Loss = 46.79048
CL target length = 10
2023-04-08 22:49:12.878954 Epoch 141  	Train Loss = 17.71200 Val Loss = 37.32923
2023-04-08 22:49:40.646829 Epoch 142  	Train Loss = 17.34006 Val Loss = 37.44394
2023-04-08 22:50:08.431976 Epoch 143  	Train Loss = 17.32735 Val Loss = 37.31430
2023-04-08 22:50:36.215410 Epoch 144  	Train Loss = 17.28830 Val Loss = 37.35728
2023-04-08 22:51:03.999486 Epoch 145  	Train Loss = 17.22316 Val Loss = 37.08277
2023-04-08 22:51:31.777598 Epoch 146  	Train Loss = 17.25729 Val Loss = 37.46239
2023-04-08 22:51:59.538049 Epoch 147  	Train Loss = 17.24020 Val Loss = 37.70593
2023-04-08 22:52:27.260775 Epoch 148  	Train Loss = 17.16856 Val Loss = 37.38181
2023-04-08 22:52:54.935407 Epoch 149  	Train Loss = 17.19693 Val Loss = 37.11377
2023-04-08 22:53:22.538277 Epoch 150  	Train Loss = 17.21846 Val Loss = 37.74712
2023-04-08 22:53:50.099388 Epoch 151  	Train Loss = 17.29223 Val Loss = 37.73150
2023-04-08 22:54:17.659361 Epoch 152  	Train Loss = 17.23200 Val Loss = 37.21256
2023-04-08 22:54:45.204130 Epoch 153  	Train Loss = 17.15497 Val Loss = 37.10102
2023-04-08 22:55:12.763993 Epoch 154  	Train Loss = 17.12161 Val Loss = 37.13695
2023-04-08 22:55:40.339544 Epoch 155  	Train Loss = 17.19789 Val Loss = 37.54659
2023-04-08 22:56:07.935885 Epoch 156  	Train Loss = 17.11853 Val Loss = 37.43856
CL target length = 11
2023-04-08 22:56:35.514463 Epoch 157  	Train Loss = 17.78727 Val Loss = 28.23219
2023-04-08 22:57:03.059459 Epoch 158  	Train Loss = 17.35666 Val Loss = 27.85397
2023-04-08 22:57:30.624403 Epoch 159  	Train Loss = 17.28897 Val Loss = 27.82607
2023-04-08 22:57:58.192206 Epoch 160  	Train Loss = 17.25033 Val Loss = 28.86813
2023-04-08 22:58:25.768560 Epoch 161  	Train Loss = 17.43567 Val Loss = 27.58677
2023-04-08 22:58:53.348670 Epoch 162  	Train Loss = 17.24596 Val Loss = 28.43930
2023-04-08 22:59:20.934845 Epoch 163  	Train Loss = 17.30090 Val Loss = 28.50389
2023-04-08 22:59:48.530557 Epoch 164  	Train Loss = 17.27148 Val Loss = 28.18955
2023-04-08 23:00:16.158143 Epoch 165  	Train Loss = 17.23716 Val Loss = 27.76447
2023-04-08 23:00:43.800499 Epoch 166  	Train Loss = 17.21969 Val Loss = 28.62339
2023-04-08 23:01:11.477824 Epoch 167  	Train Loss = 17.24622 Val Loss = 27.54962
2023-04-08 23:01:39.167969 Epoch 168  	Train Loss = 17.19266 Val Loss = 27.78786
2023-04-08 23:02:06.863626 Epoch 169  	Train Loss = 17.20310 Val Loss = 28.49906
2023-04-08 23:02:34.559827 Epoch 170  	Train Loss = 17.20370 Val Loss = 27.96982
2023-04-08 23:03:02.292766 Epoch 171  	Train Loss = 17.21837 Val Loss = 28.44021
CL target length = 12
2023-04-08 23:03:30.061735 Epoch 172  	Train Loss = 17.55736 Val Loss = 18.84178
2023-04-08 23:03:57.798737 Epoch 173  	Train Loss = 17.38601 Val Loss = 18.34211
2023-04-08 23:04:25.559534 Epoch 174  	Train Loss = 17.30618 Val Loss = 18.26666
2023-04-08 23:04:53.309796 Epoch 175  	Train Loss = 17.27255 Val Loss = 18.10854
2023-04-08 23:05:21.097020 Epoch 176  	Train Loss = 17.01994 Val Loss = 17.83215
2023-04-08 23:05:48.830406 Epoch 177  	Train Loss = 16.96208 Val Loss = 17.88557
2023-04-08 23:06:16.568708 Epoch 178  	Train Loss = 16.94981 Val Loss = 17.87219
2023-04-08 23:06:44.285061 Epoch 179  	Train Loss = 16.94863 Val Loss = 17.85871
2023-04-08 23:07:11.970310 Epoch 180  	Train Loss = 16.96555 Val Loss = 17.86454
2023-04-08 23:07:39.598480 Epoch 181  	Train Loss = 16.96693 Val Loss = 17.85742
2023-04-08 23:08:07.183346 Epoch 182  	Train Loss = 16.94763 Val Loss = 17.87436
2023-04-08 23:08:34.747049 Epoch 183  	Train Loss = 16.92419 Val Loss = 17.93894
2023-04-08 23:09:02.318992 Epoch 184  	Train Loss = 16.91690 Val Loss = 17.91899
2023-04-08 23:09:29.903769 Epoch 185  	Train Loss = 16.93814 Val Loss = 17.87137
2023-04-08 23:09:57.511180 Epoch 186  	Train Loss = 16.96798 Val Loss = 17.91665
2023-04-08 23:10:25.114183 Epoch 187  	Train Loss = 16.92500 Val Loss = 17.85037
2023-04-08 23:10:52.733832 Epoch 188  	Train Loss = 16.91365 Val Loss = 17.86093
2023-04-08 23:11:20.343199 Epoch 189  	Train Loss = 16.94280 Val Loss = 17.91228
2023-04-08 23:11:47.960300 Epoch 190  	Train Loss = 16.93033 Val Loss = 17.87730
2023-04-08 23:12:15.587563 Epoch 191  	Train Loss = 16.94635 Val Loss = 17.86822
Early stopping at epoch: 191
Best at epoch 176:
Train Loss = 17.01994
Train RMSE = 28.27780, MAE = 17.20699, MAPE = 12.44835
Val Loss = 17.83215
Val RMSE = 30.79375, MAE = 18.71157, MAPE = 12.24233
--------- Test ---------
All Steps RMSE = 30.23253, MAE = 18.61584, MAPE = 12.28831
Step 1 RMSE = 26.83956, MAE = 16.53434, MAPE = 10.99141
Step 2 RMSE = 27.89588, MAE = 17.19868, MAPE = 11.40835
Step 3 RMSE = 28.70173, MAE = 17.69092, MAPE = 11.72885
Step 4 RMSE = 29.28742, MAE = 18.03196, MAPE = 11.92877
Step 5 RMSE = 29.83415, MAE = 18.34208, MAPE = 12.12163
Step 6 RMSE = 30.27431, MAE = 18.61890, MAPE = 12.21836
Step 7 RMSE = 30.68819, MAE = 18.87976, MAPE = 12.39271
Step 8 RMSE = 31.03963, MAE = 19.11466, MAPE = 12.54909
Step 9 RMSE = 31.37802, MAE = 19.33957, MAPE = 12.70976
Step 10 RMSE = 31.70057, MAE = 19.57661, MAPE = 12.90136
Step 11 RMSE = 32.06462, MAE = 19.85373, MAPE = 13.10740
Step 12 RMSE = 32.53416, MAE = 20.20860, MAPE = 13.40187
Inference time: 2.57 s
