METRLA
Trainset:	x-(23974, 12, 207, 2)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 2)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 2)	y-(6850, 12, 207, 1)

--------- GWNET ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        80
    ],
    "clip_grad": false,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": true,
    "cl_step_size": 2500,
    "pass_device": true,
    "model_args": {
        "num_nodes": 207,
        "in_dim": 2,
        "out_dim": 12,
        "adj_path": "../data/METRLA/adj_mx.pkl",
        "adj_type": "doubletransition",
        "device": "cuda:0",
        "dropout": 0.3,
        "gcn_bool": true,
        "addaptadj": true,
        "aptinit": null
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
GWNET                                    --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─ModuleList: 1-3                        --                        --
├─ModuleList: 1-4                        --                        --
├─ModuleList: 1-5                        --                        --
├─ModuleList: 1-6                        --                        --
├─Conv2d: 1-7                            [64, 32, 207, 13]         96
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-1                       [64, 32, 207, 12]         2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-2                       [64, 32, 207, 12]         2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-3                       [64, 256, 207, 12]        8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-4                          [64, 32, 207, 12]         --
│    │    └─nconv: 3-1                   [64, 32, 207, 12]         --
│    │    └─nconv: 3-2                   [64, 32, 207, 12]         --
│    │    └─nconv: 3-3                   [64, 32, 207, 12]         --
│    │    └─nconv: 3-4                   [64, 32, 207, 12]         --
│    │    └─nconv: 3-5                   [64, 32, 207, 12]         --
│    │    └─nconv: 3-6                   [64, 32, 207, 12]         --
│    │    └─linear: 3-7                  [64, 32, 207, 12]         7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-5                  [64, 32, 207, 12]         64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-6                       [64, 32, 207, 10]         2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-7                       [64, 32, 207, 10]         2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-8                       [64, 256, 207, 10]        8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-9                          [64, 32, 207, 10]         --
│    │    └─nconv: 3-8                   [64, 32, 207, 10]         --
│    │    └─nconv: 3-9                   [64, 32, 207, 10]         --
│    │    └─nconv: 3-10                  [64, 32, 207, 10]         --
│    │    └─nconv: 3-11                  [64, 32, 207, 10]         --
│    │    └─nconv: 3-12                  [64, 32, 207, 10]         --
│    │    └─nconv: 3-13                  [64, 32, 207, 10]         --
│    │    └─linear: 3-14                 [64, 32, 207, 10]         7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-10                 [64, 32, 207, 10]         64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-11                      [64, 32, 207, 9]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-12                      [64, 32, 207, 9]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-13                      [64, 256, 207, 9]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-14                         [64, 32, 207, 9]          --
│    │    └─nconv: 3-15                  [64, 32, 207, 9]          --
│    │    └─nconv: 3-16                  [64, 32, 207, 9]          --
│    │    └─nconv: 3-17                  [64, 32, 207, 9]          --
│    │    └─nconv: 3-18                  [64, 32, 207, 9]          --
│    │    └─nconv: 3-19                  [64, 32, 207, 9]          --
│    │    └─nconv: 3-20                  [64, 32, 207, 9]          --
│    │    └─linear: 3-21                 [64, 32, 207, 9]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-15                 [64, 32, 207, 9]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-16                      [64, 32, 207, 7]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-17                      [64, 32, 207, 7]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-18                      [64, 256, 207, 7]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-19                         [64, 32, 207, 7]          --
│    │    └─nconv: 3-22                  [64, 32, 207, 7]          --
│    │    └─nconv: 3-23                  [64, 32, 207, 7]          --
│    │    └─nconv: 3-24                  [64, 32, 207, 7]          --
│    │    └─nconv: 3-25                  [64, 32, 207, 7]          --
│    │    └─nconv: 3-26                  [64, 32, 207, 7]          --
│    │    └─nconv: 3-27                  [64, 32, 207, 7]          --
│    │    └─linear: 3-28                 [64, 32, 207, 7]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-20                 [64, 32, 207, 7]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-21                      [64, 32, 207, 6]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-22                      [64, 32, 207, 6]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-23                      [64, 256, 207, 6]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-24                         [64, 32, 207, 6]          --
│    │    └─nconv: 3-29                  [64, 32, 207, 6]          --
│    │    └─nconv: 3-30                  [64, 32, 207, 6]          --
│    │    └─nconv: 3-31                  [64, 32, 207, 6]          --
│    │    └─nconv: 3-32                  [64, 32, 207, 6]          --
│    │    └─nconv: 3-33                  [64, 32, 207, 6]          --
│    │    └─nconv: 3-34                  [64, 32, 207, 6]          --
│    │    └─linear: 3-35                 [64, 32, 207, 6]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-25                 [64, 32, 207, 6]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-26                      [64, 32, 207, 4]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-27                      [64, 32, 207, 4]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-28                      [64, 256, 207, 4]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-29                         [64, 32, 207, 4]          --
│    │    └─nconv: 3-36                  [64, 32, 207, 4]          --
│    │    └─nconv: 3-37                  [64, 32, 207, 4]          --
│    │    └─nconv: 3-38                  [64, 32, 207, 4]          --
│    │    └─nconv: 3-39                  [64, 32, 207, 4]          --
│    │    └─nconv: 3-40                  [64, 32, 207, 4]          --
│    │    └─nconv: 3-41                  [64, 32, 207, 4]          --
│    │    └─linear: 3-42                 [64, 32, 207, 4]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-30                 [64, 32, 207, 4]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-31                      [64, 32, 207, 3]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-32                      [64, 32, 207, 3]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-33                      [64, 256, 207, 3]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-34                         [64, 32, 207, 3]          --
│    │    └─nconv: 3-43                  [64, 32, 207, 3]          --
│    │    └─nconv: 3-44                  [64, 32, 207, 3]          --
│    │    └─nconv: 3-45                  [64, 32, 207, 3]          --
│    │    └─nconv: 3-46                  [64, 32, 207, 3]          --
│    │    └─nconv: 3-47                  [64, 32, 207, 3]          --
│    │    └─nconv: 3-48                  [64, 32, 207, 3]          --
│    │    └─linear: 3-49                 [64, 32, 207, 3]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-35                 [64, 32, 207, 3]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-36                      [64, 32, 207, 1]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-37                      [64, 32, 207, 1]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-38                      [64, 256, 207, 1]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-39                         [64, 32, 207, 1]          --
│    │    └─nconv: 3-50                  [64, 32, 207, 1]          --
│    │    └─nconv: 3-51                  [64, 32, 207, 1]          --
│    │    └─nconv: 3-52                  [64, 32, 207, 1]          --
│    │    └─nconv: 3-53                  [64, 32, 207, 1]          --
│    │    └─nconv: 3-54                  [64, 32, 207, 1]          --
│    │    └─nconv: 3-55                  [64, 32, 207, 1]          --
│    │    └─linear: 3-56                 [64, 32, 207, 1]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-40                 [64, 32, 207, 1]          64
├─Conv2d: 1-8                            [64, 512, 207, 1]         131,584
├─Conv2d: 1-9                            [64, 12, 207, 1]          6,156
==========================================================================================
Total params: 296,812
Trainable params: 296,812
Non-trainable params: 0
Total mult-adds (G): 15.49
==========================================================================================
Input size (MB): 1.27
Forward/backward pass size (MB): 2215.91
Params size (MB): 1.19
Estimated Total Size (MB): 2218.37
==========================================================================================

Loss: MaskedMAELoss

CL target length = 1
2023-04-08 19:52:27.974107 Epoch 1  	Train Loss = 2.82099 Val Loss = 10.61737
2023-04-08 19:53:06.762390 Epoch 2  	Train Loss = 2.42416 Val Loss = 10.60037
2023-04-08 19:53:45.689781 Epoch 3  	Train Loss = 2.33080 Val Loss = 10.59274
2023-04-08 19:54:25.301525 Epoch 4  	Train Loss = 2.30954 Val Loss = 10.59762
2023-04-08 19:55:05.407747 Epoch 5  	Train Loss = 2.27749 Val Loss = 10.58943
2023-04-08 19:55:45.833491 Epoch 6  	Train Loss = 2.25684 Val Loss = 10.58946
CL target length = 2
2023-04-08 19:56:26.400516 Epoch 7  	Train Loss = 2.36729 Val Loss = 9.84675
2023-04-08 19:57:07.018145 Epoch 8  	Train Loss = 2.41169 Val Loss = 9.84430
2023-04-08 19:57:47.613043 Epoch 9  	Train Loss = 2.40109 Val Loss = 9.84258
2023-04-08 19:58:28.354228 Epoch 10  	Train Loss = 2.39013 Val Loss = 9.84229
2023-04-08 19:59:09.173445 Epoch 11  	Train Loss = 2.38153 Val Loss = 9.84009
2023-04-08 19:59:50.394646 Epoch 12  	Train Loss = 2.37563 Val Loss = 9.83984
2023-04-08 20:00:31.813331 Epoch 13  	Train Loss = 2.36349 Val Loss = 9.83851
CL target length = 3
2023-04-08 20:01:12.976926 Epoch 14  	Train Loss = 2.49899 Val Loss = 9.11116
2023-04-08 20:01:53.702254 Epoch 15  	Train Loss = 2.48704 Val Loss = 9.10199
2023-04-08 20:02:34.288271 Epoch 16  	Train Loss = 2.47726 Val Loss = 9.10758
2023-04-08 20:03:14.864616 Epoch 17  	Train Loss = 2.46887 Val Loss = 9.10395
2023-04-08 20:03:55.403193 Epoch 18  	Train Loss = 2.46362 Val Loss = 9.10247
2023-04-08 20:04:35.924106 Epoch 19  	Train Loss = 2.45334 Val Loss = 9.09751
CL target length = 4
2023-04-08 20:05:16.409010 Epoch 20  	Train Loss = 2.45688 Val Loss = 9.02974
2023-04-08 20:05:56.828773 Epoch 21  	Train Loss = 2.58619 Val Loss = 8.37873
2023-04-08 20:06:37.218130 Epoch 22  	Train Loss = 2.54374 Val Loss = 8.37917
2023-04-08 20:07:27.378870 Epoch 23  	Train Loss = 2.53813 Val Loss = 8.37239
2023-04-08 20:08:07.670241 Epoch 24  	Train Loss = 2.53039 Val Loss = 8.36588
2023-04-08 20:08:47.931573 Epoch 25  	Train Loss = 2.52564 Val Loss = 8.36452
2023-04-08 20:09:28.242650 Epoch 26  	Train Loss = 2.51592 Val Loss = 8.36239
CL target length = 5
2023-04-08 20:10:08.635000 Epoch 27  	Train Loss = 2.57399 Val Loss = 7.65100
2023-04-08 20:10:49.031287 Epoch 28  	Train Loss = 2.59726 Val Loss = 7.64391
2023-04-08 20:11:29.554596 Epoch 29  	Train Loss = 2.59352 Val Loss = 7.64711
2023-04-08 20:12:10.279248 Epoch 30  	Train Loss = 2.58661 Val Loss = 7.64214
2023-04-08 20:12:51.349841 Epoch 31  	Train Loss = 2.58088 Val Loss = 7.63505
2023-04-08 20:13:32.727454 Epoch 32  	Train Loss = 2.57579 Val Loss = 7.63696
2023-04-08 20:14:14.546845 Epoch 33  	Train Loss = 2.56885 Val Loss = 7.63844
CL target length = 6
2023-04-08 20:14:56.694965 Epoch 34  	Train Loss = 2.64146 Val Loss = 6.92361
2023-04-08 20:15:39.039659 Epoch 35  	Train Loss = 2.63965 Val Loss = 6.93040
2023-04-08 20:16:21.357753 Epoch 36  	Train Loss = 2.63393 Val Loss = 6.93335
2023-04-08 20:17:02.899765 Epoch 37  	Train Loss = 2.62560 Val Loss = 6.92871
2023-04-08 20:17:43.745927 Epoch 38  	Train Loss = 2.62098 Val Loss = 6.93181
2023-04-08 20:18:24.370109 Epoch 39  	Train Loss = 2.61708 Val Loss = 6.93455
CL target length = 7
2023-04-08 20:19:04.978770 Epoch 40  	Train Loss = 2.61876 Val Loss = 6.87189
2023-04-08 20:19:45.625176 Epoch 41  	Train Loss = 2.70516 Val Loss = 6.21018
2023-04-08 20:20:26.255036 Epoch 42  	Train Loss = 2.67780 Val Loss = 6.21039
2023-04-08 20:21:06.853485 Epoch 43  	Train Loss = 2.66949 Val Loss = 6.20555
2023-04-08 20:21:47.454102 Epoch 44  	Train Loss = 2.66630 Val Loss = 6.20950
2023-04-08 20:22:28.060731 Epoch 45  	Train Loss = 2.66636 Val Loss = 6.21654
2023-04-08 20:23:08.658521 Epoch 46  	Train Loss = 2.65546 Val Loss = 6.20071
CL target length = 8
2023-04-08 20:23:49.248450 Epoch 47  	Train Loss = 2.70268 Val Loss = 5.52331
2023-04-08 20:24:29.824536 Epoch 48  	Train Loss = 2.71561 Val Loss = 5.50089
2023-04-08 20:25:10.323579 Epoch 49  	Train Loss = 2.71509 Val Loss = 5.50793
2023-04-08 20:25:50.874068 Epoch 50  	Train Loss = 2.70438 Val Loss = 5.52561
2023-04-08 20:26:31.529839 Epoch 51  	Train Loss = 2.70411 Val Loss = 5.52177
2023-04-08 20:27:12.311527 Epoch 52  	Train Loss = 2.69818 Val Loss = 5.49730
2023-04-08 20:27:53.477328 Epoch 53  	Train Loss = 2.69778 Val Loss = 5.50512
CL target length = 9
2023-04-08 20:28:35.283328 Epoch 54  	Train Loss = 2.75470 Val Loss = 4.80006
2023-04-08 20:29:17.367531 Epoch 55  	Train Loss = 2.75390 Val Loss = 4.81028
2023-04-08 20:29:59.632866 Epoch 56  	Train Loss = 2.74354 Val Loss = 4.82284
2023-04-08 20:30:42.154924 Epoch 57  	Train Loss = 2.73833 Val Loss = 4.81417
2023-04-08 20:31:24.319324 Epoch 58  	Train Loss = 2.73609 Val Loss = 4.80289
2023-04-08 20:32:05.565733 Epoch 59  	Train Loss = 2.73133 Val Loss = 4.84276
CL target length = 10
2023-04-08 20:32:46.227046 Epoch 60  	Train Loss = 2.73214 Val Loss = 4.75931
2023-04-08 20:33:26.752746 Epoch 61  	Train Loss = 2.79598 Val Loss = 4.10441
2023-04-08 20:34:07.250351 Epoch 62  	Train Loss = 2.77293 Val Loss = 4.09589
2023-04-08 20:34:47.775840 Epoch 63  	Train Loss = 2.77509 Val Loss = 4.10388
2023-04-08 20:35:28.084353 Epoch 64  	Train Loss = 2.76596 Val Loss = 4.11507
2023-04-08 20:36:08.593471 Epoch 65  	Train Loss = 2.76503 Val Loss = 4.11207
2023-04-08 20:36:49.094512 Epoch 66  	Train Loss = 2.76150 Val Loss = 4.10418
CL target length = 11
2023-04-08 20:37:29.636958 Epoch 67  	Train Loss = 2.79578 Val Loss = 3.44835
2023-04-08 20:38:10.126603 Epoch 68  	Train Loss = 2.80307 Val Loss = 3.41691
2023-04-08 20:38:50.615228 Epoch 69  	Train Loss = 2.79917 Val Loss = 3.40915
2023-04-08 20:39:31.105838 Epoch 70  	Train Loss = 2.79548 Val Loss = 3.42797
2023-04-08 20:40:11.632929 Epoch 71  	Train Loss = 2.79390 Val Loss = 3.45024
2023-04-08 20:40:52.171091 Epoch 72  	Train Loss = 2.78480 Val Loss = 3.41571
2023-04-08 20:41:32.826471 Epoch 73  	Train Loss = 2.78768 Val Loss = 3.42255
CL target length = 12
2023-04-08 20:42:13.683655 Epoch 74  	Train Loss = 2.82783 Val Loss = 2.74589
2023-04-08 20:42:55.188801 Epoch 75  	Train Loss = 2.82670 Val Loss = 2.74946
2023-04-08 20:43:36.981972 Epoch 76  	Train Loss = 2.82043 Val Loss = 2.73915
2023-04-08 20:44:18.850323 Epoch 77  	Train Loss = 2.81683 Val Loss = 2.72914
2023-04-08 20:45:00.658945 Epoch 78  	Train Loss = 2.81312 Val Loss = 2.76355
2023-04-08 20:45:42.397558 Epoch 79  	Train Loss = 2.81095 Val Loss = 2.76036
2023-04-08 20:46:23.410303 Epoch 80  	Train Loss = 2.81093 Val Loss = 2.76476
2023-04-08 20:47:03.798434 Epoch 81  	Train Loss = 2.75702 Val Loss = 2.70770
2023-04-08 20:47:44.081226 Epoch 82  	Train Loss = 2.74908 Val Loss = 2.71684
2023-04-08 20:48:24.304641 Epoch 83  	Train Loss = 2.74502 Val Loss = 2.71452
2023-04-08 20:49:04.470500 Epoch 84  	Train Loss = 2.74516 Val Loss = 2.71824
2023-04-08 20:49:44.675344 Epoch 85  	Train Loss = 2.74386 Val Loss = 2.71530
2023-04-08 20:50:24.862810 Epoch 86  	Train Loss = 2.74150 Val Loss = 2.71076
2023-04-08 20:51:05.088337 Epoch 87  	Train Loss = 2.74103 Val Loss = 2.71621
2023-04-08 20:51:45.349263 Epoch 88  	Train Loss = 2.74086 Val Loss = 2.71725
2023-04-08 20:52:25.564592 Epoch 89  	Train Loss = 2.73911 Val Loss = 2.72441
2023-04-08 20:53:05.860353 Epoch 90  	Train Loss = 2.73869 Val Loss = 2.72703
2023-04-08 20:53:46.273128 Epoch 91  	Train Loss = 2.73724 Val Loss = 2.72982
Early stopping at epoch: 91
Best at epoch 81:
Train Loss = 2.75702
Train RMSE = 5.34152, MAE = 2.68183, MAPE = 6.94876
Val Loss = 2.70770
Val RMSE = 5.75403, MAE = 2.75737, MAPE = 7.51908
--------- Test ---------
All Steps RMSE = 6.06337, MAE = 3.00007, MAPE = 8.03730
Step 1 RMSE = 3.78646, MAE = 2.19523, MAPE = 5.22175
Step 2 RMSE = 4.56062, MAE = 2.47231, MAPE = 6.13415
Step 3 RMSE = 5.08518, MAE = 2.65793, MAPE = 6.80323
Step 4 RMSE = 5.48714, MAE = 2.80689, MAPE = 7.34259
Step 5 RMSE = 5.82820, MAE = 2.93120, MAPE = 7.80033
Step 6 RMSE = 6.11776, MAE = 3.03805, MAPE = 8.17569
Step 7 RMSE = 6.35770, MAE = 3.13179, MAPE = 8.50732
Step 8 RMSE = 6.56790, MAE = 3.21652, MAPE = 8.80356
Step 9 RMSE = 6.75688, MAE = 3.29151, MAPE = 9.06469
Step 10 RMSE = 6.91883, MAE = 3.35753, MAPE = 9.30284
Step 11 RMSE = 7.05950, MAE = 3.41931, MAPE = 9.53008
Step 12 RMSE = 7.19420, MAE = 3.48262, MAPE = 9.76156
Inference time: 3.41 s
