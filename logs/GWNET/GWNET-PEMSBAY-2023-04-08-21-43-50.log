PEMSBAY
Trainset:	x-(36465, 12, 325, 2)	y-(36465, 12, 325, 1)
Valset:  	x-(5209, 12, 325, 2)  	y-(5209, 12, 325, 1)
Testset:	x-(10419, 12, 325, 2)	y-(10419, 12, 325, 1)

--------- GWNET ---------
{
    "num_nodes": 325,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        50
    ],
    "clip_grad": false,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": true,
    "cl_step_size": 2500,
    "pass_device": true,
    "model_args": {
        "num_nodes": 325,
        "in_dim": 2,
        "out_dim": 12,
        "adj_path": "../data/PEMSBAY/adj_mx_bay.pkl",
        "adj_type": "doubletransition",
        "device": "cuda:0",
        "dropout": 0.3,
        "gcn_bool": true,
        "addaptadj": true,
        "aptinit": null
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
GWNET                                    --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─ModuleList: 1-3                        --                        --
├─ModuleList: 1-4                        --                        --
├─ModuleList: 1-5                        --                        --
├─ModuleList: 1-6                        --                        --
├─Conv2d: 1-7                            [64, 32, 325, 13]         96
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-1                       [64, 32, 325, 12]         2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-2                       [64, 32, 325, 12]         2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-3                       [64, 256, 325, 12]        8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-4                          [64, 32, 325, 12]         --
│    │    └─nconv: 3-1                   [64, 32, 325, 12]         --
│    │    └─nconv: 3-2                   [64, 32, 325, 12]         --
│    │    └─nconv: 3-3                   [64, 32, 325, 12]         --
│    │    └─nconv: 3-4                   [64, 32, 325, 12]         --
│    │    └─nconv: 3-5                   [64, 32, 325, 12]         --
│    │    └─nconv: 3-6                   [64, 32, 325, 12]         --
│    │    └─linear: 3-7                  [64, 32, 325, 12]         7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-5                  [64, 32, 325, 12]         64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-6                       [64, 32, 325, 10]         2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-7                       [64, 32, 325, 10]         2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-8                       [64, 256, 325, 10]        8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-9                          [64, 32, 325, 10]         --
│    │    └─nconv: 3-8                   [64, 32, 325, 10]         --
│    │    └─nconv: 3-9                   [64, 32, 325, 10]         --
│    │    └─nconv: 3-10                  [64, 32, 325, 10]         --
│    │    └─nconv: 3-11                  [64, 32, 325, 10]         --
│    │    └─nconv: 3-12                  [64, 32, 325, 10]         --
│    │    └─nconv: 3-13                  [64, 32, 325, 10]         --
│    │    └─linear: 3-14                 [64, 32, 325, 10]         7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-10                 [64, 32, 325, 10]         64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-11                      [64, 32, 325, 9]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-12                      [64, 32, 325, 9]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-13                      [64, 256, 325, 9]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-14                         [64, 32, 325, 9]          --
│    │    └─nconv: 3-15                  [64, 32, 325, 9]          --
│    │    └─nconv: 3-16                  [64, 32, 325, 9]          --
│    │    └─nconv: 3-17                  [64, 32, 325, 9]          --
│    │    └─nconv: 3-18                  [64, 32, 325, 9]          --
│    │    └─nconv: 3-19                  [64, 32, 325, 9]          --
│    │    └─nconv: 3-20                  [64, 32, 325, 9]          --
│    │    └─linear: 3-21                 [64, 32, 325, 9]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-15                 [64, 32, 325, 9]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-16                      [64, 32, 325, 7]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-17                      [64, 32, 325, 7]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-18                      [64, 256, 325, 7]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-19                         [64, 32, 325, 7]          --
│    │    └─nconv: 3-22                  [64, 32, 325, 7]          --
│    │    └─nconv: 3-23                  [64, 32, 325, 7]          --
│    │    └─nconv: 3-24                  [64, 32, 325, 7]          --
│    │    └─nconv: 3-25                  [64, 32, 325, 7]          --
│    │    └─nconv: 3-26                  [64, 32, 325, 7]          --
│    │    └─nconv: 3-27                  [64, 32, 325, 7]          --
│    │    └─linear: 3-28                 [64, 32, 325, 7]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-20                 [64, 32, 325, 7]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-21                      [64, 32, 325, 6]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-22                      [64, 32, 325, 6]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-23                      [64, 256, 325, 6]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-24                         [64, 32, 325, 6]          --
│    │    └─nconv: 3-29                  [64, 32, 325, 6]          --
│    │    └─nconv: 3-30                  [64, 32, 325, 6]          --
│    │    └─nconv: 3-31                  [64, 32, 325, 6]          --
│    │    └─nconv: 3-32                  [64, 32, 325, 6]          --
│    │    └─nconv: 3-33                  [64, 32, 325, 6]          --
│    │    └─nconv: 3-34                  [64, 32, 325, 6]          --
│    │    └─linear: 3-35                 [64, 32, 325, 6]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-25                 [64, 32, 325, 6]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-26                      [64, 32, 325, 4]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-27                      [64, 32, 325, 4]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-28                      [64, 256, 325, 4]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-29                         [64, 32, 325, 4]          --
│    │    └─nconv: 3-36                  [64, 32, 325, 4]          --
│    │    └─nconv: 3-37                  [64, 32, 325, 4]          --
│    │    └─nconv: 3-38                  [64, 32, 325, 4]          --
│    │    └─nconv: 3-39                  [64, 32, 325, 4]          --
│    │    └─nconv: 3-40                  [64, 32, 325, 4]          --
│    │    └─nconv: 3-41                  [64, 32, 325, 4]          --
│    │    └─linear: 3-42                 [64, 32, 325, 4]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-30                 [64, 32, 325, 4]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-31                      [64, 32, 325, 3]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-32                      [64, 32, 325, 3]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-33                      [64, 256, 325, 3]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-34                         [64, 32, 325, 3]          --
│    │    └─nconv: 3-43                  [64, 32, 325, 3]          --
│    │    └─nconv: 3-44                  [64, 32, 325, 3]          --
│    │    └─nconv: 3-45                  [64, 32, 325, 3]          --
│    │    └─nconv: 3-46                  [64, 32, 325, 3]          --
│    │    └─nconv: 3-47                  [64, 32, 325, 3]          --
│    │    └─nconv: 3-48                  [64, 32, 325, 3]          --
│    │    └─linear: 3-49                 [64, 32, 325, 3]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-35                 [64, 32, 325, 3]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-36                      [64, 32, 325, 1]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-37                      [64, 32, 325, 1]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-38                      [64, 256, 325, 1]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-39                         [64, 32, 325, 1]          --
│    │    └─nconv: 3-50                  [64, 32, 325, 1]          --
│    │    └─nconv: 3-51                  [64, 32, 325, 1]          --
│    │    └─nconv: 3-52                  [64, 32, 325, 1]          --
│    │    └─nconv: 3-53                  [64, 32, 325, 1]          --
│    │    └─nconv: 3-54                  [64, 32, 325, 1]          --
│    │    └─nconv: 3-55                  [64, 32, 325, 1]          --
│    │    └─linear: 3-56                 [64, 32, 325, 1]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-40                 [64, 32, 325, 1]          64
├─Conv2d: 1-8                            [64, 512, 325, 1]         131,584
├─Conv2d: 1-9                            [64, 12, 325, 1]          6,156
==========================================================================================
Total params: 296,812
Trainable params: 296,812
Non-trainable params: 0
Total mult-adds (G): 24.32
==========================================================================================
Input size (MB): 2.00
Forward/backward pass size (MB): 3479.09
Params size (MB): 1.19
Estimated Total Size (MB): 3482.28
==========================================================================================

Loss: MaskedMAELoss

CL target length = 1
2023-04-08 21:45:30.157118 Epoch 1  	Train Loss = 1.02776 Val Loss = 5.72456
2023-04-08 21:47:08.146952 Epoch 2  	Train Loss = 0.88479 Val Loss = 5.72096
2023-04-08 21:48:47.105253 Epoch 3  	Train Loss = 0.86274 Val Loss = 5.71816
2023-04-08 21:50:26.278126 Epoch 4  	Train Loss = 0.85693 Val Loss = 5.71801
CL target length = 2
2023-04-08 21:52:05.519414 Epoch 5  	Train Loss = 0.96546 Val Loss = 5.31440
2023-04-08 21:53:44.912347 Epoch 6  	Train Loss = 0.99605 Val Loss = 5.30602
2023-04-08 21:55:24.400251 Epoch 7  	Train Loss = 0.99436 Val Loss = 5.30870
2023-04-08 21:57:04.157389 Epoch 8  	Train Loss = 0.99045 Val Loss = 5.30660
CL target length = 3
2023-04-08 21:58:43.845335 Epoch 9  	Train Loss = 1.03074 Val Loss = 4.91343
2023-04-08 22:00:23.195091 Epoch 10  	Train Loss = 1.11121 Val Loss = 4.91322
2023-04-08 22:02:02.190243 Epoch 11  	Train Loss = 1.10618 Val Loss = 4.91019
2023-04-08 22:03:41.202070 Epoch 12  	Train Loss = 1.10532 Val Loss = 4.92291
2023-04-08 22:05:20.350633 Epoch 13  	Train Loss = 1.10395 Val Loss = 4.91132
CL target length = 4
2023-04-08 22:06:59.635844 Epoch 14  	Train Loss = 1.20128 Val Loss = 4.53225
2023-04-08 22:08:38.981379 Epoch 15  	Train Loss = 1.20176 Val Loss = 4.53326
2023-04-08 22:10:18.366724 Epoch 16  	Train Loss = 1.20097 Val Loss = 4.54794
2023-04-08 22:11:57.929251 Epoch 17  	Train Loss = 1.19781 Val Loss = 4.52955
CL target length = 5
2023-04-08 22:13:37.425805 Epoch 18  	Train Loss = 1.24577 Val Loss = 4.16575
2023-04-08 22:15:16.525065 Epoch 19  	Train Loss = 1.28417 Val Loss = 4.16302
2023-04-08 22:16:55.492114 Epoch 20  	Train Loss = 1.27989 Val Loss = 4.16092
2023-04-08 22:18:34.506506 Epoch 21  	Train Loss = 1.27427 Val Loss = 4.15750
CL target length = 6
2023-04-08 22:20:13.622372 Epoch 22  	Train Loss = 1.28423 Val Loss = 3.80672
2023-04-08 22:21:52.890575 Epoch 23  	Train Loss = 1.34752 Val Loss = 3.80197
2023-04-08 22:23:32.290460 Epoch 24  	Train Loss = 1.34197 Val Loss = 3.80956
2023-04-08 22:25:11.693227 Epoch 25  	Train Loss = 1.33565 Val Loss = 3.79869
2023-04-08 22:26:51.027655 Epoch 26  	Train Loss = 1.33076 Val Loss = 3.78838
CL target length = 7
2023-04-08 22:28:30.232245 Epoch 27  	Train Loss = 1.37921 Val Loss = 3.44014
2023-04-08 22:30:09.415507 Epoch 28  	Train Loss = 1.38759 Val Loss = 3.43042
2023-04-08 22:31:48.551222 Epoch 29  	Train Loss = 1.38119 Val Loss = 3.43363
2023-04-08 22:33:27.810514 Epoch 30  	Train Loss = 1.37727 Val Loss = 3.42655
CL target length = 8
2023-04-08 22:35:07.182226 Epoch 31  	Train Loss = 1.39693 Val Loss = 3.07936
2023-04-08 22:36:46.632549 Epoch 32  	Train Loss = 1.42689 Val Loss = 3.07139
2023-04-08 22:38:25.995499 Epoch 33  	Train Loss = 1.42242 Val Loss = 3.06764
2023-04-08 22:40:05.142392 Epoch 34  	Train Loss = 1.41741 Val Loss = 3.06007
2023-04-08 22:41:44.017468 Epoch 35  	Train Loss = 1.41112 Val Loss = 3.05750
CL target length = 9
2023-04-08 22:43:22.865607 Epoch 36  	Train Loss = 1.46142 Val Loss = 2.70832
2023-04-08 22:45:01.549490 Epoch 37  	Train Loss = 1.45401 Val Loss = 2.71322
2023-04-08 22:46:40.086275 Epoch 38  	Train Loss = 1.44863 Val Loss = 2.70150
2023-04-08 22:48:18.731407 Epoch 39  	Train Loss = 1.44612 Val Loss = 2.70208
CL target length = 10
2023-04-08 22:49:57.628886 Epoch 40  	Train Loss = 1.47222 Val Loss = 2.39744
2023-04-08 22:51:36.679229 Epoch 41  	Train Loss = 1.48599 Val Loss = 2.35069
2023-04-08 22:53:15.432165 Epoch 42  	Train Loss = 1.47830 Val Loss = 2.35867
2023-04-08 22:54:53.791718 Epoch 43  	Train Loss = 1.47334 Val Loss = 2.35689
CL target length = 11
2023-04-08 22:56:32.179433 Epoch 44  	Train Loss = 1.48272 Val Loss = 2.00613
2023-04-08 22:58:10.559789 Epoch 45  	Train Loss = 1.51148 Val Loss = 1.98755
2023-04-08 22:59:48.980531 Epoch 46  	Train Loss = 1.50409 Val Loss = 2.00872
2023-04-08 23:01:27.499615 Epoch 47  	Train Loss = 1.49991 Val Loss = 1.99434
2023-04-08 23:03:06.319041 Epoch 48  	Train Loss = 1.49910 Val Loss = 1.98802
CL target length = 12
2023-04-08 23:04:45.415046 Epoch 49  	Train Loss = 1.52855 Val Loss = 1.64856
2023-04-08 23:06:24.370771 Epoch 50  	Train Loss = 1.52650 Val Loss = 1.64657
2023-04-08 23:08:02.941371 Epoch 51  	Train Loss = 1.49378 Val Loss = 1.61272
2023-04-08 23:09:41.331879 Epoch 52  	Train Loss = 1.49009 Val Loss = 1.62047
2023-04-08 23:11:19.746829 Epoch 53  	Train Loss = 1.48908 Val Loss = 1.60925
2023-04-08 23:12:58.146426 Epoch 54  	Train Loss = 1.48732 Val Loss = 1.60841
2023-04-08 23:14:36.508589 Epoch 55  	Train Loss = 1.48677 Val Loss = 1.61299
2023-04-08 23:16:14.966530 Epoch 56  	Train Loss = 1.48488 Val Loss = 1.61248
2023-04-08 23:17:53.707785 Epoch 57  	Train Loss = 1.48465 Val Loss = 1.61096
2023-04-08 23:19:32.696430 Epoch 58  	Train Loss = 1.48303 Val Loss = 1.60550
2023-04-08 23:21:11.444756 Epoch 59  	Train Loss = 1.48256 Val Loss = 1.60518
2023-04-08 23:22:50.102559 Epoch 60  	Train Loss = 1.48220 Val Loss = 1.60774
2023-04-08 23:24:28.807317 Epoch 61  	Train Loss = 1.48114 Val Loss = 1.60999
2023-04-08 23:26:07.720883 Epoch 62  	Train Loss = 1.48078 Val Loss = 1.60983
2023-04-08 23:27:46.576164 Epoch 63  	Train Loss = 1.47985 Val Loss = 1.60590
2023-04-08 23:29:25.636382 Epoch 64  	Train Loss = 1.47953 Val Loss = 1.60636
2023-04-08 23:31:04.773537 Epoch 65  	Train Loss = 1.47823 Val Loss = 1.60099
2023-04-08 23:32:44.061538 Epoch 66  	Train Loss = 1.47814 Val Loss = 1.60587
2023-04-08 23:34:23.379352 Epoch 67  	Train Loss = 1.47715 Val Loss = 1.60473
2023-04-08 23:36:02.521540 Epoch 68  	Train Loss = 1.47660 Val Loss = 1.60802
2023-04-08 23:37:41.400250 Epoch 69  	Train Loss = 1.47661 Val Loss = 1.60368
2023-04-08 23:39:20.307443 Epoch 70  	Train Loss = 1.47519 Val Loss = 1.60385
2023-04-08 23:40:59.162291 Epoch 71  	Train Loss = 1.47556 Val Loss = 1.60129
2023-04-08 23:42:38.131641 Epoch 72  	Train Loss = 1.47445 Val Loss = 1.60200
2023-04-08 23:44:17.192681 Epoch 73  	Train Loss = 1.47374 Val Loss = 1.60510
2023-04-08 23:45:56.344135 Epoch 74  	Train Loss = 1.47364 Val Loss = 1.60106
2023-04-08 23:47:35.529068 Epoch 75  	Train Loss = 1.47293 Val Loss = 1.60630
Early stopping at epoch: 75
Best at epoch 65:
Train Loss = 1.47823
Train RMSE = 3.18387, MAE = 1.43699, MAPE = 3.11931
Val Loss = 1.60099
Val RMSE = 3.65903, MAE = 1.59560, MAPE = 3.70879
--------- Test ---------
All Steps RMSE = 3.58110, MAE = 1.56631, MAPE = 3.55947
Step 1 RMSE = 1.54459, MAE = 0.85216, MAPE = 1.64684
Step 2 RMSE = 2.21705, MAE = 1.11952, MAPE = 2.26757
Step 3 RMSE = 2.73236, MAE = 1.30302, MAPE = 2.74138
Step 4 RMSE = 3.12548, MAE = 1.43866, MAPE = 3.12354
Step 5 RMSE = 3.42785, MAE = 1.54251, MAPE = 3.43588
Step 6 RMSE = 3.66519, MAE = 1.62529, MAPE = 3.69049
Step 7 RMSE = 3.85286, MAE = 1.69370, MAPE = 3.90390
Step 8 RMSE = 4.00367, MAE = 1.75160, MAPE = 4.08750
Step 9 RMSE = 4.12849, MAE = 1.80233, MAPE = 4.25211
Step 10 RMSE = 4.23518, MAE = 1.84671, MAPE = 4.39421
Step 11 RMSE = 4.33110, MAE = 1.88866, MAPE = 4.52323
Step 12 RMSE = 4.42253, MAE = 1.93158, MAPE = 4.64702
Inference time: 8.39 s
