PEMS08
Trainset:	x-(10700, 12, 170, 2)	y-(10700, 12, 170, 1)
Valset:  	x-(3567, 12, 170, 2)  	y-(3567, 12, 170, 1)
Testset:	x-(3566, 12, 170, 2)	y-(3566, 12, 170, 1)

--------- GWNET ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        168
    ],
    "early_stop": 15,
    "clip_grad": false,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": true,
    "cl_step_size": 2500,
    "pass_device": true,
    "model_args": {
        "num_nodes": 170,
        "in_dim": 2,
        "out_dim": 12,
        "adj_path": "../data/PEMS08/adj_PEMS08_distance.pkl",
        "adj_type": "doubletransition",
        "device": "cuda:0",
        "dropout": 0.3,
        "gcn_bool": true,
        "addaptadj": true,
        "aptinit": null
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
GWNET                                    --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─ModuleList: 1-3                        --                        --
├─ModuleList: 1-4                        --                        --
├─ModuleList: 1-5                        --                        --
├─ModuleList: 1-6                        --                        --
├─Conv2d: 1-7                            [64, 32, 170, 13]         96
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-1                       [64, 32, 170, 12]         2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-2                       [64, 32, 170, 12]         2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-3                       [64, 256, 170, 12]        8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-4                          [64, 32, 170, 12]         --
│    │    └─nconv: 3-1                   [64, 32, 170, 12]         --
│    │    └─nconv: 3-2                   [64, 32, 170, 12]         --
│    │    └─nconv: 3-3                   [64, 32, 170, 12]         --
│    │    └─nconv: 3-4                   [64, 32, 170, 12]         --
│    │    └─nconv: 3-5                   [64, 32, 170, 12]         --
│    │    └─nconv: 3-6                   [64, 32, 170, 12]         --
│    │    └─linear: 3-7                  [64, 32, 170, 12]         7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-5                  [64, 32, 170, 12]         64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-6                       [64, 32, 170, 10]         2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-7                       [64, 32, 170, 10]         2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-8                       [64, 256, 170, 10]        8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-9                          [64, 32, 170, 10]         --
│    │    └─nconv: 3-8                   [64, 32, 170, 10]         --
│    │    └─nconv: 3-9                   [64, 32, 170, 10]         --
│    │    └─nconv: 3-10                  [64, 32, 170, 10]         --
│    │    └─nconv: 3-11                  [64, 32, 170, 10]         --
│    │    └─nconv: 3-12                  [64, 32, 170, 10]         --
│    │    └─nconv: 3-13                  [64, 32, 170, 10]         --
│    │    └─linear: 3-14                 [64, 32, 170, 10]         7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-10                 [64, 32, 170, 10]         64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-11                      [64, 32, 170, 9]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-12                      [64, 32, 170, 9]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-13                      [64, 256, 170, 9]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-14                         [64, 32, 170, 9]          --
│    │    └─nconv: 3-15                  [64, 32, 170, 9]          --
│    │    └─nconv: 3-16                  [64, 32, 170, 9]          --
│    │    └─nconv: 3-17                  [64, 32, 170, 9]          --
│    │    └─nconv: 3-18                  [64, 32, 170, 9]          --
│    │    └─nconv: 3-19                  [64, 32, 170, 9]          --
│    │    └─nconv: 3-20                  [64, 32, 170, 9]          --
│    │    └─linear: 3-21                 [64, 32, 170, 9]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-15                 [64, 32, 170, 9]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-16                      [64, 32, 170, 7]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-17                      [64, 32, 170, 7]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-18                      [64, 256, 170, 7]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-19                         [64, 32, 170, 7]          --
│    │    └─nconv: 3-22                  [64, 32, 170, 7]          --
│    │    └─nconv: 3-23                  [64, 32, 170, 7]          --
│    │    └─nconv: 3-24                  [64, 32, 170, 7]          --
│    │    └─nconv: 3-25                  [64, 32, 170, 7]          --
│    │    └─nconv: 3-26                  [64, 32, 170, 7]          --
│    │    └─nconv: 3-27                  [64, 32, 170, 7]          --
│    │    └─linear: 3-28                 [64, 32, 170, 7]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-20                 [64, 32, 170, 7]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-21                      [64, 32, 170, 6]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-22                      [64, 32, 170, 6]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-23                      [64, 256, 170, 6]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-24                         [64, 32, 170, 6]          --
│    │    └─nconv: 3-29                  [64, 32, 170, 6]          --
│    │    └─nconv: 3-30                  [64, 32, 170, 6]          --
│    │    └─nconv: 3-31                  [64, 32, 170, 6]          --
│    │    └─nconv: 3-32                  [64, 32, 170, 6]          --
│    │    └─nconv: 3-33                  [64, 32, 170, 6]          --
│    │    └─nconv: 3-34                  [64, 32, 170, 6]          --
│    │    └─linear: 3-35                 [64, 32, 170, 6]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-25                 [64, 32, 170, 6]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-26                      [64, 32, 170, 4]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-27                      [64, 32, 170, 4]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-28                      [64, 256, 170, 4]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-29                         [64, 32, 170, 4]          --
│    │    └─nconv: 3-36                  [64, 32, 170, 4]          --
│    │    └─nconv: 3-37                  [64, 32, 170, 4]          --
│    │    └─nconv: 3-38                  [64, 32, 170, 4]          --
│    │    └─nconv: 3-39                  [64, 32, 170, 4]          --
│    │    └─nconv: 3-40                  [64, 32, 170, 4]          --
│    │    └─nconv: 3-41                  [64, 32, 170, 4]          --
│    │    └─linear: 3-42                 [64, 32, 170, 4]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-30                 [64, 32, 170, 4]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-31                      [64, 32, 170, 3]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-32                      [64, 32, 170, 3]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-33                      [64, 256, 170, 3]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-34                         [64, 32, 170, 3]          --
│    │    └─nconv: 3-43                  [64, 32, 170, 3]          --
│    │    └─nconv: 3-44                  [64, 32, 170, 3]          --
│    │    └─nconv: 3-45                  [64, 32, 170, 3]          --
│    │    └─nconv: 3-46                  [64, 32, 170, 3]          --
│    │    └─nconv: 3-47                  [64, 32, 170, 3]          --
│    │    └─nconv: 3-48                  [64, 32, 170, 3]          --
│    │    └─linear: 3-49                 [64, 32, 170, 3]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-35                 [64, 32, 170, 3]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-36                      [64, 32, 170, 1]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-37                      [64, 32, 170, 1]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-38                      [64, 256, 170, 1]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-39                         [64, 32, 170, 1]          --
│    │    └─nconv: 3-50                  [64, 32, 170, 1]          --
│    │    └─nconv: 3-51                  [64, 32, 170, 1]          --
│    │    └─nconv: 3-52                  [64, 32, 170, 1]          --
│    │    └─nconv: 3-53                  [64, 32, 170, 1]          --
│    │    └─nconv: 3-54                  [64, 32, 170, 1]          --
│    │    └─nconv: 3-55                  [64, 32, 170, 1]          --
│    │    └─linear: 3-56                 [64, 32, 170, 1]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-40                 [64, 32, 170, 1]          64
├─Conv2d: 1-8                            [64, 512, 170, 1]         131,584
├─Conv2d: 1-9                            [64, 12, 170, 1]          6,156
==========================================================================================
Total params: 296,812
Trainable params: 296,812
Non-trainable params: 0
Total mult-adds (G): 12.72
==========================================================================================
Input size (MB): 1.04
Forward/backward pass size (MB): 1819.83
Params size (MB): 1.19
Estimated Total Size (MB): 1822.06
==========================================================================================

Loss: HuberLoss

CL target length = 1
2023-04-08 20:07:29.797827 Epoch 1  	Train Loss = 21.98393 Val Loss = 113.36484
2023-04-08 20:07:44.256877 Epoch 2  	Train Loss = 16.56709 Val Loss = 113.28916
2023-04-08 20:07:58.832783 Epoch 3  	Train Loss = 15.20023 Val Loss = 112.86390
2023-04-08 20:08:13.336281 Epoch 4  	Train Loss = 14.41370 Val Loss = 113.01219
2023-04-08 20:08:28.306645 Epoch 5  	Train Loss = 14.12520 Val Loss = 113.00476
2023-04-08 20:08:43.240468 Epoch 6  	Train Loss = 14.11468 Val Loss = 113.10324
2023-04-08 20:08:57.785331 Epoch 7  	Train Loss = 13.92445 Val Loss = 112.84642
2023-04-08 20:09:12.349700 Epoch 8  	Train Loss = 13.83333 Val Loss = 112.84379
2023-04-08 20:09:26.946656 Epoch 9  	Train Loss = 13.64477 Val Loss = 112.89587
2023-04-08 20:09:41.629688 Epoch 10  	Train Loss = 13.65117 Val Loss = 112.84762
2023-04-08 20:09:56.333193 Epoch 11  	Train Loss = 13.41200 Val Loss = 112.82148
2023-04-08 20:10:11.060304 Epoch 12  	Train Loss = 13.48982 Val Loss = 112.81816
2023-04-08 20:10:25.750338 Epoch 13  	Train Loss = 13.34776 Val Loss = 112.82995
2023-04-08 20:10:40.577526 Epoch 14  	Train Loss = 13.37306 Val Loss = 112.82253
CL target length = 2
2023-04-08 20:10:55.752466 Epoch 15  	Train Loss = 15.00841 Val Loss = 104.25976
2023-04-08 20:11:10.532415 Epoch 16  	Train Loss = 14.21102 Val Loss = 103.87270
2023-04-08 20:11:25.315123 Epoch 17  	Train Loss = 13.83220 Val Loss = 103.82062
2023-04-08 20:11:40.106516 Epoch 18  	Train Loss = 13.81833 Val Loss = 103.81038
2023-04-08 20:11:54.911852 Epoch 19  	Train Loss = 13.74902 Val Loss = 103.83099
2023-04-08 20:12:09.718520 Epoch 20  	Train Loss = 13.73048 Val Loss = 103.83636
2023-04-08 20:12:24.583886 Epoch 21  	Train Loss = 13.67948 Val Loss = 103.79323
2023-04-08 20:12:39.407733 Epoch 22  	Train Loss = 13.71869 Val Loss = 103.86796
2023-04-08 20:12:54.304690 Epoch 23  	Train Loss = 13.61845 Val Loss = 103.83508
2023-04-08 20:13:09.157677 Epoch 24  	Train Loss = 13.61406 Val Loss = 103.82926
2023-04-08 20:13:24.043152 Epoch 25  	Train Loss = 13.58425 Val Loss = 103.79787
2023-04-08 20:13:38.904090 Epoch 26  	Train Loss = 13.49453 Val Loss = 103.88619
2023-04-08 20:13:53.771403 Epoch 27  	Train Loss = 13.49583 Val Loss = 103.75575
2023-04-08 20:14:08.708836 Epoch 28  	Train Loss = 13.47406 Val Loss = 103.77196
2023-04-08 20:14:23.720117 Epoch 29  	Train Loss = 13.38882 Val Loss = 103.74926
CL target length = 3
2023-04-08 20:14:38.597194 Epoch 30  	Train Loss = 14.86669 Val Loss = 94.96897
2023-04-08 20:14:53.489947 Epoch 31  	Train Loss = 13.94341 Val Loss = 94.87940
2023-04-08 20:15:08.367292 Epoch 32  	Train Loss = 13.83318 Val Loss = 94.95882
2023-04-08 20:15:23.262852 Epoch 33  	Train Loss = 13.79844 Val Loss = 94.80932
2023-04-08 20:15:38.152723 Epoch 34  	Train Loss = 13.74577 Val Loss = 94.90502
2023-04-08 20:15:53.119769 Epoch 35  	Train Loss = 13.74449 Val Loss = 94.84831
2023-04-08 20:16:07.994845 Epoch 36  	Train Loss = 13.69411 Val Loss = 94.78517
2023-04-08 20:16:22.866034 Epoch 37  	Train Loss = 13.64486 Val Loss = 94.74303
2023-04-08 20:16:37.747918 Epoch 38  	Train Loss = 13.58821 Val Loss = 94.73263
2023-04-08 20:16:52.698071 Epoch 39  	Train Loss = 13.55784 Val Loss = 94.75347
2023-04-08 20:17:07.551382 Epoch 40  	Train Loss = 13.52847 Val Loss = 94.79785
2023-04-08 20:17:22.392309 Epoch 41  	Train Loss = 13.53864 Val Loss = 94.94031
2023-04-08 20:17:37.433891 Epoch 42  	Train Loss = 13.51845 Val Loss = 94.73528
2023-04-08 20:17:52.310348 Epoch 43  	Train Loss = 13.45823 Val Loss = 94.84653
2023-04-08 20:18:07.230641 Epoch 44  	Train Loss = 13.41909 Val Loss = 94.76696
CL target length = 4
2023-04-08 20:18:22.132127 Epoch 45  	Train Loss = 14.50644 Val Loss = 86.00209
2023-04-08 20:18:36.949948 Epoch 46  	Train Loss = 13.80547 Val Loss = 85.94558
2023-04-08 20:18:51.746352 Epoch 47  	Train Loss = 13.79006 Val Loss = 85.76902
2023-04-08 20:19:06.556596 Epoch 48  	Train Loss = 13.69487 Val Loss = 85.83211
2023-04-08 20:19:21.373786 Epoch 49  	Train Loss = 13.61690 Val Loss = 85.79020
2023-04-08 20:19:36.185878 Epoch 50  	Train Loss = 13.66471 Val Loss = 85.91102
2023-04-08 20:19:50.985915 Epoch 51  	Train Loss = 13.62500 Val Loss = 85.84621
2023-04-08 20:20:05.794791 Epoch 52  	Train Loss = 13.55418 Val Loss = 85.80321
2023-04-08 20:20:20.595026 Epoch 53  	Train Loss = 13.50847 Val Loss = 85.76726
2023-04-08 20:20:35.401233 Epoch 54  	Train Loss = 13.53107 Val Loss = 85.73079
2023-04-08 20:20:50.210684 Epoch 55  	Train Loss = 13.45830 Val Loss = 85.74094
2023-04-08 20:21:05.031126 Epoch 56  	Train Loss = 13.45594 Val Loss = 85.71799
2023-04-08 20:21:19.840847 Epoch 57  	Train Loss = 13.48416 Val Loss = 85.72048
2023-04-08 20:21:34.653242 Epoch 58  	Train Loss = 13.41602 Val Loss = 85.79638
2023-04-08 20:21:49.442654 Epoch 59  	Train Loss = 13.43839 Val Loss = 85.69627
CL target length = 5
2023-04-08 20:22:04.289428 Epoch 60  	Train Loss = 14.42211 Val Loss = 76.89903
2023-04-08 20:22:19.098550 Epoch 61  	Train Loss = 13.70197 Val Loss = 76.83941
2023-04-08 20:22:33.899804 Epoch 62  	Train Loss = 13.69458 Val Loss = 76.84385
2023-04-08 20:22:48.713748 Epoch 63  	Train Loss = 13.63844 Val Loss = 76.89872
2023-04-08 20:23:03.521121 Epoch 64  	Train Loss = 13.63886 Val Loss = 76.93756
2023-04-08 20:23:18.322669 Epoch 65  	Train Loss = 13.60570 Val Loss = 76.76204
2023-04-08 20:23:33.123881 Epoch 66  	Train Loss = 13.51374 Val Loss = 76.68867
2023-04-08 20:23:47.902001 Epoch 67  	Train Loss = 13.50745 Val Loss = 76.76920
2023-04-08 20:24:02.703205 Epoch 68  	Train Loss = 13.52463 Val Loss = 76.76157
2023-04-08 20:24:17.508896 Epoch 69  	Train Loss = 13.44645 Val Loss = 76.90209
2023-04-08 20:24:32.354900 Epoch 70  	Train Loss = 13.51009 Val Loss = 76.75784
2023-04-08 20:24:47.219520 Epoch 71  	Train Loss = 13.42926 Val Loss = 76.93538
2023-04-08 20:25:02.171694 Epoch 72  	Train Loss = 13.40954 Val Loss = 76.71052
2023-04-08 20:25:16.970609 Epoch 73  	Train Loss = 13.45286 Val Loss = 76.77134
2023-04-08 20:25:31.763571 Epoch 74  	Train Loss = 13.38980 Val Loss = 76.68186
CL target length = 6
2023-04-08 20:25:46.575363 Epoch 75  	Train Loss = 14.31184 Val Loss = 67.78428
2023-04-08 20:26:01.362746 Epoch 76  	Train Loss = 13.62056 Val Loss = 67.89607
2023-04-08 20:26:16.182783 Epoch 77  	Train Loss = 13.63507 Val Loss = 67.75365
2023-04-08 20:26:30.991032 Epoch 78  	Train Loss = 13.59628 Val Loss = 67.75640
2023-04-08 20:26:45.888359 Epoch 79  	Train Loss = 13.53289 Val Loss = 67.73737
2023-04-08 20:27:00.713303 Epoch 80  	Train Loss = 13.49402 Val Loss = 67.75154
2023-04-08 20:27:15.522920 Epoch 81  	Train Loss = 13.55227 Val Loss = 67.73430
2023-04-08 20:27:30.341153 Epoch 82  	Train Loss = 13.51889 Val Loss = 67.80828
2023-04-08 20:27:45.205343 Epoch 83  	Train Loss = 13.44100 Val Loss = 67.69545
2023-04-08 20:28:00.052935 Epoch 84  	Train Loss = 13.47843 Val Loss = 67.70954
2023-04-08 20:28:14.901930 Epoch 85  	Train Loss = 13.48540 Val Loss = 67.72884
2023-04-08 20:28:29.961274 Epoch 86  	Train Loss = 13.46148 Val Loss = 67.92178
2023-04-08 20:28:44.830541 Epoch 87  	Train Loss = 13.42513 Val Loss = 67.71374
2023-04-08 20:28:59.687304 Epoch 88  	Train Loss = 13.39823 Val Loss = 67.76395
2023-04-08 20:29:14.549584 Epoch 89  	Train Loss = 13.42119 Val Loss = 67.65559
CL target length = 7
2023-04-08 20:29:29.402332 Epoch 90  	Train Loss = 14.23285 Val Loss = 59.11511
2023-04-08 20:29:44.263128 Epoch 91  	Train Loss = 13.63627 Val Loss = 58.75357
2023-04-08 20:29:59.120999 Epoch 92  	Train Loss = 13.56729 Val Loss = 58.88057
2023-04-08 20:30:13.980074 Epoch 93  	Train Loss = 13.51954 Val Loss = 58.79381
2023-04-08 20:30:28.866149 Epoch 94  	Train Loss = 13.52591 Val Loss = 58.80297
2023-04-08 20:30:43.748943 Epoch 95  	Train Loss = 13.53933 Val Loss = 58.76174
2023-04-08 20:30:58.619381 Epoch 96  	Train Loss = 13.50793 Val Loss = 58.95909
2023-04-08 20:31:13.482384 Epoch 97  	Train Loss = 13.49545 Val Loss = 58.92422
2023-04-08 20:31:28.351082 Epoch 98  	Train Loss = 13.51273 Val Loss = 58.85742
2023-04-08 20:31:43.212424 Epoch 99  	Train Loss = 13.48408 Val Loss = 58.89287
2023-04-08 20:31:58.045164 Epoch 100  	Train Loss = 13.45385 Val Loss = 58.93705
2023-04-08 20:32:12.871651 Epoch 101  	Train Loss = 13.48602 Val Loss = 58.81577
2023-04-08 20:32:27.690301 Epoch 102  	Train Loss = 13.44957 Val Loss = 58.87722
2023-04-08 20:32:42.517486 Epoch 103  	Train Loss = 13.36756 Val Loss = 58.76627
2023-04-08 20:32:57.381675 Epoch 104  	Train Loss = 13.35650 Val Loss = 58.82676
CL target length = 8
2023-04-08 20:33:12.222590 Epoch 105  	Train Loss = 14.23885 Val Loss = 49.90481
2023-04-08 20:33:27.011977 Epoch 106  	Train Loss = 13.63458 Val Loss = 49.87961
2023-04-08 20:33:41.796474 Epoch 107  	Train Loss = 13.57214 Val Loss = 49.81548
2023-04-08 20:33:56.594823 Epoch 108  	Train Loss = 13.54657 Val Loss = 49.83902
2023-04-08 20:34:11.395386 Epoch 109  	Train Loss = 13.53577 Val Loss = 49.88487
2023-04-08 20:34:26.177534 Epoch 110  	Train Loss = 13.55363 Val Loss = 49.97216
2023-04-08 20:34:40.971372 Epoch 111  	Train Loss = 13.47585 Val Loss = 49.79193
2023-04-08 20:34:55.757077 Epoch 112  	Train Loss = 13.51588 Val Loss = 49.92670
2023-04-08 20:35:10.539680 Epoch 113  	Train Loss = 13.46392 Val Loss = 49.81231
2023-04-08 20:35:25.333534 Epoch 114  	Train Loss = 13.48000 Val Loss = 49.91865
2023-04-08 20:35:40.136082 Epoch 115  	Train Loss = 13.47447 Val Loss = 49.92149
2023-04-08 20:35:54.922658 Epoch 116  	Train Loss = 13.46270 Val Loss = 49.81222
2023-04-08 20:36:09.715184 Epoch 117  	Train Loss = 13.44307 Val Loss = 49.79191
2023-04-08 20:36:24.809526 Epoch 118  	Train Loss = 13.39589 Val Loss = 49.85818
2023-04-08 20:36:39.597419 Epoch 119  	Train Loss = 13.40891 Val Loss = 49.76154
CL target length = 9
2023-04-08 20:36:54.393356 Epoch 120  	Train Loss = 14.12772 Val Loss = 41.07571
2023-04-08 20:37:09.443567 Epoch 121  	Train Loss = 13.59287 Val Loss = 40.98644
2023-04-08 20:37:24.228280 Epoch 122  	Train Loss = 13.55292 Val Loss = 41.12607
2023-04-08 20:37:38.998559 Epoch 123  	Train Loss = 13.61398 Val Loss = 40.93277
2023-04-08 20:37:53.789259 Epoch 124  	Train Loss = 13.53936 Val Loss = 40.97569
2023-04-08 20:38:08.576853 Epoch 125  	Train Loss = 13.52889 Val Loss = 40.96665
2023-04-08 20:38:23.364343 Epoch 126  	Train Loss = 13.54724 Val Loss = 40.86313
2023-04-08 20:38:38.127534 Epoch 127  	Train Loss = 13.49128 Val Loss = 40.86001
2023-04-08 20:38:52.913599 Epoch 128  	Train Loss = 13.52339 Val Loss = 40.89260
2023-04-08 20:39:07.730767 Epoch 129  	Train Loss = 13.45640 Val Loss = 41.13762
2023-04-08 20:39:22.521492 Epoch 130  	Train Loss = 13.45113 Val Loss = 40.88689
2023-04-08 20:39:37.308816 Epoch 131  	Train Loss = 13.45203 Val Loss = 41.01261
2023-04-08 20:39:52.080538 Epoch 132  	Train Loss = 13.44363 Val Loss = 41.09051
2023-04-08 20:40:06.873590 Epoch 133  	Train Loss = 13.40812 Val Loss = 40.83324
CL target length = 10
2023-04-08 20:40:21.670763 Epoch 134  	Train Loss = 13.78435 Val Loss = 34.54097
2023-04-08 20:40:36.532390 Epoch 135  	Train Loss = 13.78926 Val Loss = 32.21098
2023-04-08 20:40:51.331540 Epoch 136  	Train Loss = 13.59934 Val Loss = 32.07840
2023-04-08 20:41:06.126948 Epoch 137  	Train Loss = 13.59459 Val Loss = 32.31462
2023-04-08 20:41:20.941025 Epoch 138  	Train Loss = 13.56897 Val Loss = 31.94307
2023-04-08 20:41:35.733894 Epoch 139  	Train Loss = 13.53707 Val Loss = 32.06118
2023-04-08 20:41:50.617189 Epoch 140  	Train Loss = 13.52934 Val Loss = 32.25612
2023-04-08 20:42:05.444699 Epoch 141  	Train Loss = 13.53635 Val Loss = 32.05357
2023-04-08 20:42:20.269705 Epoch 142  	Train Loss = 13.50734 Val Loss = 32.13630
2023-04-08 20:42:35.110240 Epoch 143  	Train Loss = 13.51731 Val Loss = 32.37340
2023-04-08 20:42:49.946242 Epoch 144  	Train Loss = 13.52405 Val Loss = 32.30386
2023-04-08 20:43:04.791454 Epoch 145  	Train Loss = 13.56089 Val Loss = 31.95975
2023-04-08 20:43:19.643156 Epoch 146  	Train Loss = 13.47719 Val Loss = 32.29748
2023-04-08 20:43:34.498218 Epoch 147  	Train Loss = 13.53295 Val Loss = 32.06509
2023-04-08 20:43:49.326522 Epoch 148  	Train Loss = 13.42864 Val Loss = 32.17699
CL target length = 11
2023-04-08 20:44:04.169738 Epoch 149  	Train Loss = 13.94805 Val Loss = 23.53117
2023-04-08 20:44:19.194333 Epoch 150  	Train Loss = 13.65112 Val Loss = 23.37040
2023-04-08 20:44:34.535874 Epoch 151  	Train Loss = 13.60378 Val Loss = 23.26062
2023-04-08 20:44:49.886895 Epoch 152  	Train Loss = 13.59525 Val Loss = 23.23979
2023-04-08 20:45:05.155640 Epoch 153  	Train Loss = 13.58067 Val Loss = 23.23039
2023-04-08 20:45:20.018467 Epoch 154  	Train Loss = 13.53452 Val Loss = 23.17488
2023-04-08 20:45:34.843501 Epoch 155  	Train Loss = 13.54173 Val Loss = 23.06938
2023-04-08 20:45:49.663404 Epoch 156  	Train Loss = 13.57675 Val Loss = 23.24020
2023-04-08 20:46:04.512834 Epoch 157  	Train Loss = 13.50603 Val Loss = 23.44066
2023-04-08 20:46:19.415230 Epoch 158  	Train Loss = 13.54432 Val Loss = 23.13662
2023-04-08 20:46:34.391211 Epoch 159  	Train Loss = 13.48331 Val Loss = 23.20591
2023-04-08 20:46:49.182539 Epoch 160  	Train Loss = 13.45244 Val Loss = 23.59603
2023-04-08 20:47:03.955481 Epoch 161  	Train Loss = 13.52446 Val Loss = 23.08632
2023-04-08 20:47:18.716016 Epoch 162  	Train Loss = 13.47912 Val Loss = 23.41901
2023-04-08 20:47:33.470630 Epoch 163  	Train Loss = 13.55433 Val Loss = 23.23858
CL target length = 12
2023-04-08 20:47:48.208866 Epoch 164  	Train Loss = 13.94082 Val Loss = 14.67388
2023-04-08 20:48:02.938270 Epoch 165  	Train Loss = 13.66023 Val Loss = 14.38426
2023-04-08 20:48:17.659140 Epoch 166  	Train Loss = 13.58929 Val Loss = 14.29734
2023-04-08 20:48:32.380409 Epoch 167  	Train Loss = 13.57180 Val Loss = 14.72563
2023-04-08 20:48:47.097508 Epoch 168  	Train Loss = 13.58951 Val Loss = 14.39525
2023-04-08 20:49:01.834270 Epoch 169  	Train Loss = 13.29819 Val Loss = 14.01798
2023-04-08 20:49:16.675021 Epoch 170  	Train Loss = 13.24356 Val Loss = 14.05452
2023-04-08 20:49:31.425041 Epoch 171  	Train Loss = 13.22423 Val Loss = 14.03658
2023-04-08 20:49:46.176211 Epoch 172  	Train Loss = 13.23279 Val Loss = 14.01944
2023-04-08 20:50:00.940244 Epoch 173  	Train Loss = 13.21776 Val Loss = 14.04083
2023-04-08 20:50:15.696303 Epoch 174  	Train Loss = 13.22507 Val Loss = 14.02870
2023-04-08 20:50:30.445716 Epoch 175  	Train Loss = 13.22921 Val Loss = 14.03854
2023-04-08 20:50:45.184964 Epoch 176  	Train Loss = 13.23170 Val Loss = 14.01484
2023-04-08 20:50:59.943514 Epoch 177  	Train Loss = 13.20800 Val Loss = 14.01862
2023-04-08 20:51:14.716388 Epoch 178  	Train Loss = 13.21547 Val Loss = 14.02357
2023-04-08 20:51:29.472135 Epoch 179  	Train Loss = 13.21166 Val Loss = 14.06496
2023-04-08 20:51:44.235018 Epoch 180  	Train Loss = 13.19440 Val Loss = 14.02297
2023-04-08 20:51:59.021953 Epoch 181  	Train Loss = 13.19333 Val Loss = 14.06355
2023-04-08 20:52:13.819476 Epoch 182  	Train Loss = 13.20083 Val Loss = 14.01525
2023-04-08 20:52:28.589404 Epoch 183  	Train Loss = 13.19946 Val Loss = 14.02714
2023-04-08 20:52:43.352543 Epoch 184  	Train Loss = 13.20157 Val Loss = 14.04839
2023-04-08 20:52:58.121742 Epoch 185  	Train Loss = 13.19323 Val Loss = 14.05304
2023-04-08 20:53:12.910800 Epoch 186  	Train Loss = 13.19653 Val Loss = 14.04634
2023-04-08 20:53:27.686437 Epoch 187  	Train Loss = 13.19680 Val Loss = 14.06523
2023-04-08 20:53:42.452211 Epoch 188  	Train Loss = 13.19240 Val Loss = 14.04920
2023-04-08 20:53:57.220597 Epoch 189  	Train Loss = 13.18291 Val Loss = 14.04254
2023-04-08 20:54:11.984049 Epoch 190  	Train Loss = 13.18699 Val Loss = 14.01548
2023-04-08 20:54:26.753199 Epoch 191  	Train Loss = 13.17929 Val Loss = 14.10051
Early stopping at epoch: 191
Best at epoch 176:
Train Loss = 13.23170
Train RMSE = 22.44289, MAE = 13.37311, MAPE = 8.67686
Val Loss = 14.01484
Val RMSE = 24.38536, MAE = 14.55020, MAPE = 10.06721
--------- Test ---------
All Steps RMSE = 23.31029, MAE = 14.36961, MAPE = 9.21277
Step 1 RMSE = 19.35279, MAE = 12.27089, MAPE = 7.94201
Step 2 RMSE = 20.62910, MAE = 12.90731, MAPE = 8.32602
Step 3 RMSE = 21.56321, MAE = 13.37408, MAPE = 8.59171
Step 4 RMSE = 22.28116, MAE = 13.74509, MAPE = 8.78395
Step 5 RMSE = 22.86822, MAE = 14.05244, MAPE = 8.97340
Step 6 RMSE = 23.38862, MAE = 14.36878, MAPE = 9.17290
Step 7 RMSE = 23.85116, MAE = 14.65017, MAPE = 9.36051
Step 8 RMSE = 24.26100, MAE = 14.91409, MAPE = 9.54137
Step 9 RMSE = 24.60529, MAE = 15.14664, MAPE = 9.71865
Step 10 RMSE = 24.92521, MAE = 15.36776, MAPE = 9.86798
Step 11 RMSE = 25.28890, MAE = 15.63117, MAPE = 10.00714
Step 12 RMSE = 25.79251, MAE = 16.00675, MAPE = 10.26760
Inference time: 1.36 s
