PEMS04
Original data shape (16992, 307, 3)
Trainset:	x-(10172, 12, 307, 1)	y-(10172, 12, 307, 1)
Valset:  	x-(3375, 12, 307, 1)  	y-(3375, 12, 307, 1)
Testset:	x-(3376, 12, 307, 1)	y-(3376, 12, 307, 1)

--------- GWNET ---------
{
    "num_nodes": 307,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        175
    ],
    "early_stop": 15,
    "clip_grad": false,
    "batch_size": 64,
    "max_epochs": 500,
    "use_cl": true,
    "cl_step_size": 2500,
    "load_npz": false,
    "pass_device": true,
    "model_args": {
        "num_nodes": 307,
        "in_dim": 1,
        "out_dim": 12,
        "adj_path": "../data/PEMS04/adj_PEMS04_distance.pkl",
        "adj_type": "doubletransition",
        "device": "cuda:0",
        "dropout": 0.3,
        "gcn_bool": true,
        "addaptadj": true,
        "aptinit": null
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
GWNET                                    [64, 12, 307, 1]          14,588
├─Conv2d: 1-1                            [64, 32, 307, 13]         64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-1                       [64, 32, 307, 12]         2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-2                       [64, 32, 307, 12]         2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-3                       [64, 256, 307, 12]        8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-4                          [64, 32, 307, 12]         --
│    │    └─nconv: 3-1                   [64, 32, 307, 12]         --
│    │    └─nconv: 3-2                   [64, 32, 307, 12]         --
│    │    └─nconv: 3-3                   [64, 32, 307, 12]         --
│    │    └─nconv: 3-4                   [64, 32, 307, 12]         --
│    │    └─nconv: 3-5                   [64, 32, 307, 12]         --
│    │    └─nconv: 3-6                   [64, 32, 307, 12]         --
│    │    └─linear: 3-7                  [64, 32, 307, 12]         7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-5                  [64, 32, 307, 12]         64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-6                       [64, 32, 307, 10]         2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-7                       [64, 32, 307, 10]         2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-8                       [64, 256, 307, 10]        8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-9                          [64, 32, 307, 10]         --
│    │    └─nconv: 3-8                   [64, 32, 307, 10]         --
│    │    └─nconv: 3-9                   [64, 32, 307, 10]         --
│    │    └─nconv: 3-10                  [64, 32, 307, 10]         --
│    │    └─nconv: 3-11                  [64, 32, 307, 10]         --
│    │    └─nconv: 3-12                  [64, 32, 307, 10]         --
│    │    └─nconv: 3-13                  [64, 32, 307, 10]         --
│    │    └─linear: 3-14                 [64, 32, 307, 10]         7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-10                 [64, 32, 307, 10]         64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-11                      [64, 32, 307, 9]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-12                      [64, 32, 307, 9]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-13                      [64, 256, 307, 9]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-14                         [64, 32, 307, 9]          --
│    │    └─nconv: 3-15                  [64, 32, 307, 9]          --
│    │    └─nconv: 3-16                  [64, 32, 307, 9]          --
│    │    └─nconv: 3-17                  [64, 32, 307, 9]          --
│    │    └─nconv: 3-18                  [64, 32, 307, 9]          --
│    │    └─nconv: 3-19                  [64, 32, 307, 9]          --
│    │    └─nconv: 3-20                  [64, 32, 307, 9]          --
│    │    └─linear: 3-21                 [64, 32, 307, 9]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-15                 [64, 32, 307, 9]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-16                      [64, 32, 307, 7]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-17                      [64, 32, 307, 7]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-18                      [64, 256, 307, 7]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-19                         [64, 32, 307, 7]          --
│    │    └─nconv: 3-22                  [64, 32, 307, 7]          --
│    │    └─nconv: 3-23                  [64, 32, 307, 7]          --
│    │    └─nconv: 3-24                  [64, 32, 307, 7]          --
│    │    └─nconv: 3-25                  [64, 32, 307, 7]          --
│    │    └─nconv: 3-26                  [64, 32, 307, 7]          --
│    │    └─nconv: 3-27                  [64, 32, 307, 7]          --
│    │    └─linear: 3-28                 [64, 32, 307, 7]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-20                 [64, 32, 307, 7]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-21                      [64, 32, 307, 6]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-22                      [64, 32, 307, 6]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-23                      [64, 256, 307, 6]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-24                         [64, 32, 307, 6]          --
│    │    └─nconv: 3-29                  [64, 32, 307, 6]          --
│    │    └─nconv: 3-30                  [64, 32, 307, 6]          --
│    │    └─nconv: 3-31                  [64, 32, 307, 6]          --
│    │    └─nconv: 3-32                  [64, 32, 307, 6]          --
│    │    └─nconv: 3-33                  [64, 32, 307, 6]          --
│    │    └─nconv: 3-34                  [64, 32, 307, 6]          --
│    │    └─linear: 3-35                 [64, 32, 307, 6]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-25                 [64, 32, 307, 6]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-26                      [64, 32, 307, 4]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-27                      [64, 32, 307, 4]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-28                      [64, 256, 307, 4]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-29                         [64, 32, 307, 4]          --
│    │    └─nconv: 3-36                  [64, 32, 307, 4]          --
│    │    └─nconv: 3-37                  [64, 32, 307, 4]          --
│    │    └─nconv: 3-38                  [64, 32, 307, 4]          --
│    │    └─nconv: 3-39                  [64, 32, 307, 4]          --
│    │    └─nconv: 3-40                  [64, 32, 307, 4]          --
│    │    └─nconv: 3-41                  [64, 32, 307, 4]          --
│    │    └─linear: 3-42                 [64, 32, 307, 4]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-30                 [64, 32, 307, 4]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-31                      [64, 32, 307, 3]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-32                      [64, 32, 307, 3]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-33                      [64, 256, 307, 3]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-34                         [64, 32, 307, 3]          --
│    │    └─nconv: 3-43                  [64, 32, 307, 3]          --
│    │    └─nconv: 3-44                  [64, 32, 307, 3]          --
│    │    └─nconv: 3-45                  [64, 32, 307, 3]          --
│    │    └─nconv: 3-46                  [64, 32, 307, 3]          --
│    │    └─nconv: 3-47                  [64, 32, 307, 3]          --
│    │    └─nconv: 3-48                  [64, 32, 307, 3]          --
│    │    └─linear: 3-49                 [64, 32, 307, 3]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-35                 [64, 32, 307, 3]          64
├─ModuleList: 1-37                       --                        (recursive)
│    └─Conv2d: 2-36                      [64, 32, 307, 1]          2,080
├─ModuleList: 1-38                       --                        (recursive)
│    └─Conv2d: 2-37                      [64, 32, 307, 1]          2,080
├─ModuleList: 1-39                       --                        (recursive)
│    └─Conv2d: 2-38                      [64, 256, 307, 1]         8,448
├─ModuleList: 1-40                       --                        (recursive)
│    └─gcn: 2-39                         [64, 32, 307, 1]          --
│    │    └─nconv: 3-50                  [64, 32, 307, 1]          --
│    │    └─nconv: 3-51                  [64, 32, 307, 1]          --
│    │    └─nconv: 3-52                  [64, 32, 307, 1]          --
│    │    └─nconv: 3-53                  [64, 32, 307, 1]          --
│    │    └─nconv: 3-54                  [64, 32, 307, 1]          --
│    │    └─nconv: 3-55                  [64, 32, 307, 1]          --
│    │    └─linear: 3-56                 [64, 32, 307, 1]          7,200
├─ModuleList: 1-41                       --                        (recursive)
│    └─BatchNorm2d: 2-40                 [64, 32, 307, 1]          64
├─Conv2d: 1-42                           [64, 512, 307, 1]         131,584
├─Conv2d: 1-43                           [64, 12, 307, 1]          6,156
==========================================================================================
Total params: 311,368
Trainable params: 311,368
Non-trainable params: 0
Total mult-adds (G): 22.96
==========================================================================================
Input size (MB): 0.94
Forward/backward pass size (MB): 3286.40
Params size (MB): 1.19
Estimated Total Size (MB): 3288.53
==========================================================================================

Loss: HuberLoss

CL target length = 1
2023-03-26 09:19:53.388287 Epoch 1  	Train Loss = 25.11127 Val Loss = 123.40421
2023-03-26 09:20:09.601971 Epoch 2  	Train Loss = 19.90064 Val Loss = 123.26597
2023-03-26 09:20:25.841471 Epoch 3  	Train Loss = 18.22436 Val Loss = 123.17736
2023-03-26 09:20:42.059862 Epoch 4  	Train Loss = 17.88760 Val Loss = 123.26090
2023-03-26 09:20:58.297492 Epoch 5  	Train Loss = 17.82098 Val Loss = 123.15121
2023-03-26 09:21:14.548181 Epoch 6  	Train Loss = 17.15831 Val Loss = 123.17610
2023-03-26 09:21:30.831114 Epoch 7  	Train Loss = 17.15814 Val Loss = 123.18715
2023-03-26 09:21:47.103800 Epoch 8  	Train Loss = 17.09642 Val Loss = 123.13599
2023-03-26 09:22:03.416113 Epoch 9  	Train Loss = 16.91128 Val Loss = 123.12958
2023-03-26 09:22:19.672769 Epoch 10  	Train Loss = 16.84054 Val Loss = 123.12454
2023-03-26 09:22:35.934923 Epoch 11  	Train Loss = 16.78624 Val Loss = 123.12256
2023-03-26 09:22:52.212596 Epoch 12  	Train Loss = 16.77237 Val Loss = 123.12828
2023-03-26 09:23:08.483128 Epoch 13  	Train Loss = 16.74659 Val Loss = 123.13017
2023-03-26 09:23:24.804369 Epoch 14  	Train Loss = 16.63028 Val Loss = 123.11082
2023-03-26 09:23:41.132232 Epoch 15  	Train Loss = 16.65190 Val Loss = 123.10818
CL target length = 2
2023-03-26 09:23:57.434038 Epoch 16  	Train Loss = 18.70346 Val Loss = 113.67366
2023-03-26 09:24:13.727241 Epoch 17  	Train Loss = 17.36866 Val Loss = 113.62672
2023-03-26 09:24:30.086484 Epoch 18  	Train Loss = 17.29355 Val Loss = 113.70494
2023-03-26 09:24:46.390935 Epoch 19  	Train Loss = 17.20565 Val Loss = 113.61316
2023-03-26 09:25:02.681537 Epoch 20  	Train Loss = 17.12349 Val Loss = 113.60198
2023-03-26 09:25:18.966945 Epoch 21  	Train Loss = 17.02836 Val Loss = 113.70886
2023-03-26 09:25:35.332956 Epoch 22  	Train Loss = 17.07165 Val Loss = 113.56787
2023-03-26 09:25:51.808491 Epoch 23  	Train Loss = 17.03793 Val Loss = 113.54578
2023-03-26 09:26:08.265487 Epoch 24  	Train Loss = 16.97390 Val Loss = 113.64588
2023-03-26 09:26:24.717651 Epoch 25  	Train Loss = 16.94096 Val Loss = 113.54340
2023-03-26 09:26:41.185431 Epoch 26  	Train Loss = 17.00186 Val Loss = 113.53954
2023-03-26 09:26:57.649265 Epoch 27  	Train Loss = 16.89742 Val Loss = 113.55131
2023-03-26 09:27:14.100972 Epoch 28  	Train Loss = 16.87897 Val Loss = 113.52174
2023-03-26 09:27:30.581380 Epoch 29  	Train Loss = 16.83593 Val Loss = 113.52590
2023-03-26 09:27:47.045994 Epoch 30  	Train Loss = 16.80913 Val Loss = 113.52348
2023-03-26 09:28:03.406873 Epoch 31  	Train Loss = 16.76127 Val Loss = 113.53136
CL target length = 3
2023-03-26 09:28:19.701255 Epoch 32  	Train Loss = 18.53414 Val Loss = 104.16466
2023-03-26 09:28:35.993028 Epoch 33  	Train Loss = 17.31227 Val Loss = 104.02884
2023-03-26 09:28:52.263433 Epoch 34  	Train Loss = 17.25744 Val Loss = 104.07441
2023-03-26 09:29:08.534287 Epoch 35  	Train Loss = 17.27785 Val Loss = 104.15492
2023-03-26 09:29:24.797356 Epoch 36  	Train Loss = 17.18124 Val Loss = 104.06490
2023-03-26 09:29:41.060537 Epoch 37  	Train Loss = 17.17435 Val Loss = 104.00493
2023-03-26 09:29:57.326660 Epoch 38  	Train Loss = 17.09441 Val Loss = 103.98398
2023-03-26 09:30:13.620745 Epoch 39  	Train Loss = 17.09145 Val Loss = 104.03246
2023-03-26 09:30:30.025853 Epoch 40  	Train Loss = 17.10282 Val Loss = 104.01779
2023-03-26 09:30:46.469536 Epoch 41  	Train Loss = 17.06018 Val Loss = 103.99000
2023-03-26 09:31:02.908573 Epoch 42  	Train Loss = 17.00987 Val Loss = 104.03194
2023-03-26 09:31:19.346036 Epoch 43  	Train Loss = 17.01935 Val Loss = 103.95881
2023-03-26 09:31:35.597157 Epoch 44  	Train Loss = 16.93763 Val Loss = 103.94479
2023-03-26 09:31:51.849132 Epoch 45  	Train Loss = 16.95520 Val Loss = 104.00400
2023-03-26 09:32:08.111780 Epoch 46  	Train Loss = 16.92225 Val Loss = 103.96692
2023-03-26 09:32:24.352864 Epoch 47  	Train Loss = 16.89450 Val Loss = 103.93980
CL target length = 4
2023-03-26 09:32:40.596326 Epoch 48  	Train Loss = 18.24603 Val Loss = 94.48258
2023-03-26 09:32:56.895268 Epoch 49  	Train Loss = 17.32454 Val Loss = 94.52136
2023-03-26 09:33:13.146209 Epoch 50  	Train Loss = 17.22985 Val Loss = 94.41027
2023-03-26 09:33:29.398152 Epoch 51  	Train Loss = 17.23025 Val Loss = 94.49544
2023-03-26 09:33:45.652397 Epoch 52  	Train Loss = 17.17496 Val Loss = 94.61113
2023-03-26 09:34:01.962078 Epoch 53  	Train Loss = 17.18754 Val Loss = 94.47428
2023-03-26 09:34:18.429704 Epoch 54  	Train Loss = 17.17691 Val Loss = 94.44522
2023-03-26 09:34:34.914493 Epoch 55  	Train Loss = 17.12442 Val Loss = 94.53705
2023-03-26 09:34:51.389976 Epoch 56  	Train Loss = 17.07776 Val Loss = 94.52687
2023-03-26 09:35:07.849233 Epoch 57  	Train Loss = 17.05943 Val Loss = 94.48437
2023-03-26 09:35:24.298650 Epoch 58  	Train Loss = 17.07006 Val Loss = 94.37743
2023-03-26 09:35:40.743602 Epoch 59  	Train Loss = 17.04247 Val Loss = 94.42696
2023-03-26 09:35:57.189658 Epoch 60  	Train Loss = 16.97106 Val Loss = 94.44274
2023-03-26 09:36:13.462971 Epoch 61  	Train Loss = 16.93540 Val Loss = 94.44577
2023-03-26 09:36:29.724281 Epoch 62  	Train Loss = 16.91739 Val Loss = 94.42113
CL target length = 5
2023-03-26 09:36:45.974601 Epoch 63  	Train Loss = 17.74518 Val Loss = 85.70090
2023-03-26 09:37:02.370533 Epoch 64  	Train Loss = 17.42953 Val Loss = 84.88809
2023-03-26 09:37:18.689082 Epoch 65  	Train Loss = 17.22507 Val Loss = 84.88340
2023-03-26 09:37:34.957591 Epoch 66  	Train Loss = 17.18703 Val Loss = 84.93221
2023-03-26 09:37:51.225976 Epoch 67  	Train Loss = 17.11588 Val Loss = 84.88708
2023-03-26 09:38:07.489228 Epoch 68  	Train Loss = 17.08035 Val Loss = 84.89075
2023-03-26 09:38:23.843037 Epoch 69  	Train Loss = 17.14526 Val Loss = 84.85643
2023-03-26 09:38:40.122393 Epoch 70  	Train Loss = 17.12180 Val Loss = 84.93973
2023-03-26 09:38:56.490758 Epoch 71  	Train Loss = 17.16294 Val Loss = 84.81199
2023-03-26 09:39:12.797627 Epoch 72  	Train Loss = 17.09583 Val Loss = 84.82455
2023-03-26 09:39:29.071519 Epoch 73  	Train Loss = 17.05486 Val Loss = 84.88373
2023-03-26 09:39:45.340021 Epoch 74  	Train Loss = 16.98929 Val Loss = 84.83192
2023-03-26 09:40:01.703836 Epoch 75  	Train Loss = 16.97115 Val Loss = 84.91107
2023-03-26 09:40:18.156613 Epoch 76  	Train Loss = 16.93918 Val Loss = 84.82572
2023-03-26 09:40:34.603500 Epoch 77  	Train Loss = 16.99326 Val Loss = 84.85632
2023-03-26 09:40:50.949198 Epoch 78  	Train Loss = 16.92616 Val Loss = 84.82985
CL target length = 6
2023-03-26 09:41:07.223488 Epoch 79  	Train Loss = 17.84158 Val Loss = 75.37434
2023-03-26 09:41:23.501195 Epoch 80  	Train Loss = 17.37186 Val Loss = 75.43211
2023-03-26 09:41:39.782914 Epoch 81  	Train Loss = 17.22187 Val Loss = 75.38192
2023-03-26 09:41:56.056007 Epoch 82  	Train Loss = 17.20179 Val Loss = 75.57872
2023-03-26 09:42:12.402546 Epoch 83  	Train Loss = 17.18539 Val Loss = 75.45813
2023-03-26 09:42:28.878078 Epoch 84  	Train Loss = 17.14849 Val Loss = 75.30291
2023-03-26 09:42:45.333343 Epoch 85  	Train Loss = 17.08825 Val Loss = 75.43312
2023-03-26 09:43:01.788694 Epoch 86  	Train Loss = 17.06309 Val Loss = 75.27156
2023-03-26 09:43:18.251702 Epoch 87  	Train Loss = 17.11841 Val Loss = 75.37152
2023-03-26 09:43:34.678783 Epoch 88  	Train Loss = 17.01303 Val Loss = 75.25726
2023-03-26 09:43:50.989704 Epoch 89  	Train Loss = 17.04778 Val Loss = 75.34945
2023-03-26 09:44:07.269258 Epoch 90  	Train Loss = 17.01681 Val Loss = 75.30632
2023-03-26 09:44:23.551019 Epoch 91  	Train Loss = 16.99056 Val Loss = 75.40656
2023-03-26 09:44:39.898796 Epoch 92  	Train Loss = 16.98795 Val Loss = 75.27360
2023-03-26 09:44:56.205228 Epoch 93  	Train Loss = 16.99746 Val Loss = 75.25394
2023-03-26 09:45:12.503129 Epoch 94  	Train Loss = 16.96574 Val Loss = 75.25128
CL target length = 7
2023-03-26 09:45:28.916670 Epoch 95  	Train Loss = 17.84497 Val Loss = 65.87850
2023-03-26 09:45:45.372152 Epoch 96  	Train Loss = 17.30146 Val Loss = 65.86108
2023-03-26 09:46:01.835906 Epoch 97  	Train Loss = 17.13612 Val Loss = 65.81761
2023-03-26 09:46:18.168913 Epoch 98  	Train Loss = 17.16576 Val Loss = 66.04329
2023-03-26 09:46:34.635277 Epoch 99  	Train Loss = 17.21781 Val Loss = 65.87373
2023-03-26 09:46:51.082179 Epoch 100  	Train Loss = 17.13293 Val Loss = 65.89121
2023-03-26 09:47:07.412254 Epoch 101  	Train Loss = 17.12819 Val Loss = 65.97009
2023-03-26 09:47:23.792252 Epoch 102  	Train Loss = 17.13908 Val Loss = 65.75481
2023-03-26 09:47:40.212362 Epoch 103  	Train Loss = 17.07309 Val Loss = 65.76289
2023-03-26 09:47:56.660348 Epoch 104  	Train Loss = 17.11908 Val Loss = 65.78278
2023-03-26 09:48:12.960477 Epoch 105  	Train Loss = 17.08172 Val Loss = 65.70996
2023-03-26 09:48:29.238985 Epoch 106  	Train Loss = 17.06413 Val Loss = 65.86254
2023-03-26 09:48:45.622119 Epoch 107  	Train Loss = 17.04750 Val Loss = 65.85226
2023-03-26 09:49:01.917494 Epoch 108  	Train Loss = 17.07189 Val Loss = 65.79159
2023-03-26 09:49:18.245484 Epoch 109  	Train Loss = 17.03692 Val Loss = 65.77695
2023-03-26 09:49:34.552294 Epoch 110  	Train Loss = 16.99021 Val Loss = 65.98525
CL target length = 8
2023-03-26 09:49:50.858442 Epoch 111  	Train Loss = 17.88219 Val Loss = 56.63269
2023-03-26 09:50:07.131005 Epoch 112  	Train Loss = 17.22250 Val Loss = 56.34036
2023-03-26 09:50:23.457730 Epoch 113  	Train Loss = 17.20857 Val Loss = 56.25911
2023-03-26 09:50:39.732479 Epoch 114  	Train Loss = 17.22690 Val Loss = 56.28343
2023-03-26 09:50:56.020672 Epoch 115  	Train Loss = 17.19750 Val Loss = 56.30699
2023-03-26 09:51:12.326824 Epoch 116  	Train Loss = 17.13339 Val Loss = 56.36304
2023-03-26 09:51:28.615741 Epoch 117  	Train Loss = 17.17665 Val Loss = 56.29681
2023-03-26 09:51:44.936626 Epoch 118  	Train Loss = 17.16881 Val Loss = 56.27003
2023-03-26 09:52:01.304966 Epoch 119  	Train Loss = 17.15583 Val Loss = 56.17505
2023-03-26 09:52:17.590583 Epoch 120  	Train Loss = 17.10503 Val Loss = 56.25274
2023-03-26 09:52:33.870923 Epoch 121  	Train Loss = 17.11143 Val Loss = 56.49547
2023-03-26 09:52:50.146038 Epoch 122  	Train Loss = 17.09704 Val Loss = 56.40474
2023-03-26 09:53:06.428039 Epoch 123  	Train Loss = 17.14982 Val Loss = 56.26913
2023-03-26 09:53:22.785308 Epoch 124  	Train Loss = 17.04704 Val Loss = 56.40788
2023-03-26 09:53:39.052889 Epoch 125  	Train Loss = 17.04381 Val Loss = 56.26469
CL target length = 9
2023-03-26 09:53:55.381018 Epoch 126  	Train Loss = 17.68515 Val Loss = 47.31610
2023-03-26 09:54:11.663879 Epoch 127  	Train Loss = 17.36362 Val Loss = 47.08525
2023-03-26 09:54:28.036046 Epoch 128  	Train Loss = 17.31150 Val Loss = 46.82780
2023-03-26 09:54:44.401135 Epoch 129  	Train Loss = 17.26643 Val Loss = 47.08816
2023-03-26 09:55:00.671048 Epoch 130  	Train Loss = 17.26264 Val Loss = 46.96836
2023-03-26 09:55:16.941125 Epoch 131  	Train Loss = 17.23592 Val Loss = 46.75902
2023-03-26 09:55:33.204630 Epoch 132  	Train Loss = 17.20868 Val Loss = 46.87417
2023-03-26 09:55:49.480474 Epoch 133  	Train Loss = 17.22133 Val Loss = 46.87470
2023-03-26 09:56:05.778718 Epoch 134  	Train Loss = 17.22377 Val Loss = 46.76690
2023-03-26 09:56:22.189933 Epoch 135  	Train Loss = 17.19199 Val Loss = 46.65580
2023-03-26 09:56:38.558772 Epoch 136  	Train Loss = 17.20504 Val Loss = 47.13958
2023-03-26 09:56:54.979631 Epoch 137  	Train Loss = 17.12319 Val Loss = 46.71472
2023-03-26 09:57:11.462393 Epoch 138  	Train Loss = 17.16350 Val Loss = 46.88379
2023-03-26 09:57:27.928582 Epoch 139  	Train Loss = 17.14090 Val Loss = 46.95843
2023-03-26 09:57:44.394451 Epoch 140  	Train Loss = 17.10610 Val Loss = 46.79569
2023-03-26 09:58:00.848271 Epoch 141  	Train Loss = 17.13695 Val Loss = 46.77539
CL target length = 10
2023-03-26 09:58:17.303458 Epoch 142  	Train Loss = 17.74161 Val Loss = 37.51941
2023-03-26 09:58:33.559697 Epoch 143  	Train Loss = 17.34131 Val Loss = 37.74674
2023-03-26 09:58:49.843386 Epoch 144  	Train Loss = 17.32926 Val Loss = 37.29272
2023-03-26 09:59:06.261370 Epoch 145  	Train Loss = 17.26475 Val Loss = 37.48164
2023-03-26 09:59:22.539900 Epoch 146  	Train Loss = 17.25951 Val Loss = 37.36130
2023-03-26 09:59:38.884106 Epoch 147  	Train Loss = 17.26936 Val Loss = 37.53467
2023-03-26 09:59:55.293672 Epoch 148  	Train Loss = 17.22966 Val Loss = 37.29178
2023-03-26 10:00:11.656088 Epoch 149  	Train Loss = 17.20754 Val Loss = 37.34342
2023-03-26 10:00:28.022827 Epoch 150  	Train Loss = 17.24222 Val Loss = 37.36593
2023-03-26 10:00:44.409663 Epoch 151  	Train Loss = 17.19922 Val Loss = 37.43784
2023-03-26 10:01:00.804985 Epoch 152  	Train Loss = 17.24251 Val Loss = 37.34243
2023-03-26 10:01:17.281429 Epoch 153  	Train Loss = 17.26087 Val Loss = 37.48870
2023-03-26 10:01:33.567975 Epoch 154  	Train Loss = 17.21660 Val Loss = 37.28196
2023-03-26 10:01:49.825845 Epoch 155  	Train Loss = 17.17747 Val Loss = 37.21283
2023-03-26 10:02:06.261927 Epoch 156  	Train Loss = 17.15770 Val Loss = 37.26675
2023-03-26 10:02:22.743171 Epoch 157  	Train Loss = 17.15859 Val Loss = 37.16152
CL target length = 11
2023-03-26 10:02:39.115136 Epoch 158  	Train Loss = 17.86687 Val Loss = 28.07940
2023-03-26 10:02:55.373919 Epoch 159  	Train Loss = 17.34497 Val Loss = 27.98485
2023-03-26 10:03:11.737337 Epoch 160  	Train Loss = 17.32648 Val Loss = 27.94461
2023-03-26 10:03:28.035339 Epoch 161  	Train Loss = 17.33747 Val Loss = 27.82701
2023-03-26 10:03:44.402121 Epoch 162  	Train Loss = 17.30034 Val Loss = 27.82373
2023-03-26 10:04:00.760358 Epoch 163  	Train Loss = 17.30669 Val Loss = 27.87888
2023-03-26 10:04:17.032339 Epoch 164  	Train Loss = 17.31374 Val Loss = 27.88818
2023-03-26 10:04:33.305175 Epoch 165  	Train Loss = 17.27027 Val Loss = 27.79910
2023-03-26 10:04:49.579983 Epoch 166  	Train Loss = 17.25057 Val Loss = 27.88708
2023-03-26 10:05:05.859963 Epoch 167  	Train Loss = 17.29177 Val Loss = 27.83361
2023-03-26 10:05:22.146403 Epoch 168  	Train Loss = 17.27292 Val Loss = 27.90744
2023-03-26 10:05:38.423229 Epoch 169  	Train Loss = 17.28058 Val Loss = 28.15067
2023-03-26 10:05:54.686072 Epoch 170  	Train Loss = 17.22681 Val Loss = 27.99204
2023-03-26 10:06:10.968777 Epoch 171  	Train Loss = 17.21975 Val Loss = 28.05963
2023-03-26 10:06:27.258872 Epoch 172  	Train Loss = 17.26577 Val Loss = 28.09749
CL target length = 12
2023-03-26 10:06:43.528027 Epoch 173  	Train Loss = 17.52427 Val Loss = 19.54563
2023-03-26 10:06:59.912455 Epoch 174  	Train Loss = 17.59964 Val Loss = 18.82547
2023-03-26 10:07:16.381139 Epoch 175  	Train Loss = 17.40362 Val Loss = 18.46089
2023-03-26 10:07:32.763046 Epoch 176  	Train Loss = 17.07347 Val Loss = 18.22615
2023-03-26 10:07:49.042712 Epoch 177  	Train Loss = 17.03798 Val Loss = 18.14387
2023-03-26 10:08:05.411235 Epoch 178  	Train Loss = 17.03820 Val Loss = 18.21950
2023-03-26 10:08:21.791953 Epoch 179  	Train Loss = 17.04049 Val Loss = 18.19324
2023-03-26 10:08:38.099253 Epoch 180  	Train Loss = 17.02836 Val Loss = 18.18261
2023-03-26 10:08:54.392566 Epoch 181  	Train Loss = 17.02666 Val Loss = 18.15998
2023-03-26 10:09:10.759886 Epoch 182  	Train Loss = 17.01344 Val Loss = 18.17243
2023-03-26 10:09:27.240287 Epoch 183  	Train Loss = 17.01571 Val Loss = 18.19131
2023-03-26 10:09:43.683480 Epoch 184  	Train Loss = 17.01502 Val Loss = 18.20973
2023-03-26 10:10:00.001831 Epoch 185  	Train Loss = 17.01228 Val Loss = 18.17435
2023-03-26 10:10:16.401434 Epoch 186  	Train Loss = 17.01084 Val Loss = 18.16555
2023-03-26 10:10:32.897779 Epoch 187  	Train Loss = 17.02124 Val Loss = 18.14333
2023-03-26 10:10:49.380718 Epoch 188  	Train Loss = 17.00359 Val Loss = 18.11835
2023-03-26 10:11:05.712923 Epoch 189  	Train Loss = 17.00592 Val Loss = 18.17120
2023-03-26 10:11:22.155722 Epoch 190  	Train Loss = 17.01141 Val Loss = 18.18704
2023-03-26 10:11:38.644854 Epoch 191  	Train Loss = 17.01568 Val Loss = 18.22186
2023-03-26 10:11:55.170285 Epoch 192  	Train Loss = 16.99624 Val Loss = 18.22441
2023-03-26 10:12:11.549546 Epoch 193  	Train Loss = 16.99159 Val Loss = 18.30849
2023-03-26 10:12:27.873362 Epoch 194  	Train Loss = 16.98845 Val Loss = 18.22715
2023-03-26 10:12:44.186059 Epoch 195  	Train Loss = 16.99637 Val Loss = 18.11182
2023-03-26 10:13:00.498837 Epoch 196  	Train Loss = 16.98584 Val Loss = 18.10660
2023-03-26 10:13:16.792915 Epoch 197  	Train Loss = 16.98220 Val Loss = 18.16728
2023-03-26 10:13:33.168857 Epoch 198  	Train Loss = 16.98441 Val Loss = 18.17059
2023-03-26 10:13:49.545477 Epoch 199  	Train Loss = 16.98982 Val Loss = 18.16913
2023-03-26 10:14:05.912077 Epoch 200  	Train Loss = 16.97905 Val Loss = 18.13141
2023-03-26 10:14:22.208254 Epoch 201  	Train Loss = 16.98339 Val Loss = 18.18366
2023-03-26 10:14:38.473697 Epoch 202  	Train Loss = 16.96826 Val Loss = 18.18322
2023-03-26 10:14:54.730343 Epoch 203  	Train Loss = 16.96948 Val Loss = 18.20306
2023-03-26 10:15:10.984055 Epoch 204  	Train Loss = 16.97432 Val Loss = 18.30248
2023-03-26 10:15:27.244254 Epoch 205  	Train Loss = 16.98231 Val Loss = 18.24089
2023-03-26 10:15:43.518565 Epoch 206  	Train Loss = 16.98013 Val Loss = 18.14711
2023-03-26 10:15:59.775520 Epoch 207  	Train Loss = 16.96964 Val Loss = 18.30941
2023-03-26 10:16:16.047122 Epoch 208  	Train Loss = 16.96984 Val Loss = 18.15315
2023-03-26 10:16:32.419932 Epoch 209  	Train Loss = 16.97601 Val Loss = 18.16987
2023-03-26 10:16:48.940116 Epoch 210  	Train Loss = 16.96811 Val Loss = 18.16854
2023-03-26 10:17:05.305063 Epoch 211  	Train Loss = 16.95186 Val Loss = 18.29177
Early stopping at epoch: 211
Best at epoch 196:
Train Loss = 16.98584
Train RMSE = 28.16143, MAE = 17.19591, MAPE = 12.47360
Val Loss = 18.10660
Val RMSE = 31.06227, MAE = 19.00672, MAPE = 12.36367
--------- Test ---------
All Steps RMSE = 31.33631, MAE = 19.39609, MAPE = 12.52288
Step 1 RMSE = 26.98227, MAE = 16.73455, MAPE = 10.92555
Step 2 RMSE = 28.20171, MAE = 17.47960, MAPE = 11.42510
Step 3 RMSE = 29.18399, MAE = 18.09640, MAPE = 11.77683
Step 4 RMSE = 29.92827, MAE = 18.53777, MAPE = 12.04555
Step 5 RMSE = 30.60294, MAE = 18.93323, MAPE = 12.23746
Step 6 RMSE = 31.19178, MAE = 19.29806, MAPE = 12.44098
Step 7 RMSE = 31.79123, MAE = 19.67585, MAPE = 12.65936
Step 8 RMSE = 32.32933, MAE = 20.02690, MAPE = 12.87199
Step 9 RMSE = 32.88910, MAE = 20.40175, MAPE = 13.10042
Step 10 RMSE = 33.41668, MAE = 20.77645, MAPE = 13.31500
Step 11 RMSE = 33.95563, MAE = 21.15569, MAPE = 13.58066
Step 12 RMSE = 34.58454, MAE = 21.63699, MAPE = 13.89555
Inference time: 1.56 s
