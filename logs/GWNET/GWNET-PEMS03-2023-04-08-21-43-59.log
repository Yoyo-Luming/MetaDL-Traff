PEMS03
Trainset:	x-(15711, 12, 358, 2)	y-(15711, 12, 358, 1)
Valset:  	x-(5237, 12, 358, 2)  	y-(5237, 12, 358, 1)
Testset:	x-(5237, 12, 358, 2)	y-(5237, 12, 358, 1)

--------- GWNET ---------
{
    "num_nodes": 358,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        115
    ],
    "clip_grad": false,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": true,
    "cl_step_size": 2500,
    "pass_device": true,
    "model_args": {
        "num_nodes": 358,
        "in_dim": 2,
        "out_dim": 12,
        "adj_path": "../data/PEMS03/adj_PEMS03_distance.pkl",
        "adj_type": "doubletransition",
        "device": "cuda:0",
        "dropout": 0.3,
        "gcn_bool": true,
        "addaptadj": true,
        "aptinit": null
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
GWNET                                    --                        --
├─ModuleList: 1-1                        --                        --
├─ModuleList: 1-2                        --                        --
├─ModuleList: 1-3                        --                        --
├─ModuleList: 1-4                        --                        --
├─ModuleList: 1-5                        --                        --
├─ModuleList: 1-6                        --                        --
├─Conv2d: 1-7                            [64, 32, 358, 13]         96
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-1                       [64, 32, 358, 12]         2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-2                       [64, 32, 358, 12]         2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-3                       [64, 256, 358, 12]        8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-4                          [64, 32, 358, 12]         --
│    │    └─nconv: 3-1                   [64, 32, 358, 12]         --
│    │    └─nconv: 3-2                   [64, 32, 358, 12]         --
│    │    └─nconv: 3-3                   [64, 32, 358, 12]         --
│    │    └─nconv: 3-4                   [64, 32, 358, 12]         --
│    │    └─nconv: 3-5                   [64, 32, 358, 12]         --
│    │    └─nconv: 3-6                   [64, 32, 358, 12]         --
│    │    └─linear: 3-7                  [64, 32, 358, 12]         7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-5                  [64, 32, 358, 12]         64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-6                       [64, 32, 358, 10]         2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-7                       [64, 32, 358, 10]         2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-8                       [64, 256, 358, 10]        8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-9                          [64, 32, 358, 10]         --
│    │    └─nconv: 3-8                   [64, 32, 358, 10]         --
│    │    └─nconv: 3-9                   [64, 32, 358, 10]         --
│    │    └─nconv: 3-10                  [64, 32, 358, 10]         --
│    │    └─nconv: 3-11                  [64, 32, 358, 10]         --
│    │    └─nconv: 3-12                  [64, 32, 358, 10]         --
│    │    └─nconv: 3-13                  [64, 32, 358, 10]         --
│    │    └─linear: 3-14                 [64, 32, 358, 10]         7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-10                 [64, 32, 358, 10]         64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-11                      [64, 32, 358, 9]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-12                      [64, 32, 358, 9]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-13                      [64, 256, 358, 9]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-14                         [64, 32, 358, 9]          --
│    │    └─nconv: 3-15                  [64, 32, 358, 9]          --
│    │    └─nconv: 3-16                  [64, 32, 358, 9]          --
│    │    └─nconv: 3-17                  [64, 32, 358, 9]          --
│    │    └─nconv: 3-18                  [64, 32, 358, 9]          --
│    │    └─nconv: 3-19                  [64, 32, 358, 9]          --
│    │    └─nconv: 3-20                  [64, 32, 358, 9]          --
│    │    └─linear: 3-21                 [64, 32, 358, 9]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-15                 [64, 32, 358, 9]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-16                      [64, 32, 358, 7]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-17                      [64, 32, 358, 7]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-18                      [64, 256, 358, 7]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-19                         [64, 32, 358, 7]          --
│    │    └─nconv: 3-22                  [64, 32, 358, 7]          --
│    │    └─nconv: 3-23                  [64, 32, 358, 7]          --
│    │    └─nconv: 3-24                  [64, 32, 358, 7]          --
│    │    └─nconv: 3-25                  [64, 32, 358, 7]          --
│    │    └─nconv: 3-26                  [64, 32, 358, 7]          --
│    │    └─nconv: 3-27                  [64, 32, 358, 7]          --
│    │    └─linear: 3-28                 [64, 32, 358, 7]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-20                 [64, 32, 358, 7]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-21                      [64, 32, 358, 6]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-22                      [64, 32, 358, 6]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-23                      [64, 256, 358, 6]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-24                         [64, 32, 358, 6]          --
│    │    └─nconv: 3-29                  [64, 32, 358, 6]          --
│    │    └─nconv: 3-30                  [64, 32, 358, 6]          --
│    │    └─nconv: 3-31                  [64, 32, 358, 6]          --
│    │    └─nconv: 3-32                  [64, 32, 358, 6]          --
│    │    └─nconv: 3-33                  [64, 32, 358, 6]          --
│    │    └─nconv: 3-34                  [64, 32, 358, 6]          --
│    │    └─linear: 3-35                 [64, 32, 358, 6]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-25                 [64, 32, 358, 6]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-26                      [64, 32, 358, 4]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-27                      [64, 32, 358, 4]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-28                      [64, 256, 358, 4]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-29                         [64, 32, 358, 4]          --
│    │    └─nconv: 3-36                  [64, 32, 358, 4]          --
│    │    └─nconv: 3-37                  [64, 32, 358, 4]          --
│    │    └─nconv: 3-38                  [64, 32, 358, 4]          --
│    │    └─nconv: 3-39                  [64, 32, 358, 4]          --
│    │    └─nconv: 3-40                  [64, 32, 358, 4]          --
│    │    └─nconv: 3-41                  [64, 32, 358, 4]          --
│    │    └─linear: 3-42                 [64, 32, 358, 4]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-30                 [64, 32, 358, 4]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-31                      [64, 32, 358, 3]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-32                      [64, 32, 358, 3]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-33                      [64, 256, 358, 3]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-34                         [64, 32, 358, 3]          --
│    │    └─nconv: 3-43                  [64, 32, 358, 3]          --
│    │    └─nconv: 3-44                  [64, 32, 358, 3]          --
│    │    └─nconv: 3-45                  [64, 32, 358, 3]          --
│    │    └─nconv: 3-46                  [64, 32, 358, 3]          --
│    │    └─nconv: 3-47                  [64, 32, 358, 3]          --
│    │    └─nconv: 3-48                  [64, 32, 358, 3]          --
│    │    └─linear: 3-49                 [64, 32, 358, 3]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-35                 [64, 32, 358, 3]          64
├─ModuleList: 1-1                        --                        --
│    └─Conv2d: 2-36                      [64, 32, 358, 1]          2,080
├─ModuleList: 1-2                        --                        --
│    └─Conv2d: 2-37                      [64, 32, 358, 1]          2,080
├─ModuleList: 1-4                        --                        --
│    └─Conv2d: 2-38                      [64, 256, 358, 1]         8,448
├─ModuleList: 1-6                        --                        --
│    └─gcn: 2-39                         [64, 32, 358, 1]          --
│    │    └─nconv: 3-50                  [64, 32, 358, 1]          --
│    │    └─nconv: 3-51                  [64, 32, 358, 1]          --
│    │    └─nconv: 3-52                  [64, 32, 358, 1]          --
│    │    └─nconv: 3-53                  [64, 32, 358, 1]          --
│    │    └─nconv: 3-54                  [64, 32, 358, 1]          --
│    │    └─nconv: 3-55                  [64, 32, 358, 1]          --
│    │    └─linear: 3-56                 [64, 32, 358, 1]          7,200
├─ModuleList: 1-5                        --                        --
│    └─BatchNorm2d: 2-40                 [64, 32, 358, 1]          64
├─Conv2d: 1-8                            [64, 512, 358, 1]         131,584
├─Conv2d: 1-9                            [64, 12, 358, 1]          6,156
==========================================================================================
Total params: 296,812
Trainable params: 296,812
Non-trainable params: 0
Total mult-adds (G): 26.78
==========================================================================================
Input size (MB): 2.20
Forward/backward pass size (MB): 3832.35
Params size (MB): 1.19
Estimated Total Size (MB): 3835.74
==========================================================================================

Loss: HuberLoss

CL target length = 1
2023-04-08 21:44:49.094403 Epoch 1  	Train Loss = 19.50249 Val Loss = 111.87111
2023-04-08 21:45:36.393671 Epoch 2  	Train Loss = 14.78549 Val Loss = 111.77661
2023-04-08 21:46:23.902383 Epoch 3  	Train Loss = 13.68642 Val Loss = 111.85239
2023-04-08 21:47:12.051744 Epoch 4  	Train Loss = 13.34157 Val Loss = 111.81717
2023-04-08 21:48:00.664077 Epoch 5  	Train Loss = 13.18715 Val Loss = 111.80905
2023-04-08 21:48:49.606168 Epoch 6  	Train Loss = 13.01860 Val Loss = 111.72612
2023-04-08 21:49:38.691988 Epoch 7  	Train Loss = 12.85869 Val Loss = 111.75770
2023-04-08 21:50:27.868524 Epoch 8  	Train Loss = 12.76868 Val Loss = 111.71483
2023-04-08 21:51:17.271488 Epoch 9  	Train Loss = 12.64664 Val Loss = 111.69517
2023-04-08 21:52:06.761587 Epoch 10  	Train Loss = 12.46608 Val Loss = 111.68456
CL target length = 2
2023-04-08 21:52:56.315238 Epoch 11  	Train Loss = 14.38876 Val Loss = 102.74036
2023-04-08 21:53:46.117639 Epoch 12  	Train Loss = 13.10626 Val Loss = 102.76464
2023-04-08 21:54:36.106200 Epoch 13  	Train Loss = 13.05012 Val Loss = 102.72204
2023-04-08 21:55:26.583170 Epoch 14  	Train Loss = 12.92740 Val Loss = 102.69613
2023-04-08 21:56:17.631111 Epoch 15  	Train Loss = 12.87265 Val Loss = 102.71631
2023-04-08 21:57:09.002050 Epoch 16  	Train Loss = 12.84016 Val Loss = 102.68644
2023-04-08 21:58:00.456466 Epoch 17  	Train Loss = 12.78451 Val Loss = 102.70971
2023-04-08 21:58:51.515171 Epoch 18  	Train Loss = 12.75636 Val Loss = 102.66824
2023-04-08 21:59:41.403178 Epoch 19  	Train Loss = 12.68076 Val Loss = 102.66046
2023-04-08 22:00:30.425603 Epoch 20  	Train Loss = 12.74077 Val Loss = 102.67946
CL target length = 3
2023-04-08 22:01:19.378850 Epoch 21  	Train Loss = 13.89626 Val Loss = 93.80293
2023-04-08 22:02:08.365724 Epoch 22  	Train Loss = 13.16552 Val Loss = 93.74394
2023-04-08 22:02:57.441147 Epoch 23  	Train Loss = 13.03340 Val Loss = 93.74585
2023-04-08 22:03:46.589799 Epoch 24  	Train Loss = 13.02848 Val Loss = 93.75665
2023-04-08 22:04:35.823778 Epoch 25  	Train Loss = 13.01021 Val Loss = 93.71366
2023-04-08 22:05:25.150606 Epoch 26  	Train Loss = 12.91322 Val Loss = 93.71656
2023-04-08 22:06:14.627024 Epoch 27  	Train Loss = 12.89091 Val Loss = 93.71676
2023-04-08 22:07:04.290269 Epoch 28  	Train Loss = 12.91781 Val Loss = 93.67545
2023-04-08 22:07:54.079823 Epoch 29  	Train Loss = 12.83412 Val Loss = 93.72668
2023-04-08 22:08:43.993898 Epoch 30  	Train Loss = 12.80956 Val Loss = 93.69735
CL target length = 4
2023-04-08 22:09:34.394630 Epoch 31  	Train Loss = 13.75367 Val Loss = 84.81678
2023-04-08 22:10:25.356797 Epoch 32  	Train Loss = 13.15452 Val Loss = 84.84257
2023-04-08 22:11:16.666960 Epoch 33  	Train Loss = 13.10858 Val Loss = 84.79274
2023-04-08 22:12:08.138282 Epoch 34  	Train Loss = 13.06886 Val Loss = 84.77533
2023-04-08 22:12:59.550776 Epoch 35  	Train Loss = 13.05939 Val Loss = 84.73165
2023-04-08 22:13:50.011996 Epoch 36  	Train Loss = 12.97074 Val Loss = 84.76977
2023-04-08 22:14:39.348148 Epoch 37  	Train Loss = 12.92762 Val Loss = 84.80459
2023-04-08 22:15:28.291509 Epoch 38  	Train Loss = 12.93756 Val Loss = 84.79082
2023-04-08 22:16:17.190097 Epoch 39  	Train Loss = 12.90626 Val Loss = 84.74861
2023-04-08 22:17:06.172221 Epoch 40  	Train Loss = 12.86093 Val Loss = 84.75815
CL target length = 5
2023-04-08 22:17:55.304451 Epoch 41  	Train Loss = 13.52524 Val Loss = 76.09462
2023-04-08 22:18:44.414963 Epoch 42  	Train Loss = 13.13741 Val Loss = 75.82730
2023-04-08 22:19:33.625005 Epoch 43  	Train Loss = 13.08488 Val Loss = 75.88559
2023-04-08 22:20:22.992780 Epoch 44  	Train Loss = 13.04877 Val Loss = 75.82095
2023-04-08 22:21:12.462086 Epoch 45  	Train Loss = 13.02543 Val Loss = 75.79151
2023-04-08 22:22:02.113241 Epoch 46  	Train Loss = 12.97943 Val Loss = 75.81770
2023-04-08 22:22:52.006483 Epoch 47  	Train Loss = 12.95511 Val Loss = 75.96390
2023-04-08 22:23:41.983090 Epoch 48  	Train Loss = 12.90065 Val Loss = 75.87195
2023-04-08 22:24:32.050844 Epoch 49  	Train Loss = 12.94299 Val Loss = 75.86003
2023-04-08 22:25:21.808235 Epoch 50  	Train Loss = 12.90629 Val Loss = 75.84370
CL target length = 6
2023-04-08 22:26:11.418046 Epoch 51  	Train Loss = 13.38947 Val Loss = 66.98512
2023-04-08 22:27:00.940385 Epoch 52  	Train Loss = 13.16804 Val Loss = 66.94630
2023-04-08 22:27:50.311630 Epoch 53  	Train Loss = 13.05710 Val Loss = 66.85352
2023-04-08 22:28:39.695621 Epoch 54  	Train Loss = 13.03759 Val Loss = 66.85878
2023-04-08 22:29:28.941695 Epoch 55  	Train Loss = 12.99242 Val Loss = 66.83453
2023-04-08 22:30:18.117753 Epoch 56  	Train Loss = 12.93902 Val Loss = 66.95903
2023-04-08 22:31:07.289954 Epoch 57  	Train Loss = 13.00717 Val Loss = 66.84953
2023-04-08 22:31:56.518942 Epoch 58  	Train Loss = 12.94655 Val Loss = 66.95188
2023-04-08 22:32:45.997725 Epoch 59  	Train Loss = 12.92775 Val Loss = 66.95505
2023-04-08 22:33:35.859698 Epoch 60  	Train Loss = 12.89824 Val Loss = 66.91926
CL target length = 7
2023-04-08 22:34:25.863211 Epoch 61  	Train Loss = 13.14960 Val Loss = 58.59343
2023-04-08 22:35:15.894872 Epoch 62  	Train Loss = 13.26993 Val Loss = 58.14996
2023-04-08 22:36:06.126045 Epoch 63  	Train Loss = 13.06229 Val Loss = 57.94722
2023-04-08 22:36:56.642535 Epoch 64  	Train Loss = 13.04203 Val Loss = 57.98091
2023-04-08 22:37:46.688845 Epoch 65  	Train Loss = 13.04055 Val Loss = 57.99984
2023-04-08 22:38:36.340694 Epoch 66  	Train Loss = 13.00916 Val Loss = 57.98183
2023-04-08 22:39:25.950877 Epoch 67  	Train Loss = 12.99779 Val Loss = 57.87561
2023-04-08 22:40:16.518877 Epoch 68  	Train Loss = 12.99450 Val Loss = 57.98600
2023-04-08 22:41:07.676069 Epoch 69  	Train Loss = 12.97836 Val Loss = 57.95113
2023-04-08 22:41:59.064527 Epoch 70  	Train Loss = 12.91460 Val Loss = 57.89637
2023-04-08 22:42:50.406790 Epoch 71  	Train Loss = 12.88643 Val Loss = 57.92806
CL target length = 8
2023-04-08 22:43:41.743306 Epoch 72  	Train Loss = 13.44460 Val Loss = 49.08711
2023-04-08 22:44:32.946898 Epoch 73  	Train Loss = 13.07851 Val Loss = 49.04087
2023-04-08 22:45:24.049538 Epoch 74  	Train Loss = 13.07988 Val Loss = 49.15919
2023-04-08 22:46:15.136300 Epoch 75  	Train Loss = 13.04298 Val Loss = 49.02724
2023-04-08 22:47:06.246316 Epoch 76  	Train Loss = 13.02315 Val Loss = 49.09238
2023-04-08 22:47:57.465170 Epoch 77  	Train Loss = 13.00252 Val Loss = 49.04209
2023-04-08 22:48:49.003676 Epoch 78  	Train Loss = 12.96164 Val Loss = 48.97347
2023-04-08 22:49:40.662892 Epoch 79  	Train Loss = 12.97203 Val Loss = 48.99884
2023-04-08 22:50:32.367539 Epoch 80  	Train Loss = 12.96479 Val Loss = 49.04087
2023-04-08 22:51:24.098929 Epoch 81  	Train Loss = 12.92638 Val Loss = 49.09421
CL target length = 9
2023-04-08 22:52:15.624052 Epoch 82  	Train Loss = 13.37867 Val Loss = 40.16171
2023-04-08 22:53:06.209405 Epoch 83  	Train Loss = 13.06896 Val Loss = 40.02222
2023-04-08 22:53:56.088539 Epoch 84  	Train Loss = 13.04766 Val Loss = 40.29474
2023-04-08 22:54:46.033918 Epoch 85  	Train Loss = 13.05812 Val Loss = 40.10413
2023-04-08 22:55:36.168218 Epoch 86  	Train Loss = 13.04561 Val Loss = 40.18316
2023-04-08 22:56:26.544902 Epoch 87  	Train Loss = 13.00181 Val Loss = 40.07222
2023-04-08 22:57:16.843910 Epoch 88  	Train Loss = 13.02006 Val Loss = 40.18978
2023-04-08 22:58:07.323835 Epoch 89  	Train Loss = 12.98083 Val Loss = 40.07985
2023-04-08 22:58:57.900849 Epoch 90  	Train Loss = 12.97154 Val Loss = 40.12810
2023-04-08 22:59:48.612782 Epoch 91  	Train Loss = 12.94365 Val Loss = 40.06147
CL target length = 10
2023-04-08 23:00:39.545025 Epoch 92  	Train Loss = 13.39925 Val Loss = 31.42302
2023-04-08 23:01:30.747649 Epoch 93  	Train Loss = 13.06963 Val Loss = 31.27364
2023-04-08 23:02:22.128437 Epoch 94  	Train Loss = 13.05022 Val Loss = 31.27306
2023-04-08 23:03:14.006879 Epoch 95  	Train Loss = 13.05477 Val Loss = 31.51182
2023-04-08 23:04:06.499545 Epoch 96  	Train Loss = 13.05285 Val Loss = 31.33035
2023-04-08 23:04:58.524582 Epoch 97  	Train Loss = 13.01476 Val Loss = 31.30322
2023-04-08 23:05:50.337208 Epoch 98  	Train Loss = 12.96199 Val Loss = 31.18592
2023-04-08 23:06:41.986421 Epoch 99  	Train Loss = 12.98737 Val Loss = 31.42609
2023-04-08 23:07:32.967811 Epoch 100  	Train Loss = 12.97204 Val Loss = 31.17713
2023-04-08 23:08:23.361583 Epoch 101  	Train Loss = 12.97690 Val Loss = 31.23648
CL target length = 11
2023-04-08 23:09:13.900335 Epoch 102  	Train Loss = 13.36464 Val Loss = 22.51463
2023-04-08 23:10:04.637260 Epoch 103  	Train Loss = 13.07655 Val Loss = 22.39797
2023-04-08 23:10:55.449294 Epoch 104  	Train Loss = 13.08243 Val Loss = 22.41448
2023-04-08 23:11:46.335496 Epoch 105  	Train Loss = 13.05408 Val Loss = 22.34281
2023-04-08 23:12:37.264109 Epoch 106  	Train Loss = 12.99996 Val Loss = 22.33748
2023-04-08 23:13:28.144732 Epoch 107  	Train Loss = 13.03162 Val Loss = 22.58216
2023-04-08 23:14:19.067457 Epoch 108  	Train Loss = 13.01193 Val Loss = 22.66808
2023-04-08 23:15:10.317840 Epoch 109  	Train Loss = 13.00909 Val Loss = 22.47167
2023-04-08 23:16:01.734054 Epoch 110  	Train Loss = 13.01089 Val Loss = 22.36113
2023-04-08 23:16:53.263522 Epoch 111  	Train Loss = 12.99274 Val Loss = 22.21679
CL target length = 12
2023-04-08 23:17:45.323282 Epoch 112  	Train Loss = 13.26304 Val Loss = 13.55337
2023-04-08 23:18:37.884207 Epoch 113  	Train Loss = 13.08284 Val Loss = 13.42879
2023-04-08 23:19:29.914152 Epoch 114  	Train Loss = 13.05852 Val Loss = 13.36611
2023-04-08 23:20:21.680240 Epoch 115  	Train Loss = 13.05986 Val Loss = 13.42504
2023-04-08 23:21:13.152803 Epoch 116  	Train Loss = 12.78638 Val Loss = 13.24735
2023-04-08 23:22:03.731495 Epoch 117  	Train Loss = 12.75600 Val Loss = 13.23834
2023-04-08 23:22:53.386489 Epoch 118  	Train Loss = 12.75008 Val Loss = 13.26344
2023-04-08 23:23:42.739027 Epoch 119  	Train Loss = 12.74763 Val Loss = 13.24689
2023-04-08 23:24:32.087599 Epoch 120  	Train Loss = 12.74746 Val Loss = 13.24688
2023-04-08 23:25:21.458143 Epoch 121  	Train Loss = 12.73596 Val Loss = 13.24183
2023-04-08 23:26:10.741201 Epoch 122  	Train Loss = 12.74231 Val Loss = 13.25008
2023-04-08 23:27:00.125017 Epoch 123  	Train Loss = 12.73167 Val Loss = 13.23541
2023-04-08 23:27:49.475760 Epoch 124  	Train Loss = 12.73500 Val Loss = 13.21876
2023-04-08 23:28:38.852608 Epoch 125  	Train Loss = 12.72716 Val Loss = 13.24587
2023-04-08 23:29:28.360338 Epoch 126  	Train Loss = 12.72832 Val Loss = 13.22385
2023-04-08 23:30:18.146169 Epoch 127  	Train Loss = 12.72447 Val Loss = 13.21363
2023-04-08 23:31:08.243587 Epoch 128  	Train Loss = 12.71853 Val Loss = 13.27244
2023-04-08 23:31:58.818150 Epoch 129  	Train Loss = 12.72317 Val Loss = 13.24673
2023-04-08 23:32:50.079658 Epoch 130  	Train Loss = 12.72110 Val Loss = 13.23200
2023-04-08 23:33:41.143394 Epoch 131  	Train Loss = 12.71878 Val Loss = 13.25412
2023-04-08 23:34:31.663108 Epoch 132  	Train Loss = 12.71426 Val Loss = 13.24546
2023-04-08 23:35:21.950837 Epoch 133  	Train Loss = 12.70387 Val Loss = 13.21785
2023-04-08 23:36:11.553539 Epoch 134  	Train Loss = 12.70902 Val Loss = 13.26503
2023-04-08 23:37:00.552380 Epoch 135  	Train Loss = 12.70874 Val Loss = 13.23400
2023-04-08 23:37:49.412269 Epoch 136  	Train Loss = 12.70563 Val Loss = 13.22425
2023-04-08 23:38:38.410839 Epoch 137  	Train Loss = 12.70675 Val Loss = 13.23088
Early stopping at epoch: 137
Best at epoch 127:
Train Loss = 12.72447
Train RMSE = 20.96662, MAE = 12.93656, MAPE = 12.19115
Val Loss = 13.21363
Val RMSE = 21.99861, MAE = 13.75756, MAPE = 13.16702
--------- Test ---------
All Steps RMSE = 25.20895, MAE = 14.41514, MAPE = 14.44501
Step 1 RMSE = 19.92954, MAE = 11.77150, MAPE = 11.87714
Step 2 RMSE = 21.96474, MAE = 12.72781, MAPE = 12.92591
Step 3 RMSE = 23.20511, MAE = 13.33663, MAPE = 13.71873
Step 4 RMSE = 24.07569, MAE = 13.76784, MAPE = 13.90316
Step 5 RMSE = 24.75044, MAE = 14.12151, MAPE = 14.19859
Step 6 RMSE = 25.36663, MAE = 14.45161, MAPE = 14.41045
Step 7 RMSE = 25.92472, MAE = 14.78256, MAPE = 14.78928
Step 8 RMSE = 26.43992, MAE = 15.09632, MAPE = 15.15807
Step 9 RMSE = 26.81891, MAE = 15.35001, MAPE = 15.34356
Step 10 RMSE = 27.17719, MAE = 15.57791, MAPE = 15.57928
Step 11 RMSE = 27.55012, MAE = 15.83265, MAPE = 15.67625
Step 12 RMSE = 28.00105, MAE = 16.16531, MAPE = 15.75946
Inference time: 4.58 s
