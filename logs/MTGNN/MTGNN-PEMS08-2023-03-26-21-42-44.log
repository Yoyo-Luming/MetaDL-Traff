PEMS08
Original data shape (17856, 170, 3)
Trainset:	x-(10690, 12, 170, 1)	y-(10690, 12, 170, 1)
Valset:  	x-(3548, 12, 170, 1)  	y-(3548, 12, 170, 1)
Testset:	x-(3549, 12, 170, 1)	y-(3549, 12, 170, 1)

--------- MTGNN ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        168
    ],
    "early_stop": 15,
    "clip_grad": 5,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": true,
    "cl_step_size": 2500,
    "load_npz": false,
    "pass_device": true,
    "model_args": {
        "num_nodes": 170,
        "in_dim": 1,
        "seq_length": 12,
        "out_dim": 12,
        "device": "cuda:0",
        "gcn_true": true,
        "buildA_true": true,
        "gcn_depth": 2,
        "predefined_A": null,
        "static_feat": null,
        "dropout": 0.3,
        "subgraph_size": 20,
        "node_dim": 40,
        "dilation_exponential": 1,
        "conv_channels": 32,
        "residual_channels": 32,
        "skip_channels": 64,
        "end_channels": 128,
        "layers": 3,
        "propalpha": 0.05,
        "tanhalpha": 3,
        "layer_norm_affline": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MTGNN                                    [64, 12, 170, 1]          3,168
├─graph_constructor: 1-1                 [170, 170]                --
│    └─Embedding: 2-1                    [170, 40]                 6,800
│    └─Embedding: 2-2                    [170, 40]                 6,800
│    └─Linear: 2-3                       [170, 40]                 1,640
│    └─Linear: 2-4                       [170, 40]                 1,640
├─Conv2d: 1-2                            [64, 32, 170, 19]         64
├─Conv2d: 1-3                            [64, 64, 170, 1]          1,280
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-5            [64, 32, 170, 13]         --
│    │    └─ModuleList: 3-1              --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-6            [64, 32, 170, 13]         --
│    │    └─ModuleList: 3-2              --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-7                       [64, 64, 170, 1]          26,688
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-8                      [64, 32, 170, 13]         --
│    │    └─nconv: 3-3                   [64, 32, 170, 13]         --
│    │    └─nconv: 3-4                   [64, 32, 170, 13]         --
│    │    └─linear: 3-5                  [64, 32, 170, 13]         3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-9                      [64, 32, 170, 13]         --
│    │    └─nconv: 3-6                   [64, 32, 170, 13]         --
│    │    └─nconv: 3-7                   [64, 32, 170, 13]         --
│    │    └─linear: 3-8                  [64, 32, 170, 13]         3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-10                   [64, 32, 170, 13]         141,440
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-11           [64, 32, 170, 7]          --
│    │    └─ModuleList: 3-9              --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-12           [64, 32, 170, 7]          --
│    │    └─ModuleList: 3-10             --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-13                      [64, 64, 170, 1]          14,400
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-14                     [64, 32, 170, 7]          --
│    │    └─nconv: 3-11                  [64, 32, 170, 7]          --
│    │    └─nconv: 3-12                  [64, 32, 170, 7]          --
│    │    └─linear: 3-13                 [64, 32, 170, 7]          3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-15                     [64, 32, 170, 7]          --
│    │    └─nconv: 3-14                  [64, 32, 170, 7]          --
│    │    └─nconv: 3-15                  [64, 32, 170, 7]          --
│    │    └─linear: 3-16                 [64, 32, 170, 7]          3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-16                   [64, 32, 170, 7]          76,160
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-17           [64, 32, 170, 1]          --
│    │    └─ModuleList: 3-17             --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-18           [64, 32, 170, 1]          --
│    │    └─ModuleList: 3-18             --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-19                      [64, 64, 170, 1]          2,112
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-20                     [64, 32, 170, 1]          --
│    │    └─nconv: 3-19                  [64, 32, 170, 1]          --
│    │    └─nconv: 3-20                  [64, 32, 170, 1]          --
│    │    └─linear: 3-21                 [64, 32, 170, 1]          3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-21                     [64, 32, 170, 1]          --
│    │    └─nconv: 3-22                  [64, 32, 170, 1]          --
│    │    └─nconv: 3-23                  [64, 32, 170, 1]          --
│    │    └─linear: 3-24                 [64, 32, 170, 1]          3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-22                   [64, 32, 170, 1]          10,880
├─Conv2d: 1-22                           [64, 64, 170, 1]          2,112
├─Conv2d: 1-23                           [64, 128, 170, 1]         8,320
├─Conv2d: 1-24                           [64, 12, 170, 1]          1,548
==========================================================================================
Total params: 351,516
Trainable params: 351,516
Non-trainable params: 0
Total mult-adds (G): 4.66
==========================================================================================
Input size (MB): 0.52
Forward/backward pass size (MB): 427.41
Params size (MB): 1.39
Estimated Total Size (MB): 429.33
==========================================================================================

Loss: HuberLoss

CL target length = 1
2023-03-26 21:42:55.189629 Epoch 1  	Train Loss = 20.10753 Val Loss = 112.95275
2023-03-26 21:43:02.288577 Epoch 2  	Train Loss = 14.88873 Val Loss = 112.89504
2023-03-26 21:43:09.762889 Epoch 3  	Train Loss = 14.82551 Val Loss = 112.86879
2023-03-26 21:43:17.526039 Epoch 4  	Train Loss = 14.62826 Val Loss = 112.93720
2023-03-26 21:43:25.616284 Epoch 5  	Train Loss = 14.26306 Val Loss = 112.88783
2023-03-26 21:43:32.419228 Epoch 6  	Train Loss = 14.07087 Val Loss = 112.90844
2023-03-26 21:43:39.242046 Epoch 7  	Train Loss = 13.98047 Val Loss = 112.87244
2023-03-26 21:43:45.734637 Epoch 8  	Train Loss = 14.05309 Val Loss = 112.85096
2023-03-26 21:43:52.885153 Epoch 9  	Train Loss = 14.05333 Val Loss = 112.81958
2023-03-26 21:44:00.381401 Epoch 10  	Train Loss = 13.90705 Val Loss = 112.80634
2023-03-26 21:44:07.547617 Epoch 11  	Train Loss = 13.77338 Val Loss = 112.82231
2023-03-26 21:44:14.938756 Epoch 12  	Train Loss = 13.65806 Val Loss = 112.85284
2023-03-26 21:44:22.482259 Epoch 13  	Train Loss = 13.62688 Val Loss = 112.87038
2023-03-26 21:44:29.767924 Epoch 14  	Train Loss = 13.63594 Val Loss = 112.78743
CL target length = 2
2023-03-26 21:44:36.912362 Epoch 15  	Train Loss = 15.85469 Val Loss = 104.33864
2023-03-26 21:44:43.547138 Epoch 16  	Train Loss = 14.26509 Val Loss = 103.87354
2023-03-26 21:44:50.015534 Epoch 17  	Train Loss = 14.00762 Val Loss = 104.05162
2023-03-26 21:44:57.159833 Epoch 18  	Train Loss = 13.95507 Val Loss = 103.88320
2023-03-26 21:45:04.148380 Epoch 19  	Train Loss = 13.97582 Val Loss = 103.80418
2023-03-26 21:45:11.149157 Epoch 20  	Train Loss = 13.83819 Val Loss = 103.82752
2023-03-26 21:45:18.386592 Epoch 21  	Train Loss = 13.74228 Val Loss = 103.86995
2023-03-26 21:45:25.841109 Epoch 22  	Train Loss = 13.80703 Val Loss = 103.88194
2023-03-26 21:45:33.093351 Epoch 23  	Train Loss = 13.72601 Val Loss = 103.82005
2023-03-26 21:45:40.437843 Epoch 24  	Train Loss = 13.63287 Val Loss = 103.79712
2023-03-26 21:45:47.981633 Epoch 25  	Train Loss = 13.60088 Val Loss = 103.82122
2023-03-26 21:45:55.214026 Epoch 26  	Train Loss = 13.56319 Val Loss = 103.79689
2023-03-26 21:46:02.458411 Epoch 27  	Train Loss = 13.53138 Val Loss = 103.83882
2023-03-26 21:46:09.911854 Epoch 28  	Train Loss = 13.55003 Val Loss = 103.83022
2023-03-26 21:46:17.656235 Epoch 29  	Train Loss = 13.47351 Val Loss = 103.78620
CL target length = 3
2023-03-26 21:46:25.167318 Epoch 30  	Train Loss = 15.16361 Val Loss = 94.99691
2023-03-26 21:46:32.349119 Epoch 31  	Train Loss = 13.99754 Val Loss = 94.89324
2023-03-26 21:46:39.156113 Epoch 32  	Train Loss = 13.91000 Val Loss = 94.85647
2023-03-26 21:46:46.042713 Epoch 33  	Train Loss = 13.82139 Val Loss = 94.78162
2023-03-26 21:46:53.129440 Epoch 34  	Train Loss = 13.77262 Val Loss = 94.87628
2023-03-26 21:47:00.583799 Epoch 35  	Train Loss = 13.78848 Val Loss = 94.88593
2023-03-26 21:47:07.720978 Epoch 36  	Train Loss = 13.71280 Val Loss = 94.92023
2023-03-26 21:47:14.794409 Epoch 37  	Train Loss = 13.67241 Val Loss = 94.81609
2023-03-26 21:47:22.210430 Epoch 38  	Train Loss = 13.66330 Val Loss = 94.85284
2023-03-26 21:47:29.358711 Epoch 39  	Train Loss = 13.58870 Val Loss = 94.74327
2023-03-26 21:47:36.567585 Epoch 40  	Train Loss = 13.59379 Val Loss = 94.78615
2023-03-26 21:47:43.689712 Epoch 41  	Train Loss = 13.55765 Val Loss = 94.75461
2023-03-26 21:47:50.852032 Epoch 42  	Train Loss = 13.49215 Val Loss = 94.75228
2023-03-26 21:47:58.074950 Epoch 43  	Train Loss = 13.49926 Val Loss = 94.79491
2023-03-26 21:48:05.172726 Epoch 44  	Train Loss = 13.46873 Val Loss = 94.72676
CL target length = 4
2023-03-26 21:48:12.355556 Epoch 45  	Train Loss = 14.78692 Val Loss = 86.20385
2023-03-26 21:48:19.634622 Epoch 46  	Train Loss = 13.91765 Val Loss = 86.05697
2023-03-26 21:48:26.027302 Epoch 47  	Train Loss = 13.86199 Val Loss = 85.82642
2023-03-26 21:48:32.385145 Epoch 48  	Train Loss = 13.78964 Val Loss = 85.81434
2023-03-26 21:48:38.819021 Epoch 49  	Train Loss = 13.75870 Val Loss = 85.89618
2023-03-26 21:48:45.504031 Epoch 50  	Train Loss = 13.78272 Val Loss = 85.80086
2023-03-26 21:48:52.347261 Epoch 51  	Train Loss = 13.71394 Val Loss = 85.92856
2023-03-26 21:48:59.720104 Epoch 52  	Train Loss = 13.68187 Val Loss = 85.82339
2023-03-26 21:49:06.785789 Epoch 53  	Train Loss = 13.70408 Val Loss = 85.85098
2023-03-26 21:49:13.762489 Epoch 54  	Train Loss = 13.66728 Val Loss = 85.77749
2023-03-26 21:49:20.135377 Epoch 55  	Train Loss = 13.58516 Val Loss = 85.83872
2023-03-26 21:49:26.854328 Epoch 56  	Train Loss = 13.62105 Val Loss = 85.87022
2023-03-26 21:49:33.752851 Epoch 57  	Train Loss = 13.58882 Val Loss = 85.80865
2023-03-26 21:49:40.930162 Epoch 58  	Train Loss = 13.56748 Val Loss = 85.94648
2023-03-26 21:49:48.183762 Epoch 59  	Train Loss = 13.55377 Val Loss = 85.86266
CL target length = 5
2023-03-26 21:49:55.422651 Epoch 60  	Train Loss = 14.65808 Val Loss = 77.04293
2023-03-26 21:50:02.377013 Epoch 61  	Train Loss = 13.84035 Val Loss = 76.90381
2023-03-26 21:50:08.746306 Epoch 62  	Train Loss = 13.83277 Val Loss = 77.06813
2023-03-26 21:50:15.113364 Epoch 63  	Train Loss = 13.75333 Val Loss = 76.88453
2023-03-26 21:50:21.780687 Epoch 64  	Train Loss = 13.76773 Val Loss = 77.01911
2023-03-26 21:50:28.589088 Epoch 65  	Train Loss = 13.77187 Val Loss = 76.90221
2023-03-26 21:50:35.528303 Epoch 66  	Train Loss = 13.71637 Val Loss = 76.97018
2023-03-26 21:50:42.886784 Epoch 67  	Train Loss = 13.73873 Val Loss = 76.87290
2023-03-26 21:50:50.057664 Epoch 68  	Train Loss = 13.77085 Val Loss = 76.87949
2023-03-26 21:50:57.438984 Epoch 69  	Train Loss = 13.67866 Val Loss = 77.03270
2023-03-26 21:51:04.718593 Epoch 70  	Train Loss = 13.70857 Val Loss = 76.90551
2023-03-26 21:51:11.937063 Epoch 71  	Train Loss = 13.64418 Val Loss = 76.83950
2023-03-26 21:51:19.181664 Epoch 72  	Train Loss = 13.65631 Val Loss = 76.91824
2023-03-26 21:51:26.588477 Epoch 73  	Train Loss = 13.66472 Val Loss = 77.01886
2023-03-26 21:51:33.952651 Epoch 74  	Train Loss = 13.64082 Val Loss = 76.98857
CL target length = 6
2023-03-26 21:51:41.332017 Epoch 75  	Train Loss = 14.63780 Val Loss = 68.02491
2023-03-26 21:51:48.342662 Epoch 76  	Train Loss = 13.89097 Val Loss = 68.03011
2023-03-26 21:51:55.192723 Epoch 77  	Train Loss = 13.84023 Val Loss = 68.13626
2023-03-26 21:52:02.198947 Epoch 78  	Train Loss = 13.79749 Val Loss = 68.06902
2023-03-26 21:52:09.165962 Epoch 79  	Train Loss = 13.80212 Val Loss = 68.13528
2023-03-26 21:52:16.225937 Epoch 80  	Train Loss = 13.79751 Val Loss = 68.08831
2023-03-26 21:52:23.390904 Epoch 81  	Train Loss = 13.73432 Val Loss = 68.10775
2023-03-26 21:52:30.623475 Epoch 82  	Train Loss = 13.75748 Val Loss = 68.09884
2023-03-26 21:52:37.893759 Epoch 83  	Train Loss = 13.79240 Val Loss = 68.06451
2023-03-26 21:52:44.832348 Epoch 84  	Train Loss = 13.75217 Val Loss = 68.14351
2023-03-26 21:52:51.459579 Epoch 85  	Train Loss = 13.78273 Val Loss = 68.00516
2023-03-26 21:52:58.065635 Epoch 86  	Train Loss = 13.69889 Val Loss = 68.00356
2023-03-26 21:53:04.410766 Epoch 87  	Train Loss = 13.67116 Val Loss = 68.09332
2023-03-26 21:53:11.099353 Epoch 88  	Train Loss = 13.65734 Val Loss = 68.07354
2023-03-26 21:53:18.339828 Epoch 89  	Train Loss = 13.69024 Val Loss = 68.09611
CL target length = 7
2023-03-26 21:53:25.523807 Epoch 90  	Train Loss = 14.55775 Val Loss = 59.58953
2023-03-26 21:53:32.671198 Epoch 91  	Train Loss = 13.90626 Val Loss = 59.27651
2023-03-26 21:53:39.726391 Epoch 92  	Train Loss = 13.91266 Val Loss = 59.48868
2023-03-26 21:53:47.077764 Epoch 93  	Train Loss = 13.86562 Val Loss = 59.07651
2023-03-26 21:53:54.201955 Epoch 94  	Train Loss = 13.83556 Val Loss = 59.12367
2023-03-26 21:54:00.846378 Epoch 95  	Train Loss = 13.78895 Val Loss = 59.09059
2023-03-26 21:54:08.163914 Epoch 96  	Train Loss = 13.83136 Val Loss = 59.26926
2023-03-26 21:54:14.528185 Epoch 97  	Train Loss = 13.78097 Val Loss = 59.09322
2023-03-26 21:54:20.890546 Epoch 98  	Train Loss = 13.77081 Val Loss = 59.47459
2023-03-26 21:54:27.803183 Epoch 99  	Train Loss = 13.77220 Val Loss = 59.35078
2023-03-26 21:54:34.990902 Epoch 100  	Train Loss = 13.76310 Val Loss = 59.25459
2023-03-26 21:54:42.348365 Epoch 101  	Train Loss = 13.74499 Val Loss = 59.37920
2023-03-26 21:54:48.963386 Epoch 102  	Train Loss = 13.69108 Val Loss = 59.17594
2023-03-26 21:54:55.977162 Epoch 103  	Train Loss = 13.73375 Val Loss = 59.03486
2023-03-26 21:55:03.467388 Epoch 104  	Train Loss = 13.73567 Val Loss = 59.26485
CL target length = 8
2023-03-26 21:55:10.858308 Epoch 105  	Train Loss = 14.57737 Val Loss = 50.55201
2023-03-26 21:55:18.102345 Epoch 106  	Train Loss = 13.86528 Val Loss = 50.28887
2023-03-26 21:55:25.542863 Epoch 107  	Train Loss = 13.84830 Val Loss = 50.25848
2023-03-26 21:55:33.060172 Epoch 108  	Train Loss = 13.85695 Val Loss = 50.28148
2023-03-26 21:55:40.324508 Epoch 109  	Train Loss = 13.87547 Val Loss = 50.39609
2023-03-26 21:55:47.515544 Epoch 110  	Train Loss = 13.82658 Val Loss = 50.82290
2023-03-26 21:55:54.108649 Epoch 111  	Train Loss = 13.84380 Val Loss = 50.60595
2023-03-26 21:56:00.949114 Epoch 112  	Train Loss = 13.85547 Val Loss = 50.43388
2023-03-26 21:56:07.813212 Epoch 113  	Train Loss = 13.76921 Val Loss = 50.36177
2023-03-26 21:56:14.205117 Epoch 114  	Train Loss = 13.78759 Val Loss = 50.69141
2023-03-26 21:56:21.005753 Epoch 115  	Train Loss = 13.82104 Val Loss = 50.51471
2023-03-26 21:56:27.845129 Epoch 116  	Train Loss = 13.74945 Val Loss = 50.64897
2023-03-26 21:56:35.116681 Epoch 117  	Train Loss = 13.72562 Val Loss = 50.62236
2023-03-26 21:56:42.164518 Epoch 118  	Train Loss = 13.78346 Val Loss = 51.03158
2023-03-26 21:56:48.933099 Epoch 119  	Train Loss = 13.69909 Val Loss = 50.40068
CL target length = 9
2023-03-26 21:56:55.541574 Epoch 120  	Train Loss = 14.61947 Val Loss = 42.32637
2023-03-26 21:57:01.900181 Epoch 121  	Train Loss = 13.88823 Val Loss = 41.58013
2023-03-26 21:57:08.344006 Epoch 122  	Train Loss = 13.93467 Val Loss = 41.87347
2023-03-26 21:57:15.285802 Epoch 123  	Train Loss = 13.85896 Val Loss = 41.69350
2023-03-26 21:57:21.914339 Epoch 124  	Train Loss = 13.91406 Val Loss = 41.70255
2023-03-26 21:57:28.614261 Epoch 125  	Train Loss = 13.85648 Val Loss = 41.69965
2023-03-26 21:57:35.435801 Epoch 126  	Train Loss = 13.88708 Val Loss = 41.51320
2023-03-26 21:57:42.705065 Epoch 127  	Train Loss = 13.86969 Val Loss = 42.30047
2023-03-26 21:57:49.841920 Epoch 128  	Train Loss = 13.84015 Val Loss = 41.54292
2023-03-26 21:57:57.000627 Epoch 129  	Train Loss = 13.83613 Val Loss = 42.44537
2023-03-26 21:58:04.220263 Epoch 130  	Train Loss = 13.85438 Val Loss = 41.61088
2023-03-26 21:58:11.399260 Epoch 131  	Train Loss = 13.79067 Val Loss = 41.57060
2023-03-26 21:58:18.819148 Epoch 132  	Train Loss = 13.83239 Val Loss = 41.98054
2023-03-26 21:58:26.102810 Epoch 133  	Train Loss = 13.81740 Val Loss = 41.88449
CL target length = 10
2023-03-26 21:58:33.345235 Epoch 134  	Train Loss = 14.28797 Val Loss = 33.23844
2023-03-26 21:58:40.821549 Epoch 135  	Train Loss = 14.09651 Val Loss = 32.83762
2023-03-26 21:58:48.215623 Epoch 136  	Train Loss = 13.96638 Val Loss = 33.35488
2023-03-26 21:58:54.860915 Epoch 137  	Train Loss = 13.90777 Val Loss = 33.03232
2023-03-26 21:59:01.202272 Epoch 138  	Train Loss = 13.91912 Val Loss = 33.18651
2023-03-26 21:59:07.908762 Epoch 139  	Train Loss = 13.88707 Val Loss = 33.41819
2023-03-26 21:59:15.121662 Epoch 140  	Train Loss = 13.91025 Val Loss = 33.68358
2023-03-26 21:59:22.379456 Epoch 141  	Train Loss = 13.87240 Val Loss = 33.45147
2023-03-26 21:59:29.644656 Epoch 142  	Train Loss = 13.87848 Val Loss = 33.65096
2023-03-26 21:59:37.013919 Epoch 143  	Train Loss = 13.87620 Val Loss = 33.11403
2023-03-26 21:59:44.114983 Epoch 144  	Train Loss = 13.87537 Val Loss = 33.15780
2023-03-26 21:59:51.009271 Epoch 145  	Train Loss = 13.88340 Val Loss = 33.15458
2023-03-26 21:59:58.333033 Epoch 146  	Train Loss = 13.85278 Val Loss = 33.32372
2023-03-26 22:00:05.522766 Epoch 147  	Train Loss = 13.81764 Val Loss = 33.07828
2023-03-26 22:00:12.279309 Epoch 148  	Train Loss = 13.82772 Val Loss = 32.98302
CL target length = 11
2023-03-26 22:00:19.600772 Epoch 149  	Train Loss = 14.39343 Val Loss = 24.58064
2023-03-26 22:00:26.493490 Epoch 150  	Train Loss = 14.02951 Val Loss = 25.68210
2023-03-26 22:00:32.918697 Epoch 151  	Train Loss = 13.98471 Val Loss = 24.33484
2023-03-26 22:00:39.317189 Epoch 152  	Train Loss = 13.99231 Val Loss = 24.95032
2023-03-26 22:00:46.168958 Epoch 153  	Train Loss = 13.92403 Val Loss = 24.57709
2023-03-26 22:00:53.307781 Epoch 154  	Train Loss = 13.93890 Val Loss = 24.59993
2023-03-26 22:00:59.969029 Epoch 155  	Train Loss = 13.86464 Val Loss = 24.41891
2023-03-26 22:01:07.265697 Epoch 156  	Train Loss = 13.88139 Val Loss = 24.61359
2023-03-26 22:01:14.543150 Epoch 157  	Train Loss = 13.89001 Val Loss = 24.50398
2023-03-26 22:01:21.477363 Epoch 158  	Train Loss = 13.91200 Val Loss = 24.70904
2023-03-26 22:01:28.907408 Epoch 159  	Train Loss = 13.88787 Val Loss = 24.16529
2023-03-26 22:01:36.208455 Epoch 160  	Train Loss = 13.88539 Val Loss = 24.88119
2023-03-26 22:01:43.429761 Epoch 161  	Train Loss = 13.88703 Val Loss = 25.20912
2023-03-26 22:01:50.465563 Epoch 162  	Train Loss = 13.85645 Val Loss = 24.33242
2023-03-26 22:01:56.826388 Epoch 163  	Train Loss = 13.83001 Val Loss = 24.12383
CL target length = 12
2023-03-26 22:02:04.131805 Epoch 164  	Train Loss = 14.35847 Val Loss = 15.38067
2023-03-26 22:02:11.465055 Epoch 165  	Train Loss = 14.00540 Val Loss = 15.50076
2023-03-26 22:02:18.658270 Epoch 166  	Train Loss = 13.95671 Val Loss = 15.92797
2023-03-26 22:02:25.942337 Epoch 167  	Train Loss = 13.94676 Val Loss = 15.74446
2023-03-26 22:02:32.687622 Epoch 168  	Train Loss = 13.90041 Val Loss = 15.64901
2023-03-26 22:02:39.664831 Epoch 169  	Train Loss = 13.72119 Val Loss = 15.54571
2023-03-26 22:02:47.136769 Epoch 170  	Train Loss = 13.68300 Val Loss = 15.42876
2023-03-26 22:02:54.472851 Epoch 171  	Train Loss = 13.66503 Val Loss = 15.51361
2023-03-26 22:03:01.870632 Epoch 172  	Train Loss = 13.69320 Val Loss = 15.44174
2023-03-26 22:03:08.572177 Epoch 173  	Train Loss = 13.66069 Val Loss = 15.48460
2023-03-26 22:03:15.373752 Epoch 174  	Train Loss = 13.64073 Val Loss = 15.71886
2023-03-26 22:03:22.714086 Epoch 175  	Train Loss = 13.64158 Val Loss = 15.49676
2023-03-26 22:03:30.147817 Epoch 176  	Train Loss = 13.59959 Val Loss = 15.48374
2023-03-26 22:03:37.661503 Epoch 177  	Train Loss = 13.62490 Val Loss = 15.61512
2023-03-26 22:03:45.275417 Epoch 178  	Train Loss = 13.65206 Val Loss = 15.39328
2023-03-26 22:03:52.740007 Epoch 179  	Train Loss = 13.63838 Val Loss = 15.46497
Early stopping at epoch: 179
Best at epoch 164:
Train Loss = 14.35847
Train RMSE = 23.08183, MAE = 13.84771, MAPE = 8.88756
Val Loss = 15.38067
Val RMSE = 25.91828, MAE = 15.90450, MAPE = 12.00693
--------- Test ---------
All Steps RMSE = 25.49022, MAE = 16.25233, MAPE = 9.98853
Step 1 RMSE = 20.15226, MAE = 13.06579, MAPE = 8.35264
Step 2 RMSE = 21.57727, MAE = 13.85845, MAPE = 8.76055
Step 3 RMSE = 22.75734, MAE = 14.57248, MAPE = 9.11040
Step 4 RMSE = 23.75730, MAE = 15.16380, MAPE = 9.41292
Step 5 RMSE = 24.57796, MAE = 15.67556, MAPE = 9.68481
Step 6 RMSE = 25.30387, MAE = 16.12296, MAPE = 9.92394
Step 7 RMSE = 26.10648, MAE = 16.66358, MAPE = 10.18755
Step 8 RMSE = 26.82143, MAE = 17.15375, MAPE = 10.44316
Step 9 RMSE = 27.46810, MAE = 17.61536, MAPE = 10.68626
Step 10 RMSE = 27.95645, MAE = 17.96470, MAPE = 10.86603
Step 11 RMSE = 28.43366, MAE = 18.30396, MAPE = 11.07316
Step 12 RMSE = 29.20944, MAE = 18.86752, MAPE = 11.36102
Inference time: 0.50 s
