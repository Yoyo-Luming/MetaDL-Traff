PEMS07
Original data shape (28224, 883, 1)
Trainset:	x-(16911, 12, 883, 1)	y-(16911, 12, 883, 1)
Valset:  	x-(5622, 12, 883, 1)  	y-(5622, 12, 883, 1)
Testset:	x-(5622, 12, 883, 1)	y-(5622, 12, 883, 1)

--------- MTGNN ---------
{
    "num_nodes": 883,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        108
    ],
    "clip_grad": 5,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": true,
    "cl_step_size": 2500,
    "load_npz": false,
    "pass_device": true,
    "model_args": {
        "num_nodes": 883,
        "in_dim": 1,
        "seq_length": 12,
        "out_dim": 12,
        "device": "cuda:0",
        "gcn_true": true,
        "buildA_true": true,
        "gcn_depth": 2,
        "predefined_A": null,
        "static_feat": null,
        "dropout": 0.3,
        "subgraph_size": 20,
        "node_dim": 40,
        "dilation_exponential": 1,
        "conv_channels": 32,
        "residual_channels": 32,
        "skip_channels": 64,
        "end_channels": 128,
        "layers": 3,
        "propalpha": 0.05,
        "tanhalpha": 3,
        "layer_norm_affline": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MTGNN                                    [64, 12, 883, 1]          3,168
├─graph_constructor: 1-1                 [883, 883]                --
│    └─Embedding: 2-1                    [883, 40]                 35,320
│    └─Embedding: 2-2                    [883, 40]                 35,320
│    └─Linear: 2-3                       [883, 40]                 1,640
│    └─Linear: 2-4                       [883, 40]                 1,640
├─Conv2d: 1-2                            [64, 32, 883, 19]         64
├─Conv2d: 1-3                            [64, 64, 883, 1]          1,280
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-5            [64, 32, 883, 13]         --
│    │    └─ModuleList: 3-1              --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-6            [64, 32, 883, 13]         --
│    │    └─ModuleList: 3-2              --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-7                       [64, 64, 883, 1]          26,688
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-8                      [64, 32, 883, 13]         --
│    │    └─nconv: 3-3                   [64, 32, 883, 13]         --
│    │    └─nconv: 3-4                   [64, 32, 883, 13]         --
│    │    └─linear: 3-5                  [64, 32, 883, 13]         3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-9                      [64, 32, 883, 13]         --
│    │    └─nconv: 3-6                   [64, 32, 883, 13]         --
│    │    └─nconv: 3-7                   [64, 32, 883, 13]         --
│    │    └─linear: 3-8                  [64, 32, 883, 13]         3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-10                   [64, 32, 883, 13]         734,656
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-11           [64, 32, 883, 7]          --
│    │    └─ModuleList: 3-9              --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-12           [64, 32, 883, 7]          --
│    │    └─ModuleList: 3-10             --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-13                      [64, 64, 883, 1]          14,400
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-14                     [64, 32, 883, 7]          --
│    │    └─nconv: 3-11                  [64, 32, 883, 7]          --
│    │    └─nconv: 3-12                  [64, 32, 883, 7]          --
│    │    └─linear: 3-13                 [64, 32, 883, 7]          3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-15                     [64, 32, 883, 7]          --
│    │    └─nconv: 3-14                  [64, 32, 883, 7]          --
│    │    └─nconv: 3-15                  [64, 32, 883, 7]          --
│    │    └─linear: 3-16                 [64, 32, 883, 7]          3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-16                   [64, 32, 883, 7]          395,584
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-17           [64, 32, 883, 1]          --
│    │    └─ModuleList: 3-17             --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-18           [64, 32, 883, 1]          --
│    │    └─ModuleList: 3-18             --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-19                      [64, 64, 883, 1]          2,112
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-20                     [64, 32, 883, 1]          --
│    │    └─nconv: 3-19                  [64, 32, 883, 1]          --
│    │    └─nconv: 3-20                  [64, 32, 883, 1]          --
│    │    └─linear: 3-21                 [64, 32, 883, 1]          3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-21                     [64, 32, 883, 1]          --
│    │    └─nconv: 3-22                  [64, 32, 883, 1]          --
│    │    └─nconv: 3-23                  [64, 32, 883, 1]          --
│    │    └─linear: 3-24                 [64, 32, 883, 1]          3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-22                   [64, 32, 883, 1]          56,512
├─Conv2d: 1-22                           [64, 64, 883, 1]          2,112
├─Conv2d: 1-23                           [64, 128, 883, 1]         8,320
├─Conv2d: 1-24                           [64, 12, 883, 1]          1,548
==========================================================================================
Total params: 1,366,828
Trainable params: 1,366,828
Non-trainable params: 0
Total mult-adds (G): 24.24
==========================================================================================
Input size (MB): 2.71
Forward/backward pass size (MB): 2220.02
Params size (MB): 5.45
Estimated Total Size (MB): 2228.18
==========================================================================================

Loss: HuberLoss

CL target length = 1
2023-03-26 21:43:29.597485 Epoch 1  	Train Loss = 23.63969 Val Loss = 144.64249
2023-03-26 21:44:14.087791 Epoch 2  	Train Loss = 19.30371 Val Loss = 144.56410
2023-03-26 21:44:58.710075 Epoch 3  	Train Loss = 18.88839 Val Loss = 144.48114
2023-03-26 21:45:43.180680 Epoch 4  	Train Loss = 18.58252 Val Loss = 144.54531
2023-03-26 21:46:28.051325 Epoch 5  	Train Loss = 18.70485 Val Loss = 144.52688
2023-03-26 21:47:12.723444 Epoch 6  	Train Loss = 18.61472 Val Loss = 144.45623
2023-03-26 21:47:57.278820 Epoch 7  	Train Loss = 18.25678 Val Loss = 144.46275
2023-03-26 21:48:41.602218 Epoch 8  	Train Loss = 18.24892 Val Loss = 144.50377
2023-03-26 21:49:26.343496 Epoch 9  	Train Loss = 18.08063 Val Loss = 144.45336
CL target length = 2
2023-03-26 21:50:11.137263 Epoch 10  	Train Loss = 20.44022 Val Loss = 133.15118
2023-03-26 21:50:55.844954 Epoch 11  	Train Loss = 19.03044 Val Loss = 133.10644
2023-03-26 21:51:40.430755 Epoch 12  	Train Loss = 18.73623 Val Loss = 133.01493
2023-03-26 21:52:24.785053 Epoch 13  	Train Loss = 18.73922 Val Loss = 133.09964
2023-03-26 21:53:09.361299 Epoch 14  	Train Loss = 18.51238 Val Loss = 132.96938
2023-03-26 21:53:53.814396 Epoch 15  	Train Loss = 18.45552 Val Loss = 132.99390
2023-03-26 21:54:38.217623 Epoch 16  	Train Loss = 18.34656 Val Loss = 132.98241
2023-03-26 21:55:22.872722 Epoch 17  	Train Loss = 18.25064 Val Loss = 132.98201
2023-03-26 21:56:07.550605 Epoch 18  	Train Loss = 18.07134 Val Loss = 132.96729
CL target length = 3
2023-03-26 21:56:52.288633 Epoch 19  	Train Loss = 19.46155 Val Loss = 121.73537
2023-03-26 21:57:36.960541 Epoch 20  	Train Loss = 18.81294 Val Loss = 121.69028
2023-03-26 21:58:21.486094 Epoch 21  	Train Loss = 18.74597 Val Loss = 121.66893
2023-03-26 21:59:06.154583 Epoch 22  	Train Loss = 18.66616 Val Loss = 121.55209
2023-03-26 21:59:50.714728 Epoch 23  	Train Loss = 18.57203 Val Loss = 121.57414
2023-03-26 22:00:35.412657 Epoch 24  	Train Loss = 18.59173 Val Loss = 121.54635
2023-03-26 22:01:20.107153 Epoch 25  	Train Loss = 18.42470 Val Loss = 121.71206
2023-03-26 22:02:04.968312 Epoch 26  	Train Loss = 18.42075 Val Loss = 121.68040
2023-03-26 22:02:49.897884 Epoch 27  	Train Loss = 18.39634 Val Loss = 121.54934
2023-03-26 22:03:34.718314 Epoch 28  	Train Loss = 18.29038 Val Loss = 121.61219
CL target length = 4
2023-03-26 22:04:19.736224 Epoch 29  	Train Loss = 19.73986 Val Loss = 110.40755
2023-03-26 22:05:04.548081 Epoch 30  	Train Loss = 18.82295 Val Loss = 110.15984
2023-03-26 22:05:49.217424 Epoch 31  	Train Loss = 18.73630 Val Loss = 110.15035
2023-03-26 22:06:33.836020 Epoch 32  	Train Loss = 18.64561 Val Loss = 110.10573
2023-03-26 22:07:18.543531 Epoch 33  	Train Loss = 18.55460 Val Loss = 110.26491
2023-03-26 22:08:03.173702 Epoch 34  	Train Loss = 18.54582 Val Loss = 110.19648
2023-03-26 22:08:47.605716 Epoch 35  	Train Loss = 18.53932 Val Loss = 110.16619
2023-03-26 22:09:32.245100 Epoch 36  	Train Loss = 18.46801 Val Loss = 110.23787
2023-03-26 22:10:16.892782 Epoch 37  	Train Loss = 18.45693 Val Loss = 110.31093
CL target length = 5
2023-03-26 22:11:01.547288 Epoch 38  	Train Loss = 19.43200 Val Loss = 98.80352
2023-03-26 22:11:46.172464 Epoch 39  	Train Loss = 18.82943 Val Loss = 98.80051
2023-03-26 22:12:30.644371 Epoch 40  	Train Loss = 18.82501 Val Loss = 98.85258
2023-03-26 22:13:15.115943 Epoch 41  	Train Loss = 18.69951 Val Loss = 98.76701
2023-03-26 22:13:59.473518 Epoch 42  	Train Loss = 18.71281 Val Loss = 98.91098
2023-03-26 22:14:44.004664 Epoch 43  	Train Loss = 18.66474 Val Loss = 98.78090
2023-03-26 22:15:29.233001 Epoch 44  	Train Loss = 18.61311 Val Loss = 98.72848
2023-03-26 22:16:14.157685 Epoch 45  	Train Loss = 18.62057 Val Loss = 98.94218
2023-03-26 22:16:58.692292 Epoch 46  	Train Loss = 18.55859 Val Loss = 98.74416
2023-03-26 22:17:43.445676 Epoch 47  	Train Loss = 18.52755 Val Loss = 98.86302
CL target length = 6
2023-03-26 22:18:28.407277 Epoch 48  	Train Loss = 19.60292 Val Loss = 87.45850
2023-03-26 22:19:13.111486 Epoch 49  	Train Loss = 18.84830 Val Loss = 87.44806
2023-03-26 22:19:57.342084 Epoch 50  	Train Loss = 18.81350 Val Loss = 87.45554
2023-03-26 22:20:41.980866 Epoch 51  	Train Loss = 18.75517 Val Loss = 87.63398
2023-03-26 22:21:26.320312 Epoch 52  	Train Loss = 18.81977 Val Loss = 87.47079
2023-03-26 22:22:10.954284 Epoch 53  	Train Loss = 18.69009 Val Loss = 87.38093
2023-03-26 22:22:55.521352 Epoch 54  	Train Loss = 18.73299 Val Loss = 87.56573
2023-03-26 22:23:39.939568 Epoch 55  	Train Loss = 18.67940 Val Loss = 87.38209
2023-03-26 22:24:24.092459 Epoch 56  	Train Loss = 18.67395 Val Loss = 87.53101
CL target length = 7
2023-03-26 22:25:08.286017 Epoch 57  	Train Loss = 19.46731 Val Loss = 76.48940
2023-03-26 22:25:52.412159 Epoch 58  	Train Loss = 19.03686 Val Loss = 76.19185
2023-03-26 22:26:36.640931 Epoch 59  	Train Loss = 18.95029 Val Loss = 76.29720
2023-03-26 22:27:21.011068 Epoch 60  	Train Loss = 18.92952 Val Loss = 76.19363
2023-03-26 22:28:05.883488 Epoch 61  	Train Loss = 18.93178 Val Loss = 76.31762
2023-03-26 22:28:50.176675 Epoch 62  	Train Loss = 18.87976 Val Loss = 76.16723
2023-03-26 22:29:34.608719 Epoch 63  	Train Loss = 18.83387 Val Loss = 76.22523
2023-03-26 22:30:19.386660 Epoch 64  	Train Loss = 18.80884 Val Loss = 76.17904
2023-03-26 22:31:03.574261 Epoch 65  	Train Loss = 18.83331 Val Loss = 76.28758
2023-03-26 22:31:47.905482 Epoch 66  	Train Loss = 18.78238 Val Loss = 76.20957
CL target length = 8
2023-03-26 22:32:32.479938 Epoch 67  	Train Loss = 19.65249 Val Loss = 65.11841
2023-03-26 22:33:17.473114 Epoch 68  	Train Loss = 19.04476 Val Loss = 65.23247
2023-03-26 22:34:02.418675 Epoch 69  	Train Loss = 19.08584 Val Loss = 65.04910
2023-03-26 22:34:47.078060 Epoch 70  	Train Loss = 18.98925 Val Loss = 65.04451
2023-03-26 22:35:31.489073 Epoch 71  	Train Loss = 19.03240 Val Loss = 64.86831
2023-03-26 22:36:15.875337 Epoch 72  	Train Loss = 18.96967 Val Loss = 64.90491
2023-03-26 22:37:00.774570 Epoch 73  	Train Loss = 18.97739 Val Loss = 65.04902
2023-03-26 22:37:45.387372 Epoch 74  	Train Loss = 18.93628 Val Loss = 65.00962
2023-03-26 22:38:29.961685 Epoch 75  	Train Loss = 18.91928 Val Loss = 64.88460
CL target length = 9
2023-03-26 22:39:14.904731 Epoch 76  	Train Loss = 19.53879 Val Loss = 53.71771
2023-03-26 22:39:59.261009 Epoch 77  	Train Loss = 19.18726 Val Loss = 53.86357
2023-03-26 22:40:43.375574 Epoch 78  	Train Loss = 19.12863 Val Loss = 53.81311
2023-03-26 22:41:27.504044 Epoch 79  	Train Loss = 19.10865 Val Loss = 54.06907
2023-03-26 22:42:12.132551 Epoch 80  	Train Loss = 19.08951 Val Loss = 53.94959
2023-03-26 22:42:56.334053 Epoch 81  	Train Loss = 18.99829 Val Loss = 53.72003
2023-03-26 22:43:40.662436 Epoch 82  	Train Loss = 19.01849 Val Loss = 53.90312
2023-03-26 22:44:25.183869 Epoch 83  	Train Loss = 19.04970 Val Loss = 53.82138
2023-03-26 22:45:09.431329 Epoch 84  	Train Loss = 19.00788 Val Loss = 53.80223
CL target length = 10
2023-03-26 22:45:53.646266 Epoch 85  	Train Loss = 19.47208 Val Loss = 42.92416
2023-03-26 22:46:38.093766 Epoch 86  	Train Loss = 19.26036 Val Loss = 42.82575
2023-03-26 22:47:22.829337 Epoch 87  	Train Loss = 19.18024 Val Loss = 42.56255
2023-03-26 22:48:07.595329 Epoch 88  	Train Loss = 19.19091 Val Loss = 42.77114
2023-03-26 22:48:52.285097 Epoch 89  	Train Loss = 19.14624 Val Loss = 42.47560
2023-03-26 22:49:36.809105 Epoch 90  	Train Loss = 19.15251 Val Loss = 42.36493
2023-03-26 22:50:20.937021 Epoch 91  	Train Loss = 19.13644 Val Loss = 42.74104
2023-03-26 22:51:05.251962 Epoch 92  	Train Loss = 19.07882 Val Loss = 42.59753
2023-03-26 22:51:49.877224 Epoch 93  	Train Loss = 19.07908 Val Loss = 42.59710
2023-03-26 22:52:34.638045 Epoch 94  	Train Loss = 19.07820 Val Loss = 42.72802
CL target length = 11
2023-03-26 22:53:18.877333 Epoch 95  	Train Loss = 19.71486 Val Loss = 31.64317
2023-03-26 22:54:03.493130 Epoch 96  	Train Loss = 19.23552 Val Loss = 31.55032
2023-03-26 22:54:47.720912 Epoch 97  	Train Loss = 19.24463 Val Loss = 31.65723
2023-03-26 22:55:32.253755 Epoch 98  	Train Loss = 19.20560 Val Loss = 31.68787
2023-03-26 22:56:16.786309 Epoch 99  	Train Loss = 19.21605 Val Loss = 31.72862
2023-03-26 22:57:01.065467 Epoch 100  	Train Loss = 19.22294 Val Loss = 31.51573
2023-03-26 22:57:45.266291 Epoch 101  	Train Loss = 19.16860 Val Loss = 31.39248
2023-03-26 22:58:29.426472 Epoch 102  	Train Loss = 19.18061 Val Loss = 31.37114
2023-03-26 22:59:14.374774 Epoch 103  	Train Loss = 19.21109 Val Loss = 31.77601
CL target length = 12
2023-03-26 22:59:58.560384 Epoch 104  	Train Loss = 19.64384 Val Loss = 20.36578
2023-03-26 23:00:42.745826 Epoch 105  	Train Loss = 19.37311 Val Loss = 20.55745
2023-03-26 23:01:26.989502 Epoch 106  	Train Loss = 19.33961 Val Loss = 20.54428
2023-03-26 23:02:11.375088 Epoch 107  	Train Loss = 19.30796 Val Loss = 20.45341
2023-03-26 23:02:55.574220 Epoch 108  	Train Loss = 19.32120 Val Loss = 20.94627
2023-03-26 23:03:40.068248 Epoch 109  	Train Loss = 18.92157 Val Loss = 20.10475
2023-03-26 23:04:24.613089 Epoch 110  	Train Loss = 18.84345 Val Loss = 20.02253
2023-03-26 23:05:08.754556 Epoch 111  	Train Loss = 18.83378 Val Loss = 20.21768
2023-03-26 23:05:53.387499 Epoch 112  	Train Loss = 18.81740 Val Loss = 20.13274
2023-03-26 23:06:38.176241 Epoch 113  	Train Loss = 18.80280 Val Loss = 20.08133
2023-03-26 23:07:22.400709 Epoch 114  	Train Loss = 18.79378 Val Loss = 20.05896
2023-03-26 23:08:07.732660 Epoch 115  	Train Loss = 18.78104 Val Loss = 19.96483
2023-03-26 23:08:52.023183 Epoch 116  	Train Loss = 18.77178 Val Loss = 20.01789
2023-03-26 23:09:36.296189 Epoch 117  	Train Loss = 18.77153 Val Loss = 20.15768
2023-03-26 23:10:20.495771 Epoch 118  	Train Loss = 18.75270 Val Loss = 20.05285
2023-03-26 23:11:04.789811 Epoch 119  	Train Loss = 18.75173 Val Loss = 20.02775
2023-03-26 23:11:48.961442 Epoch 120  	Train Loss = 18.74122 Val Loss = 20.00450
2023-03-26 23:12:33.186678 Epoch 121  	Train Loss = 18.74282 Val Loss = 20.03598
2023-03-26 23:13:17.667945 Epoch 122  	Train Loss = 18.73392 Val Loss = 19.95031
2023-03-26 23:14:01.975265 Epoch 123  	Train Loss = 18.72679 Val Loss = 20.00149
2023-03-26 23:14:46.194463 Epoch 124  	Train Loss = 18.72252 Val Loss = 20.10541
2023-03-26 23:15:30.631024 Epoch 125  	Train Loss = 18.71256 Val Loss = 19.94650
2023-03-26 23:16:14.928476 Epoch 126  	Train Loss = 18.70289 Val Loss = 20.08106
2023-03-26 23:16:59.174537 Epoch 127  	Train Loss = 18.70161 Val Loss = 20.06068
2023-03-26 23:17:43.276606 Epoch 128  	Train Loss = 18.68730 Val Loss = 19.98165
2023-03-26 23:18:27.452363 Epoch 129  	Train Loss = 18.68931 Val Loss = 20.18237
2023-03-26 23:19:11.635309 Epoch 130  	Train Loss = 18.68462 Val Loss = 19.95300
2023-03-26 23:19:55.795465 Epoch 131  	Train Loss = 18.67362 Val Loss = 19.98368
2023-03-26 23:20:40.032642 Epoch 132  	Train Loss = 18.67310 Val Loss = 19.99977
2023-03-26 23:21:24.685158 Epoch 133  	Train Loss = 18.67460 Val Loss = 19.96666
2023-03-26 23:22:09.114241 Epoch 134  	Train Loss = 18.66556 Val Loss = 19.95956
2023-03-26 23:22:53.267805 Epoch 135  	Train Loss = 18.66374 Val Loss = 20.03532
Early stopping at epoch: 135
Best at epoch 125:
Train Loss = 18.71256
Train RMSE = 31.47003, MAE = 19.03038, MAPE = 8.79663
Val Loss = 19.94650
Val RMSE = 33.60540, MAE = 20.56866, MAPE = 9.45998
--------- Test ---------
All Steps RMSE = 34.23558, MAE = 21.03617, MAPE = 9.19562
Step 1 RMSE = 27.42147, MAE = 17.28242, MAPE = 7.61376
Step 2 RMSE = 29.71214, MAE = 18.48586, MAPE = 8.06162
Step 3 RMSE = 31.23406, MAE = 19.33249, MAPE = 8.39944
Step 4 RMSE = 32.35458, MAE = 19.96007, MAPE = 8.67953
Step 5 RMSE = 33.31815, MAE = 20.50521, MAPE = 8.89296
Step 6 RMSE = 34.18871, MAE = 21.02098, MAPE = 9.19891
Step 7 RMSE = 34.99118, MAE = 21.49892, MAPE = 9.38697
Step 8 RMSE = 35.74356, MAE = 21.95956, MAPE = 9.52224
Step 9 RMSE = 36.42410, MAE = 22.39161, MAPE = 9.80917
Step 10 RMSE = 37.09668, MAE = 22.82390, MAPE = 9.99835
Step 11 RMSE = 37.81139, MAE = 23.28265, MAPE = 10.24567
Step 12 RMSE = 38.65342, MAE = 23.88771, MAPE = 10.53742
Inference time: 4.20 s
