PEMS03
Original data shape (26208, 358, 1)
Trainset:	x-(15701, 12, 358, 1)	y-(15701, 12, 358, 1)
Valset:  	x-(5219, 12, 358, 1)  	y-(5219, 12, 358, 1)
Testset:	x-(5219, 12, 358, 1)	y-(5219, 12, 358, 1)

--------- MTGNN ---------
{
    "num_nodes": 358,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        115
    ],
    "clip_grad": 5,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": true,
    "cl_step_size": 2500,
    "load_npz": false,
    "pass_device": true,
    "model_args": {
        "num_nodes": 358,
        "in_dim": 1,
        "seq_length": 12,
        "out_dim": 12,
        "device": "cuda:0",
        "gcn_true": true,
        "buildA_true": true,
        "gcn_depth": 2,
        "predefined_A": null,
        "static_feat": null,
        "dropout": 0.3,
        "subgraph_size": 20,
        "node_dim": 40,
        "dilation_exponential": 1,
        "conv_channels": 32,
        "residual_channels": 32,
        "skip_channels": 64,
        "end_channels": 128,
        "layers": 3,
        "propalpha": 0.05,
        "tanhalpha": 3,
        "layer_norm_affline": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MTGNN                                    [64, 12, 358, 1]          3,168
├─graph_constructor: 1-1                 [358, 358]                --
│    └─Embedding: 2-1                    [358, 40]                 14,320
│    └─Embedding: 2-2                    [358, 40]                 14,320
│    └─Linear: 2-3                       [358, 40]                 1,640
│    └─Linear: 2-4                       [358, 40]                 1,640
├─Conv2d: 1-2                            [64, 32, 358, 19]         64
├─Conv2d: 1-3                            [64, 64, 358, 1]          1,280
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-5            [64, 32, 358, 13]         --
│    │    └─ModuleList: 3-1              --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-6            [64, 32, 358, 13]         --
│    │    └─ModuleList: 3-2              --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-7                       [64, 64, 358, 1]          26,688
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-8                      [64, 32, 358, 13]         --
│    │    └─nconv: 3-3                   [64, 32, 358, 13]         --
│    │    └─nconv: 3-4                   [64, 32, 358, 13]         --
│    │    └─linear: 3-5                  [64, 32, 358, 13]         3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-9                      [64, 32, 358, 13]         --
│    │    └─nconv: 3-6                   [64, 32, 358, 13]         --
│    │    └─nconv: 3-7                   [64, 32, 358, 13]         --
│    │    └─linear: 3-8                  [64, 32, 358, 13]         3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-10                   [64, 32, 358, 13]         297,856
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-11           [64, 32, 358, 7]          --
│    │    └─ModuleList: 3-9              --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-12           [64, 32, 358, 7]          --
│    │    └─ModuleList: 3-10             --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-13                      [64, 64, 358, 1]          14,400
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-14                     [64, 32, 358, 7]          --
│    │    └─nconv: 3-11                  [64, 32, 358, 7]          --
│    │    └─nconv: 3-12                  [64, 32, 358, 7]          --
│    │    └─linear: 3-13                 [64, 32, 358, 7]          3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-15                     [64, 32, 358, 7]          --
│    │    └─nconv: 3-14                  [64, 32, 358, 7]          --
│    │    └─nconv: 3-15                  [64, 32, 358, 7]          --
│    │    └─linear: 3-16                 [64, 32, 358, 7]          3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-16                   [64, 32, 358, 7]          160,384
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-17           [64, 32, 358, 1]          --
│    │    └─ModuleList: 3-17             --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-18           [64, 32, 358, 1]          --
│    │    └─ModuleList: 3-18             --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-19                      [64, 64, 358, 1]          2,112
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-20                     [64, 32, 358, 1]          --
│    │    └─nconv: 3-19                  [64, 32, 358, 1]          --
│    │    └─nconv: 3-20                  [64, 32, 358, 1]          --
│    │    └─linear: 3-21                 [64, 32, 358, 1]          3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-21                     [64, 32, 358, 1]          --
│    │    └─nconv: 3-22                  [64, 32, 358, 1]          --
│    │    └─nconv: 3-23                  [64, 32, 358, 1]          --
│    │    └─linear: 3-24                 [64, 32, 358, 1]          3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-22                   [64, 32, 358, 1]          22,912
├─Conv2d: 1-22                           [64, 64, 358, 1]          2,112
├─Conv2d: 1-23                           [64, 128, 358, 1]         8,320
├─Conv2d: 1-24                           [64, 12, 358, 1]          1,548
==========================================================================================
Total params: 619,228
Trainable params: 619,228
Non-trainable params: 0
Total mult-adds (G): 9.81
==========================================================================================
Input size (MB): 1.10
Forward/backward pass size (MB): 900.08
Params size (MB): 2.46
Estimated Total Size (MB): 903.64
==========================================================================================

Loss: HuberLoss

CL target length = 1
2023-03-26 21:42:48.102706 Epoch 1  	Train Loss = 17.54980 Val Loss = 111.78189
2023-03-26 21:43:03.844488 Epoch 2  	Train Loss = 14.00533 Val Loss = 111.76000
2023-03-26 21:43:19.886861 Epoch 3  	Train Loss = 13.90989 Val Loss = 111.81442
2023-03-26 21:43:35.963822 Epoch 4  	Train Loss = 13.50849 Val Loss = 111.72656
2023-03-26 21:43:51.949234 Epoch 5  	Train Loss = 13.43604 Val Loss = 111.72307
2023-03-26 21:44:08.274664 Epoch 6  	Train Loss = 13.39602 Val Loss = 111.81878
2023-03-26 21:44:24.148837 Epoch 7  	Train Loss = 13.26984 Val Loss = 111.76040
2023-03-26 21:44:40.128465 Epoch 8  	Train Loss = 13.13353 Val Loss = 111.77646
2023-03-26 21:44:56.093862 Epoch 9  	Train Loss = 13.20202 Val Loss = 111.71897
2023-03-26 21:45:12.063686 Epoch 10  	Train Loss = 12.98545 Val Loss = 111.77530
CL target length = 2
2023-03-26 21:45:27.935186 Epoch 11  	Train Loss = 15.04617 Val Loss = 102.73791
2023-03-26 21:45:43.829197 Epoch 12  	Train Loss = 13.59218 Val Loss = 102.74528
2023-03-26 21:45:59.939268 Epoch 13  	Train Loss = 13.34684 Val Loss = 102.74675
2023-03-26 21:46:15.878554 Epoch 14  	Train Loss = 13.20733 Val Loss = 102.78323
2023-03-26 21:46:31.792569 Epoch 15  	Train Loss = 13.11724 Val Loss = 102.68107
2023-03-26 21:46:47.532284 Epoch 16  	Train Loss = 12.96518 Val Loss = 102.66774
2023-03-26 21:47:03.484174 Epoch 17  	Train Loss = 12.82335 Val Loss = 102.67727
2023-03-26 21:47:19.249994 Epoch 18  	Train Loss = 12.79427 Val Loss = 102.74772
2023-03-26 21:47:35.053783 Epoch 19  	Train Loss = 12.74196 Val Loss = 102.67731
2023-03-26 21:47:50.800948 Epoch 20  	Train Loss = 12.67485 Val Loss = 102.62119
CL target length = 3
2023-03-26 21:48:06.542897 Epoch 21  	Train Loss = 14.05242 Val Loss = 93.74364
2023-03-26 21:48:22.170675 Epoch 22  	Train Loss = 13.15056 Val Loss = 93.69479
2023-03-26 21:48:37.738067 Epoch 23  	Train Loss = 13.10368 Val Loss = 93.72807
2023-03-26 21:48:53.175989 Epoch 24  	Train Loss = 13.04604 Val Loss = 93.72290
2023-03-26 21:49:08.841056 Epoch 25  	Train Loss = 13.01664 Val Loss = 93.68189
2023-03-26 21:49:24.626080 Epoch 26  	Train Loss = 12.91636 Val Loss = 93.75682
2023-03-26 21:49:40.186032 Epoch 27  	Train Loss = 12.83863 Val Loss = 93.76817
2023-03-26 21:49:55.833316 Epoch 28  	Train Loss = 12.82793 Val Loss = 93.67589
2023-03-26 21:50:11.109355 Epoch 29  	Train Loss = 12.80205 Val Loss = 93.73281
2023-03-26 21:50:26.962957 Epoch 30  	Train Loss = 12.79079 Val Loss = 93.68330
CL target length = 4
2023-03-26 21:50:43.420222 Epoch 31  	Train Loss = 13.76731 Val Loss = 84.91315
2023-03-26 21:50:59.215485 Epoch 32  	Train Loss = 13.08026 Val Loss = 84.73549
2023-03-26 21:51:15.011734 Epoch 33  	Train Loss = 13.00725 Val Loss = 84.73592
2023-03-26 21:51:30.797803 Epoch 34  	Train Loss = 12.94015 Val Loss = 84.76615
2023-03-26 21:51:46.721738 Epoch 35  	Train Loss = 12.93560 Val Loss = 84.85877
2023-03-26 21:52:02.446451 Epoch 36  	Train Loss = 12.94050 Val Loss = 84.72314
2023-03-26 21:52:18.168732 Epoch 37  	Train Loss = 12.87703 Val Loss = 84.78137
2023-03-26 21:52:33.507891 Epoch 38  	Train Loss = 12.83286 Val Loss = 84.67812
2023-03-26 21:52:48.995070 Epoch 39  	Train Loss = 12.84176 Val Loss = 84.72408
2023-03-26 21:53:04.444669 Epoch 40  	Train Loss = 12.78171 Val Loss = 84.68707
CL target length = 5
2023-03-26 21:53:19.887593 Epoch 41  	Train Loss = 13.57795 Val Loss = 75.83415
2023-03-26 21:53:35.307483 Epoch 42  	Train Loss = 13.09745 Val Loss = 75.80362
2023-03-26 21:53:50.661342 Epoch 43  	Train Loss = 13.04614 Val Loss = 75.85658
2023-03-26 21:54:06.122312 Epoch 44  	Train Loss = 12.99195 Val Loss = 75.84911
2023-03-26 21:54:21.606850 Epoch 45  	Train Loss = 12.96267 Val Loss = 75.84795
2023-03-26 21:54:36.818084 Epoch 46  	Train Loss = 12.92696 Val Loss = 75.81524
2023-03-26 21:54:52.205104 Epoch 47  	Train Loss = 12.90888 Val Loss = 75.78196
2023-03-26 21:55:07.720756 Epoch 48  	Train Loss = 12.87224 Val Loss = 75.77173
2023-03-26 21:55:23.499673 Epoch 49  	Train Loss = 12.86030 Val Loss = 75.75784
2023-03-26 21:55:39.311621 Epoch 50  	Train Loss = 12.84268 Val Loss = 75.75370
CL target length = 6
2023-03-26 21:55:55.170957 Epoch 51  	Train Loss = 13.46593 Val Loss = 66.93867
2023-03-26 21:56:10.841797 Epoch 52  	Train Loss = 13.07122 Val Loss = 66.91399
2023-03-26 21:56:26.569636 Epoch 53  	Train Loss = 13.03821 Val Loss = 66.92070
2023-03-26 21:56:42.315586 Epoch 54  	Train Loss = 12.99156 Val Loss = 66.90687
2023-03-26 21:56:58.032504 Epoch 55  	Train Loss = 12.98611 Val Loss = 66.89418
2023-03-26 21:57:13.676510 Epoch 56  	Train Loss = 12.95724 Val Loss = 66.84712
2023-03-26 21:57:29.306752 Epoch 57  	Train Loss = 12.94538 Val Loss = 66.97446
2023-03-26 21:57:45.013952 Epoch 58  	Train Loss = 12.91156 Val Loss = 66.85859
2023-03-26 21:58:00.738778 Epoch 59  	Train Loss = 12.91088 Val Loss = 66.83248
2023-03-26 21:58:16.464651 Epoch 60  	Train Loss = 12.87613 Val Loss = 66.85065
CL target length = 7
2023-03-26 21:58:32.226586 Epoch 61  	Train Loss = 13.18494 Val Loss = 61.83146
2023-03-26 21:58:47.830536 Epoch 62  	Train Loss = 13.25276 Val Loss = 58.01235
2023-03-26 21:59:03.153757 Epoch 63  	Train Loss = 13.06146 Val Loss = 58.01287
2023-03-26 21:59:18.359529 Epoch 64  	Train Loss = 13.01835 Val Loss = 57.91648
2023-03-26 21:59:33.670873 Epoch 65  	Train Loss = 13.01694 Val Loss = 57.94462
2023-03-26 21:59:49.000351 Epoch 66  	Train Loss = 12.97587 Val Loss = 58.03721
2023-03-26 22:00:04.416235 Epoch 67  	Train Loss = 12.93079 Val Loss = 57.98059
2023-03-26 22:00:19.979163 Epoch 68  	Train Loss = 12.95427 Val Loss = 57.88539
2023-03-26 22:00:35.757487 Epoch 69  	Train Loss = 12.91541 Val Loss = 57.91321
2023-03-26 22:00:51.105564 Epoch 70  	Train Loss = 12.90157 Val Loss = 57.88378
2023-03-26 22:01:06.369559 Epoch 71  	Train Loss = 12.91454 Val Loss = 57.97948
CL target length = 8
2023-03-26 22:01:21.945757 Epoch 72  	Train Loss = 13.49824 Val Loss = 48.99495
2023-03-26 22:01:37.613535 Epoch 73  	Train Loss = 13.03484 Val Loss = 49.01283
2023-03-26 22:01:53.467280 Epoch 74  	Train Loss = 13.00467 Val Loss = 49.04151
2023-03-26 22:02:09.246362 Epoch 75  	Train Loss = 13.01614 Val Loss = 49.05350
2023-03-26 22:02:24.915635 Epoch 76  	Train Loss = 12.99385 Val Loss = 49.23813
2023-03-26 22:02:40.553542 Epoch 77  	Train Loss = 12.99572 Val Loss = 49.08137
2023-03-26 22:02:56.368891 Epoch 78  	Train Loss = 12.99663 Val Loss = 48.97977
2023-03-26 22:03:11.848259 Epoch 79  	Train Loss = 12.95569 Val Loss = 49.09890
2023-03-26 22:03:27.607945 Epoch 80  	Train Loss = 12.97536 Val Loss = 49.04316
2023-03-26 22:03:43.414503 Epoch 81  	Train Loss = 12.92714 Val Loss = 49.07941
CL target length = 9
2023-03-26 22:03:59.387916 Epoch 82  	Train Loss = 13.47378 Val Loss = 40.35477
2023-03-26 22:04:15.009655 Epoch 83  	Train Loss = 13.09937 Val Loss = 40.19569
2023-03-26 22:04:30.395825 Epoch 84  	Train Loss = 13.04432 Val Loss = 40.17452
2023-03-26 22:04:46.017383 Epoch 85  	Train Loss = 13.02248 Val Loss = 40.11463
2023-03-26 22:05:01.526011 Epoch 86  	Train Loss = 13.01118 Val Loss = 40.38045
2023-03-26 22:05:16.823100 Epoch 87  	Train Loss = 13.01833 Val Loss = 40.16724
2023-03-26 22:05:32.160770 Epoch 88  	Train Loss = 12.98738 Val Loss = 40.11131
2023-03-26 22:05:47.836379 Epoch 89  	Train Loss = 12.96990 Val Loss = 40.20008
2023-03-26 22:06:03.481451 Epoch 90  	Train Loss = 12.97405 Val Loss = 40.44180
2023-03-26 22:06:19.029414 Epoch 91  	Train Loss = 12.97038 Val Loss = 40.28026
CL target length = 10
2023-03-26 22:06:34.468250 Epoch 92  	Train Loss = 13.41755 Val Loss = 31.40650
2023-03-26 22:06:50.082380 Epoch 93  	Train Loss = 13.06795 Val Loss = 31.32549
2023-03-26 22:07:06.084248 Epoch 94  	Train Loss = 13.07062 Val Loss = 31.38134
2023-03-26 22:07:21.782637 Epoch 95  	Train Loss = 13.06309 Val Loss = 31.45065
2023-03-26 22:07:37.462657 Epoch 96  	Train Loss = 13.04962 Val Loss = 31.31566
2023-03-26 22:07:52.846469 Epoch 97  	Train Loss = 13.02759 Val Loss = 31.34904
2023-03-26 22:08:08.195332 Epoch 98  	Train Loss = 13.00314 Val Loss = 31.28119
2023-03-26 22:08:23.694718 Epoch 99  	Train Loss = 12.99110 Val Loss = 31.39869
2023-03-26 22:08:39.070498 Epoch 100  	Train Loss = 12.99329 Val Loss = 31.35135
2023-03-26 22:08:54.594243 Epoch 101  	Train Loss = 13.00815 Val Loss = 31.36255
CL target length = 11
2023-03-26 22:09:10.008826 Epoch 102  	Train Loss = 13.40098 Val Loss = 22.54417
2023-03-26 22:09:25.349357 Epoch 103  	Train Loss = 13.11790 Val Loss = 22.47502
2023-03-26 22:09:40.609054 Epoch 104  	Train Loss = 13.09810 Val Loss = 22.53221
2023-03-26 22:09:55.805025 Epoch 105  	Train Loss = 13.07342 Val Loss = 22.39831
2023-03-26 22:10:11.462025 Epoch 106  	Train Loss = 13.03593 Val Loss = 22.57508
2023-03-26 22:10:27.299569 Epoch 107  	Train Loss = 13.05112 Val Loss = 22.59880
2023-03-26 22:10:43.151098 Epoch 108  	Train Loss = 13.06296 Val Loss = 22.51238
2023-03-26 22:10:58.705324 Epoch 109  	Train Loss = 13.04230 Val Loss = 22.38345
2023-03-26 22:11:14.651617 Epoch 110  	Train Loss = 13.04363 Val Loss = 22.66206
2023-03-26 22:11:30.560276 Epoch 111  	Train Loss = 13.05574 Val Loss = 22.49293
CL target length = 12
2023-03-26 22:11:46.401973 Epoch 112  	Train Loss = 13.37994 Val Loss = 13.69818
2023-03-26 22:12:02.385495 Epoch 113  	Train Loss = 13.13396 Val Loss = 13.69935
2023-03-26 22:12:18.478110 Epoch 114  	Train Loss = 13.10881 Val Loss = 13.73001
2023-03-26 22:12:34.398986 Epoch 115  	Train Loss = 13.08479 Val Loss = 13.80759
2023-03-26 22:12:50.108720 Epoch 116  	Train Loss = 12.85376 Val Loss = 13.46860
2023-03-26 22:13:05.860636 Epoch 117  	Train Loss = 12.82286 Val Loss = 13.48299
2023-03-26 22:13:21.674734 Epoch 118  	Train Loss = 12.81640 Val Loss = 13.48590
2023-03-26 22:13:37.454131 Epoch 119  	Train Loss = 12.81134 Val Loss = 13.46550
2023-03-26 22:13:53.187266 Epoch 120  	Train Loss = 12.79601 Val Loss = 13.45224
2023-03-26 22:14:08.948022 Epoch 121  	Train Loss = 12.79072 Val Loss = 13.45539
2023-03-26 22:14:24.591015 Epoch 122  	Train Loss = 12.78310 Val Loss = 13.43159
2023-03-26 22:14:40.319054 Epoch 123  	Train Loss = 12.78593 Val Loss = 13.45735
2023-03-26 22:14:55.769354 Epoch 124  	Train Loss = 12.77925 Val Loss = 13.45699
2023-03-26 22:15:11.516264 Epoch 125  	Train Loss = 12.77493 Val Loss = 13.43779
2023-03-26 22:15:27.412518 Epoch 126  	Train Loss = 12.77486 Val Loss = 13.44942
2023-03-26 22:15:43.167910 Epoch 127  	Train Loss = 12.76318 Val Loss = 13.46685
2023-03-26 22:15:59.068974 Epoch 128  	Train Loss = 12.76070 Val Loss = 13.45767
2023-03-26 22:16:14.393467 Epoch 129  	Train Loss = 12.76317 Val Loss = 13.45271
2023-03-26 22:16:29.951549 Epoch 130  	Train Loss = 12.75758 Val Loss = 13.43738
2023-03-26 22:16:45.693621 Epoch 131  	Train Loss = 12.75563 Val Loss = 13.42297
2023-03-26 22:17:01.444410 Epoch 132  	Train Loss = 12.75431 Val Loss = 13.43790
2023-03-26 22:17:17.341361 Epoch 133  	Train Loss = 12.74698 Val Loss = 13.43631
2023-03-26 22:17:32.912086 Epoch 134  	Train Loss = 12.74876 Val Loss = 13.43965
2023-03-26 22:17:48.208421 Epoch 135  	Train Loss = 12.74242 Val Loss = 13.44901
2023-03-26 22:18:03.797227 Epoch 136  	Train Loss = 12.73875 Val Loss = 13.40750
2023-03-26 22:18:19.447519 Epoch 137  	Train Loss = 12.73098 Val Loss = 13.45954
2023-03-26 22:18:35.302644 Epoch 138  	Train Loss = 12.73023 Val Loss = 13.40561
2023-03-26 22:18:50.964504 Epoch 139  	Train Loss = 12.73222 Val Loss = 13.47627
2023-03-26 22:19:06.471693 Epoch 140  	Train Loss = 12.72950 Val Loss = 13.43676
2023-03-26 22:19:21.971044 Epoch 141  	Train Loss = 12.72179 Val Loss = 13.45673
2023-03-26 22:19:37.572125 Epoch 142  	Train Loss = 12.71746 Val Loss = 13.44807
2023-03-26 22:19:53.203810 Epoch 143  	Train Loss = 12.72018 Val Loss = 13.44237
2023-03-26 22:20:08.990356 Epoch 144  	Train Loss = 12.71308 Val Loss = 13.45355
2023-03-26 22:20:24.651668 Epoch 145  	Train Loss = 12.71259 Val Loss = 13.45211
2023-03-26 22:20:40.299145 Epoch 146  	Train Loss = 12.71797 Val Loss = 13.45604
2023-03-26 22:20:55.967932 Epoch 147  	Train Loss = 12.70836 Val Loss = 13.44720
2023-03-26 22:21:11.662466 Epoch 148  	Train Loss = 12.70517 Val Loss = 13.46431
Early stopping at epoch: 148
Best at epoch 138:
Train Loss = 12.73023
Train RMSE = 21.14038, MAE = 13.05200, MAPE = 12.06422
Val Loss = 13.40561
Val RMSE = 22.28976, MAE = 13.96969, MAPE = 12.70297
--------- Test ---------
All Steps RMSE = 26.28553, MAE = 15.07476, MAPE = 14.17872
Step 1 RMSE = 20.07517, MAE = 12.08368, MAPE = 11.99590
Step 2 RMSE = 22.09992, MAE = 13.04152, MAPE = 12.63685
Step 3 RMSE = 23.58347, MAE = 13.72782, MAPE = 13.07937
Step 4 RMSE = 24.73119, MAE = 14.26361, MAPE = 13.43450
Step 5 RMSE = 25.59349, MAE = 14.68633, MAPE = 13.75398
Step 6 RMSE = 26.34923, MAE = 15.08593, MAPE = 14.07324
Step 7 RMSE = 27.08517, MAE = 15.48929, MAPE = 14.43580
Step 8 RMSE = 27.73230, MAE = 15.86040, MAPE = 14.76108
Step 9 RMSE = 28.27231, MAE = 16.17604, MAPE = 15.09329
Step 10 RMSE = 28.77118, MAE = 16.46975, MAPE = 15.28170
Step 11 RMSE = 29.28236, MAE = 16.79435, MAPE = 15.59996
Step 12 RMSE = 29.92243, MAE = 17.21831, MAPE = 15.99903
Inference time: 1.38 s
