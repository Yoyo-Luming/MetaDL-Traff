METRLA
Trainset:	x-(23974, 12, 207, 2)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 2)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 2)	y-(6850, 12, 207, 1)

--------- MTGNN ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "milestones": [
        80
    ],
    "clip_grad": 5,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": true,
    "cl_step_size": 2500,
    "pass_device": true,
    "model_args": {
        "num_nodes": 207,
        "in_dim": 2,
        "seq_length": 12,
        "out_dim": 12,
        "device": "cuda:0",
        "gcn_true": true,
        "buildA_true": true,
        "gcn_depth": 2,
        "predefined_A": null,
        "static_feat": null,
        "dropout": 0.3,
        "subgraph_size": 20,
        "node_dim": 40,
        "dilation_exponential": 1,
        "conv_channels": 32,
        "residual_channels": 32,
        "skip_channels": 64,
        "end_channels": 128,
        "layers": 3,
        "propalpha": 0.05,
        "tanhalpha": 3,
        "layer_norm_affline": true
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
MTGNN                                    [64, 12, 207, 1]          3,168
├─graph_constructor: 1-1                 [207, 207]                --
│    └─Embedding: 2-1                    [207, 40]                 8,280
│    └─Embedding: 2-2                    [207, 40]                 8,280
│    └─Linear: 2-3                       [207, 40]                 1,640
│    └─Linear: 2-4                       [207, 40]                 1,640
├─Conv2d: 1-2                            [64, 32, 207, 19]         96
├─Conv2d: 1-3                            [64, 64, 207, 1]          2,496
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-5            [64, 32, 207, 13]         --
│    │    └─ModuleList: 3-1              --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-6            [64, 32, 207, 13]         --
│    │    └─ModuleList: 3-2              --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-7                       [64, 64, 207, 1]          26,688
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-8                      [64, 32, 207, 13]         --
│    │    └─nconv: 3-3                   [64, 32, 207, 13]         --
│    │    └─nconv: 3-4                   [64, 32, 207, 13]         --
│    │    └─linear: 3-5                  [64, 32, 207, 13]         3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-9                      [64, 32, 207, 13]         --
│    │    └─nconv: 3-6                   [64, 32, 207, 13]         --
│    │    └─nconv: 3-7                   [64, 32, 207, 13]         --
│    │    └─linear: 3-8                  [64, 32, 207, 13]         3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-10                   [64, 32, 207, 13]         172,224
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-11           [64, 32, 207, 7]          --
│    │    └─ModuleList: 3-9              --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-12           [64, 32, 207, 7]          --
│    │    └─ModuleList: 3-10             --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-13                      [64, 64, 207, 1]          14,400
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-14                     [64, 32, 207, 7]          --
│    │    └─nconv: 3-11                  [64, 32, 207, 7]          --
│    │    └─nconv: 3-12                  [64, 32, 207, 7]          --
│    │    └─linear: 3-13                 [64, 32, 207, 7]          3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-15                     [64, 32, 207, 7]          --
│    │    └─nconv: 3-14                  [64, 32, 207, 7]          --
│    │    └─nconv: 3-15                  [64, 32, 207, 7]          --
│    │    └─linear: 3-16                 [64, 32, 207, 7]          3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-16                   [64, 32, 207, 7]          92,736
├─ModuleList: 1-16                       --                        (recursive)
│    └─dilated_inception: 2-17           [64, 32, 207, 1]          --
│    │    └─ModuleList: 3-17             --                        4,640
├─ModuleList: 1-17                       --                        (recursive)
│    └─dilated_inception: 2-18           [64, 32, 207, 1]          --
│    │    └─ModuleList: 3-18             --                        4,640
├─ModuleList: 1-18                       --                        (recursive)
│    └─Conv2d: 2-19                      [64, 64, 207, 1]          2,112
├─ModuleList: 1-19                       --                        (recursive)
│    └─mixprop: 2-20                     [64, 32, 207, 1]          --
│    │    └─nconv: 3-19                  [64, 32, 207, 1]          --
│    │    └─nconv: 3-20                  [64, 32, 207, 1]          --
│    │    └─linear: 3-21                 [64, 32, 207, 1]          3,104
├─ModuleList: 1-20                       --                        (recursive)
│    └─mixprop: 2-21                     [64, 32, 207, 1]          --
│    │    └─nconv: 3-22                  [64, 32, 207, 1]          --
│    │    └─nconv: 3-23                  [64, 32, 207, 1]          --
│    │    └─linear: 3-24                 [64, 32, 207, 1]          3,104
├─ModuleList: 1-21                       --                        (recursive)
│    └─LayerNorm: 2-22                   [64, 32, 207, 1]          13,248
├─Conv2d: 1-22                           [64, 64, 207, 1]          2,112
├─Conv2d: 1-23                           [64, 128, 207, 1]         8,320
├─Conv2d: 1-24                           [64, 12, 207, 1]          1,548
==========================================================================================
Total params: 405,452
Trainable params: 405,452
Non-trainable params: 0
Total mult-adds (G): 5.70
==========================================================================================
Input size (MB): 1.27
Forward/backward pass size (MB): 520.43
Params size (MB): 1.61
Estimated Total Size (MB): 523.32
==========================================================================================

Loss: MaskedMAELoss

CL target length = 1
2023-04-16 12:05:25.289944 Epoch 1  	Train Loss = 2.64637 Val Loss = 10.60226
2023-04-16 12:06:07.168673 Epoch 2  	Train Loss = 2.37679 Val Loss = 10.59741
2023-04-16 12:06:49.955625 Epoch 3  	Train Loss = 2.33952 Val Loss = 10.59705
2023-04-16 12:07:27.927951 Epoch 4  	Train Loss = 2.31131 Val Loss = 10.59458
2023-04-16 12:08:10.080431 Epoch 5  	Train Loss = 2.29416 Val Loss = 10.59439
2023-04-16 12:08:50.500909 Epoch 6  	Train Loss = 2.27231 Val Loss = 10.59193
CL target length = 2
2023-04-16 12:09:31.114250 Epoch 7  	Train Loss = 2.38449 Val Loss = 9.84667
2023-04-16 12:10:13.254410 Epoch 8  	Train Loss = 2.38408 Val Loss = 9.84397
2023-04-16 12:10:53.627398 Epoch 9  	Train Loss = 2.35527 Val Loss = 9.83540
2023-04-16 12:11:34.733656 Epoch 10  	Train Loss = 2.33367 Val Loss = 9.83369
2023-04-16 12:12:16.787725 Epoch 11  	Train Loss = 2.31781 Val Loss = 9.83407
2023-04-16 12:12:57.300298 Epoch 12  	Train Loss = 2.30675 Val Loss = 9.83111
2023-04-16 12:13:38.175805 Epoch 13  	Train Loss = 2.29394 Val Loss = 9.82837
CL target length = 3
2023-04-16 12:14:20.644737 Epoch 14  	Train Loss = 2.42601 Val Loss = 9.09038
2023-04-16 12:14:59.700161 Epoch 15  	Train Loss = 2.40261 Val Loss = 9.08883
2023-04-16 12:15:41.509082 Epoch 16  	Train Loss = 2.39099 Val Loss = 9.09030
2023-04-16 12:16:24.204069 Epoch 17  	Train Loss = 2.38818 Val Loss = 9.08317
2023-04-16 12:17:02.314256 Epoch 18  	Train Loss = 2.37731 Val Loss = 9.08595
2023-04-16 12:17:43.391672 Epoch 19  	Train Loss = 2.37212 Val Loss = 9.08163
CL target length = 4
2023-04-16 12:18:23.445251 Epoch 20  	Train Loss = 2.37452 Val Loss = 9.03419
2023-04-16 12:19:04.811737 Epoch 21  	Train Loss = 2.50366 Val Loss = 8.35388
2023-04-16 12:19:46.663492 Epoch 22  	Train Loss = 2.45844 Val Loss = 8.35806
2023-04-16 12:20:25.624237 Epoch 23  	Train Loss = 2.45305 Val Loss = 8.35045
2023-04-16 12:21:07.683312 Epoch 24  	Train Loss = 2.44852 Val Loss = 8.34957
2023-04-16 12:21:50.283384 Epoch 25  	Train Loss = 2.44287 Val Loss = 8.34606
2023-04-16 12:22:28.684729 Epoch 26  	Train Loss = 2.43911 Val Loss = 8.34837
CL target length = 5
2023-04-16 12:23:10.727647 Epoch 27  	Train Loss = 2.50022 Val Loss = 7.62781
2023-04-16 12:23:53.626539 Epoch 28  	Train Loss = 2.52031 Val Loss = 7.63239
2023-04-16 12:24:32.162689 Epoch 29  	Train Loss = 2.51017 Val Loss = 7.62224
2023-04-16 12:25:13.967910 Epoch 30  	Train Loss = 2.50711 Val Loss = 7.62403
2023-04-16 12:25:54.167740 Epoch 31  	Train Loss = 2.50555 Val Loss = 7.63071
2023-04-16 12:26:35.059708 Epoch 32  	Train Loss = 2.49869 Val Loss = 7.61867
2023-04-16 12:27:17.289281 Epoch 33  	Train Loss = 2.49835 Val Loss = 7.62858
CL target length = 6
2023-04-16 12:27:57.861612 Epoch 34  	Train Loss = 2.57465 Val Loss = 6.90403
2023-04-16 12:28:38.662852 Epoch 35  	Train Loss = 2.56503 Val Loss = 6.90417
2023-04-16 12:29:20.665316 Epoch 36  	Train Loss = 2.56232 Val Loss = 6.90658
2023-04-16 12:30:00.678274 Epoch 37  	Train Loss = 2.55648 Val Loss = 6.90671
2023-04-16 12:30:41.407114 Epoch 38  	Train Loss = 2.55305 Val Loss = 6.90733
2023-04-16 12:31:22.016589 Epoch 39  	Train Loss = 2.55103 Val Loss = 6.90146
CL target length = 7
2023-04-16 12:32:01.705818 Epoch 40  	Train Loss = 2.55302 Val Loss = 6.86011
2023-04-16 12:32:43.973251 Epoch 41  	Train Loss = 2.63404 Val Loss = 6.19055
2023-04-16 12:33:26.008789 Epoch 42  	Train Loss = 2.60733 Val Loss = 6.19689
2023-04-16 12:34:06.296858 Epoch 43  	Train Loss = 2.60360 Val Loss = 6.19895
2023-04-16 12:34:48.604791 Epoch 44  	Train Loss = 2.60296 Val Loss = 6.19849
2023-04-16 12:35:30.380785 Epoch 45  	Train Loss = 2.59786 Val Loss = 6.19624
2023-04-16 12:36:10.059205 Epoch 46  	Train Loss = 2.59449 Val Loss = 6.19853
CL target length = 8
2023-04-16 12:36:52.119997 Epoch 47  	Train Loss = 2.63926 Val Loss = 5.49667
2023-04-16 12:37:32.240429 Epoch 48  	Train Loss = 2.64837 Val Loss = 5.48404
2023-04-16 12:38:12.295668 Epoch 49  	Train Loss = 2.64306 Val Loss = 5.48380
2023-04-16 12:38:55.186787 Epoch 50  	Train Loss = 2.63944 Val Loss = 5.49661
2023-04-16 12:39:33.636327 Epoch 51  	Train Loss = 2.63853 Val Loss = 5.49004
2023-04-16 12:40:15.633944 Epoch 52  	Train Loss = 2.63436 Val Loss = 5.49057
2023-04-16 12:40:56.778936 Epoch 53  	Train Loss = 2.63305 Val Loss = 5.48053
CL target length = 9
2023-04-16 12:41:37.335931 Epoch 54  	Train Loss = 2.69001 Val Loss = 4.79539
2023-04-16 12:42:19.147942 Epoch 55  	Train Loss = 2.68130 Val Loss = 4.79683
2023-04-16 12:43:01.849501 Epoch 56  	Train Loss = 2.67312 Val Loss = 4.78572
2023-04-16 12:43:40.080475 Epoch 57  	Train Loss = 2.67099 Val Loss = 4.82794
2023-04-16 12:44:22.010771 Epoch 58  	Train Loss = 2.67119 Val Loss = 4.79222
2023-04-16 12:45:03.914566 Epoch 59  	Train Loss = 2.66955 Val Loss = 4.78354
CL target length = 10
2023-04-16 12:45:43.079529 Epoch 60  	Train Loss = 2.66695 Val Loss = 4.74249
2023-04-16 12:46:24.968519 Epoch 61  	Train Loss = 2.72943 Val Loss = 4.10013
2023-04-16 12:47:06.595921 Epoch 62  	Train Loss = 2.70401 Val Loss = 4.09750
2023-04-16 12:47:46.434784 Epoch 63  	Train Loss = 2.70341 Val Loss = 4.09976
2023-04-16 12:48:28.611348 Epoch 64  	Train Loss = 2.70006 Val Loss = 4.11163
2023-04-16 12:49:10.377968 Epoch 65  	Train Loss = 2.69968 Val Loss = 4.11241
2023-04-16 12:49:50.017156 Epoch 66  	Train Loss = 2.69666 Val Loss = 4.10861
CL target length = 11
2023-04-16 12:50:32.264402 Epoch 67  	Train Loss = 2.72458 Val Loss = 3.42086
2023-04-16 12:51:12.416419 Epoch 68  	Train Loss = 2.73434 Val Loss = 3.41863
2023-04-16 12:51:53.248959 Epoch 69  	Train Loss = 2.72894 Val Loss = 3.40036
2023-04-16 12:52:35.315116 Epoch 70  	Train Loss = 2.72708 Val Loss = 3.41266
2023-04-16 12:53:15.103402 Epoch 71  	Train Loss = 2.72417 Val Loss = 3.42765
2023-04-16 12:53:56.702979 Epoch 72  	Train Loss = 2.72128 Val Loss = 3.39353
2023-04-16 12:54:38.799535 Epoch 73  	Train Loss = 2.72068 Val Loss = 3.40517
CL target length = 12
2023-04-16 12:55:17.667394 Epoch 74  	Train Loss = 2.75886 Val Loss = 2.74130
2023-04-16 12:56:00.498390 Epoch 75  	Train Loss = 2.75243 Val Loss = 2.73732
2023-04-16 12:56:43.045674 Epoch 76  	Train Loss = 2.75142 Val Loss = 2.74364
2023-04-16 12:57:21.952180 Epoch 77  	Train Loss = 2.74957 Val Loss = 2.73725
2023-04-16 12:58:03.768665 Epoch 78  	Train Loss = 2.74075 Val Loss = 2.73941
2023-04-16 12:58:45.458691 Epoch 79  	Train Loss = 2.74345 Val Loss = 2.73221
2023-04-16 12:59:24.592402 Epoch 80  	Train Loss = 2.73901 Val Loss = 2.73492
2023-04-16 13:00:07.001359 Epoch 81  	Train Loss = 2.69770 Val Loss = 2.70515
2023-04-16 13:00:47.049235 Epoch 82  	Train Loss = 2.68923 Val Loss = 2.70320
2023-04-16 13:01:28.224520 Epoch 83  	Train Loss = 2.68734 Val Loss = 2.70245
2023-04-16 13:02:10.495298 Epoch 84  	Train Loss = 2.68529 Val Loss = 2.70641
2023-04-16 13:02:50.538305 Epoch 85  	Train Loss = 2.68481 Val Loss = 2.70710
2023-04-16 13:03:32.190039 Epoch 86  	Train Loss = 2.68404 Val Loss = 2.70262
2023-04-16 13:04:14.901334 Epoch 87  	Train Loss = 2.68288 Val Loss = 2.70418
2023-04-16 13:04:54.196789 Epoch 88  	Train Loss = 2.68201 Val Loss = 2.70883
2023-04-16 13:05:36.176693 Epoch 89  	Train Loss = 2.68029 Val Loss = 2.70953
2023-04-16 13:06:18.080928 Epoch 90  	Train Loss = 2.67954 Val Loss = 2.70023
2023-04-16 13:06:56.669548 Epoch 91  	Train Loss = 2.67850 Val Loss = 2.70021
2023-04-16 13:07:38.636462 Epoch 92  	Train Loss = 2.67785 Val Loss = 2.71247
2023-04-16 13:08:20.736062 Epoch 93  	Train Loss = 2.67738 Val Loss = 2.70805
2023-04-16 13:08:58.688794 Epoch 94  	Train Loss = 2.67657 Val Loss = 2.70804
2023-04-16 13:09:31.426739 Epoch 95  	Train Loss = 2.67471 Val Loss = 2.71711
2023-04-16 13:10:04.476266 Epoch 96  	Train Loss = 2.67494 Val Loss = 2.70629
2023-04-16 13:10:36.986376 Epoch 97  	Train Loss = 2.67452 Val Loss = 2.70823
2023-04-16 13:11:09.558326 Epoch 98  	Train Loss = 2.67410 Val Loss = 2.70471
2023-04-16 13:11:41.985231 Epoch 99  	Train Loss = 2.67285 Val Loss = 2.70894
2023-04-16 13:12:14.557115 Epoch 100  	Train Loss = 2.67364 Val Loss = 2.71085
2023-04-16 13:12:47.291487 Epoch 101  	Train Loss = 2.67268 Val Loss = 2.70484
Early stopping at epoch: 101
Best at epoch 91:
Train Loss = 2.67850
Train RMSE = 5.22019, MAE = 2.62360, MAPE = 6.83215
Val Loss = 2.70021
Val RMSE = 5.73466, MAE = 2.73633, MAPE = 7.65826
--------- Test ---------
All Steps RMSE = 6.11265, MAE = 2.99719, MAPE = 8.05698
Step 1 RMSE = 3.90788, MAE = 2.23362, MAPE = 5.39635
Step 2 RMSE = 4.63289, MAE = 2.48447, MAPE = 6.20141
Step 3 RMSE = 5.14122, MAE = 2.66451, MAPE = 6.83322
Step 4 RMSE = 5.55771, MAE = 2.81039, MAPE = 7.36234
Step 5 RMSE = 5.88714, MAE = 2.92907, MAPE = 7.78563
Step 6 RMSE = 6.16391, MAE = 3.03124, MAPE = 8.15427
Step 7 RMSE = 6.40537, MAE = 3.12121, MAPE = 8.48427
Step 8 RMSE = 6.61085, MAE = 3.20169, MAPE = 8.77613
Step 9 RMSE = 6.78975, MAE = 3.27470, MAPE = 9.05872
Step 10 RMSE = 6.95230, MAE = 3.34161, MAPE = 9.31669
Step 11 RMSE = 7.09197, MAE = 3.40415, MAPE = 9.54785
Step 12 RMSE = 7.22527, MAE = 3.46973, MAPE = 9.76716
Inference time: 2.82 s
