PEMS08
Original data shape (17856, 170, 3)
Trainset:	x-(10690, 12, 170, 1)	y-(10690, 12, 170, 1)
Valset:  	x-(3548, 12, 170, 1)  	y-(3548, 12, 170, 1)
Testset:	x-(3549, 12, 170, 1)	y-(3549, 12, 170, 1)

--------- STWA ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "lr": 0.001,
    "milestones": [
        40
    ],
    "clip_grad": false,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "load_npz": false,
    "pass_device": true,
    "model_args": {
        "num_nodes": 170,
        "input_dim": 1,
        "output_dim": 1,
        "lag": 12,
        "horizon": 12,
        "device": "cuda:0",
        "channels": 16,
        "memory_size": 16,
        "dynamic": true
    }
}
====================================================================================================
Layer (type:depth-idx)                             Output Shape              Param #
====================================================================================================
STWA                                               [64, 12, 170, 1]          --
├─Sequential: 1-1                                  [64, 170, 16]             --
│    └─Linear: 2-1                                 [64, 170, 32]             416
│    └─Tanh: 2-2                                   [64, 170, 32]             --
│    └─Linear: 2-3                                 [64, 170, 32]             1,056
│    └─Tanh: 2-4                                   [64, 170, 32]             --
│    └─Linear: 2-5                                 [64, 170, 16]             528
├─Sequential: 1-2                                  [64, 170, 16]             --
│    └─Linear: 2-6                                 [64, 170, 32]             416
│    └─Tanh: 2-7                                   [64, 170, 32]             --
│    └─Linear: 2-8                                 [64, 170, 32]             1,056
│    └─Tanh: 2-9                                   [64, 170, 32]             --
│    └─Linear: 2-10                                [64, 170, 16]             528
├─Linear: 1-3                                      [64, 12, 170, 16]         32
├─ModuleList: 1-8                                  --                        (recursive)
│    └─Layer: 2-11                                 [64, 12, 170, 16]         70,720
│    │    └─ModuleList: 3-1                        --                        6,100
│    │    └─ModuleList: 3-2                        --                        6,100
│    │    └─TemporalAttention: 3-3                 [64, 2, 170, 16]          544
│    │    └─SpatialAttention: 3-4                  [64, 2, 170, 16]          544
│    │    └─Sequential: 3-5                        [64, 2, 170, 16]          544
│    │    └─TemporalAttention: 3-6                 [64, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-7                  [64, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-8                        [64, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-9                 [64, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-10                 [64, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-11                       [64, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-12                [64, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-13                 [64, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-14                       [64, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-15                [64, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-16                 [64, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-17                       [64, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-18                [64, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-19                 [64, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-20                       [64, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-21                [64, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-22                 [64, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-23                       [64, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-24                [64, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-25                 [64, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-26                       [64, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-27                [64, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-28                 [64, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-29                       [64, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-30                [64, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-31                 [64, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-32                       [64, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-33                [64, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-34                 [64, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-35                       [64, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-36                [64, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-37                 [64, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-38                       [64, 2, 170, 16]          (recursive)
├─ModuleList: 1-9                                  --                        (recursive)
│    └─Linear: 2-12                                [64, 170, 256]            49,408
├─ModuleList: 1-8                                  --                        (recursive)
│    └─Layer: 2-13                                 [64, 3, 170, 16]          21,760
│    │    └─ModuleList: 3-39                       --                        6,100
│    │    └─ModuleList: 3-40                       --                        6,100
│    │    └─TemporalAttention: 3-41                [64, 2, 170, 16]          544
│    │    └─SpatialAttention: 3-42                 [64, 2, 170, 16]          544
│    │    └─Sequential: 3-43                       [64, 2, 170, 16]          544
│    │    └─TemporalAttention: 3-44                [64, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-45                 [64, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-46                       [64, 2, 170, 16]          (recursive)
│    │    └─TemporalAttention: 3-47                [64, 2, 170, 16]          (recursive)
│    │    └─SpatialAttention: 3-48                 [64, 2, 170, 16]          (recursive)
│    │    └─Sequential: 3-49                       [64, 2, 170, 16]          (recursive)
├─ModuleList: 1-9                                  --                        (recursive)
│    └─Linear: 2-14                                [64, 170, 256]            12,544
├─ModuleList: 1-8                                  --                        (recursive)
│    └─Layer: 2-15                                 [64, 1, 170, 16]          10,880
│    │    └─ModuleList: 3-50                       --                        6,100
│    │    └─ModuleList: 3-51                       --                        6,100
│    │    └─TemporalAttention: 3-52                [64, 2, 170, 16]          544
│    │    └─SpatialAttention: 3-53                 [64, 2, 170, 16]          544
│    │    └─Sequential: 3-54                       [64, 2, 170, 16]          544
├─ModuleList: 1-9                                  --                        (recursive)
│    └─Linear: 2-16                                [64, 170, 256]            4,352
├─Sequential: 1-10                                 [64, 170, 12]             --
│    └─Linear: 2-17                                [64, 170, 512]            131,584
│    └─ReLU: 2-18                                  [64, 170, 512]            --
│    └─Linear: 2-19                                [64, 170, 12]             6,156
====================================================================================================
Total params: 352,932
Trainable params: 352,932
Non-trainable params: 0
Total mult-adds (M): 17.33
====================================================================================================
Input size (MB): 0.52
Forward/backward pass size (MB): 554.62
Params size (MB): 1.00
Estimated Total Size (MB): 556.14
====================================================================================================

Loss: HuberLoss

2023-03-26 17:09:33.830604 Epoch 1  	Train Loss = 47.11752 Val Loss = 26.99786
2023-03-26 17:10:14.278240 Epoch 2  	Train Loss = 23.52601 Val Loss = 23.24442
2023-03-26 17:10:54.360923 Epoch 3  	Train Loss = 21.52918 Val Loss = 21.44723
2023-03-26 17:11:34.808904 Epoch 4  	Train Loss = 20.03696 Val Loss = 20.56915
2023-03-26 17:12:14.946692 Epoch 5  	Train Loss = 19.43091 Val Loss = 20.69141
2023-03-26 17:12:55.086628 Epoch 6  	Train Loss = 18.80960 Val Loss = 19.43027
2023-03-26 17:13:35.260557 Epoch 7  	Train Loss = 18.31671 Val Loss = 21.10362
2023-03-26 17:14:15.623433 Epoch 8  	Train Loss = 18.03861 Val Loss = 19.76981
2023-03-26 17:14:55.788910 Epoch 9  	Train Loss = 17.66881 Val Loss = 18.46481
2023-03-26 17:15:36.172513 Epoch 10  	Train Loss = 17.32294 Val Loss = 19.91258
2023-03-26 17:16:16.500166 Epoch 11  	Train Loss = 17.34892 Val Loss = 18.20705
2023-03-26 17:16:56.855892 Epoch 12  	Train Loss = 17.05177 Val Loss = 17.91231
2023-03-26 17:17:37.406247 Epoch 13  	Train Loss = 16.85625 Val Loss = 18.23516
2023-03-26 17:18:18.201797 Epoch 14  	Train Loss = 16.69322 Val Loss = 18.33834
2023-03-26 17:18:58.697656 Epoch 15  	Train Loss = 16.69119 Val Loss = 17.82949
2023-03-26 17:19:38.928044 Epoch 16  	Train Loss = 16.44730 Val Loss = 19.92251
2023-03-26 17:20:19.286032 Epoch 17  	Train Loss = 16.48717 Val Loss = 17.10089
2023-03-26 17:20:59.439350 Epoch 18  	Train Loss = 16.26711 Val Loss = 18.84002
2023-03-26 17:21:39.579338 Epoch 19  	Train Loss = 16.18165 Val Loss = 17.08927
2023-03-26 17:22:19.720069 Epoch 20  	Train Loss = 16.02265 Val Loss = 16.94042
2023-03-26 17:22:59.829799 Epoch 21  	Train Loss = 15.98055 Val Loss = 16.75121
2023-03-26 17:23:39.954071 Epoch 22  	Train Loss = 15.82844 Val Loss = 19.77575
2023-03-26 17:24:20.098748 Epoch 23  	Train Loss = 15.97257 Val Loss = 17.04819
2023-03-26 17:25:00.539544 Epoch 24  	Train Loss = 15.70979 Val Loss = 16.76077
2023-03-26 17:25:41.026573 Epoch 25  	Train Loss = 15.52978 Val Loss = 16.78424
2023-03-26 17:26:21.425232 Epoch 26  	Train Loss = 15.55515 Val Loss = 18.83664
2023-03-26 17:27:01.977709 Epoch 27  	Train Loss = 15.59213 Val Loss = 16.94649
2023-03-26 17:27:42.443686 Epoch 28  	Train Loss = 15.45136 Val Loss = 16.89919
2023-03-26 17:28:22.711149 Epoch 29  	Train Loss = 15.36512 Val Loss = 17.82575
2023-03-26 17:29:02.928972 Epoch 30  	Train Loss = 15.51815 Val Loss = 16.61780
2023-03-26 17:29:43.125566 Epoch 31  	Train Loss = 15.19325 Val Loss = 16.52495
2023-03-26 17:30:23.332281 Epoch 32  	Train Loss = 15.16758 Val Loss = 16.68145
2023-03-26 17:31:03.520036 Epoch 33  	Train Loss = 15.15154 Val Loss = 17.56215
2023-03-26 17:31:43.695905 Epoch 34  	Train Loss = 15.18777 Val Loss = 16.36065
2023-03-26 17:32:23.896619 Epoch 35  	Train Loss = 15.12373 Val Loss = 16.14271
2023-03-26 17:33:04.273092 Epoch 36  	Train Loss = 14.97453 Val Loss = 16.20939
2023-03-26 17:33:44.615417 Epoch 37  	Train Loss = 15.02269 Val Loss = 16.36645
2023-03-26 17:34:24.987059 Epoch 38  	Train Loss = 15.04400 Val Loss = 16.90864
2023-03-26 17:35:05.415980 Epoch 39  	Train Loss = 14.88995 Val Loss = 17.10271
2023-03-26 17:35:45.805431 Epoch 40  	Train Loss = 14.96083 Val Loss = 16.63118
2023-03-26 17:36:26.087871 Epoch 41  	Train Loss = 14.53222 Val Loss = 15.91893
2023-03-26 17:37:06.243100 Epoch 42  	Train Loss = 14.44449 Val Loss = 15.92417
2023-03-26 17:37:46.514623 Epoch 43  	Train Loss = 14.42094 Val Loss = 15.96296
2023-03-26 17:38:26.693474 Epoch 44  	Train Loss = 14.43488 Val Loss = 15.95131
2023-03-26 17:39:06.840464 Epoch 45  	Train Loss = 14.40882 Val Loss = 15.95983
2023-03-26 17:39:46.992082 Epoch 46  	Train Loss = 14.42539 Val Loss = 15.97712
2023-03-26 17:40:27.153214 Epoch 47  	Train Loss = 14.41787 Val Loss = 15.92239
2023-03-26 17:41:07.327724 Epoch 48  	Train Loss = 14.40000 Val Loss = 15.96825
2023-03-26 17:41:47.512460 Epoch 49  	Train Loss = 14.38437 Val Loss = 15.89586
2023-03-26 17:42:27.727046 Epoch 50  	Train Loss = 14.35866 Val Loss = 15.88558
2023-03-26 17:43:07.946411 Epoch 51  	Train Loss = 14.39290 Val Loss = 15.95046
2023-03-26 17:43:48.174084 Epoch 52  	Train Loss = 14.38752 Val Loss = 15.88753
2023-03-26 17:44:28.359599 Epoch 53  	Train Loss = 14.34259 Val Loss = 15.94005
2023-03-26 17:45:08.633717 Epoch 54  	Train Loss = 14.34285 Val Loss = 16.08057
2023-03-26 17:45:48.907657 Epoch 55  	Train Loss = 14.36486 Val Loss = 15.89616
2023-03-26 17:46:29.604697 Epoch 56  	Train Loss = 14.31598 Val Loss = 15.87097
2023-03-26 17:47:09.876359 Epoch 57  	Train Loss = 14.35991 Val Loss = 15.88394
2023-03-26 17:47:50.215863 Epoch 58  	Train Loss = 14.33511 Val Loss = 15.89867
2023-03-26 17:48:30.452047 Epoch 59  	Train Loss = 14.31489 Val Loss = 15.84895
2023-03-26 17:49:10.703161 Epoch 60  	Train Loss = 14.31902 Val Loss = 15.94756
2023-03-26 17:49:50.945088 Epoch 61  	Train Loss = 14.29897 Val Loss = 15.99330
2023-03-26 17:50:31.164628 Epoch 62  	Train Loss = 14.31655 Val Loss = 15.82957
2023-03-26 17:51:11.718943 Epoch 63  	Train Loss = 14.27880 Val Loss = 16.02106
2023-03-26 17:51:51.874478 Epoch 64  	Train Loss = 14.30006 Val Loss = 15.87254
2023-03-26 17:52:31.987077 Epoch 65  	Train Loss = 14.26209 Val Loss = 16.08366
2023-03-26 17:53:12.141338 Epoch 66  	Train Loss = 14.27738 Val Loss = 15.82568
2023-03-26 17:53:52.371218 Epoch 67  	Train Loss = 14.25518 Val Loss = 15.94139
2023-03-26 17:54:32.569040 Epoch 68  	Train Loss = 14.26255 Val Loss = 15.86166
2023-03-26 17:55:12.681891 Epoch 69  	Train Loss = 14.22532 Val Loss = 16.12462
2023-03-26 17:55:52.916449 Epoch 70  	Train Loss = 14.24802 Val Loss = 15.96517
2023-03-26 17:56:33.259421 Epoch 71  	Train Loss = 14.23099 Val Loss = 15.94760
2023-03-26 17:57:13.912786 Epoch 72  	Train Loss = 14.24509 Val Loss = 15.91105
2023-03-26 17:57:54.139059 Epoch 73  	Train Loss = 14.26718 Val Loss = 16.02676
2023-03-26 17:58:34.470059 Epoch 74  	Train Loss = 14.19029 Val Loss = 15.97313
2023-03-26 17:59:14.732096 Epoch 75  	Train Loss = 14.22127 Val Loss = 15.96862
2023-03-26 17:59:54.963358 Epoch 76  	Train Loss = 14.20894 Val Loss = 15.88048
Early stopping at epoch: 76
Best at epoch 66:
Train Loss = 14.27738
Train RMSE = 24.28446, MAE = 14.67639, MAPE = 9.49865
Val Loss = 15.82568
Val RMSE = 26.24176, MAE = 16.37163, MAPE = 11.83170
--------- Test ---------
All Steps RMSE = 25.27887, MAE = 16.05807, MAPE = 10.37077
Step 1 RMSE = 21.99640, MAE = 14.24121, MAPE = 9.39775
Step 2 RMSE = 22.77611, MAE = 14.64551, MAPE = 9.52257
Step 3 RMSE = 23.47906, MAE = 15.04875, MAPE = 9.81840
Step 4 RMSE = 24.09589, MAE = 15.39642, MAPE = 10.04949
Step 5 RMSE = 24.64939, MAE = 15.70287, MAPE = 10.09328
Step 6 RMSE = 25.18596, MAE = 16.00181, MAPE = 10.18848
Step 7 RMSE = 25.65506, MAE = 16.25627, MAPE = 10.35875
Step 8 RMSE = 26.06592, MAE = 16.48079, MAPE = 10.60965
Step 9 RMSE = 26.45819, MAE = 16.71074, MAPE = 10.80157
Step 10 RMSE = 26.91211, MAE = 17.01722, MAPE = 10.96147
Step 11 RMSE = 27.36754, MAE = 17.37883, MAPE = 11.23752
Step 12 RMSE = 27.94577, MAE = 17.81639, MAPE = 11.41027
Inference time: 2.31 s
