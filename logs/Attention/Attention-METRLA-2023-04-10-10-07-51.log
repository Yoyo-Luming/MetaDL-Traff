METRLA
Trainset:	x-(23974, 12, 207, 2)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 2)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 2)	y-(6850, 12, 207, 1)

--------- Attention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.7,
    "val_size": 0.1,
    "time_of_day": true,
    "lr": 0.001,
    "milestones": [
        10,
        60
    ],
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "model_args": {
        "num_nodes": 207,
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 64,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Attention                                --                        --
├─Linear: 1-1                            [64, 207, 12, 64]         128
├─Linear: 1-2                            [64, 207, 12, 64]         128
├─Sequential: 1-3                        [64, 207, 12, 64]         --
│    └─SelfAttentionLayer: 2-1           [64, 207, 12, 64]         --
│    │    └─AttentionLayer: 3-1          [64, 207, 12, 64]         16,640
│    │    └─LayerNorm: 3-2               [64, 207, 12, 64]         128
│    │    └─Sequential: 3-3              [64, 207, 12, 64]         33,088
│    │    └─LayerNorm: 3-4               [64, 207, 12, 64]         128
│    └─SelfAttentionLayer: 2-2           [64, 207, 12, 64]         --
│    │    └─AttentionLayer: 3-5          [64, 207, 12, 64]         16,640
│    │    └─LayerNorm: 3-6               [64, 207, 12, 64]         128
│    │    └─Sequential: 3-7              [64, 207, 12, 64]         33,088
│    │    └─LayerNorm: 3-8               [64, 207, 12, 64]         128
│    └─SelfAttentionLayer: 2-3           [64, 207, 12, 64]         --
│    │    └─AttentionLayer: 3-9          [64, 207, 12, 64]         16,640
│    │    └─LayerNorm: 3-10              [64, 207, 12, 64]         128
│    │    └─Sequential: 3-11             [64, 207, 12, 64]         33,088
│    │    └─LayerNorm: 3-12              [64, 207, 12, 64]         128
├─Linear: 1-4                            [64, 207, 64, 12]         156
├─Linear: 1-5                            [64, 207, 12, 1]          65
==========================================================================================
Total params: 150,429
Trainable params: 150,429
Non-trainable params: 0
Total mult-adds (M): 9.63
==========================================================================================
Input size (MB): 1.27
Forward/backward pass size (MB): 2931.52
Params size (MB): 0.60
Estimated Total Size (MB): 2933.39
==========================================================================================

Loss: MaskedMAELoss

2023-04-10 10:08:28.959857 Epoch 1  	Train Loss = 4.64866 Val Loss = 3.93895
2023-04-10 10:09:02.660498 Epoch 2  	Train Loss = 3.81175 Val Loss = 3.40180
2023-04-10 10:09:36.718250 Epoch 3  	Train Loss = 3.66506 Val Loss = 3.38507
2023-04-10 10:10:11.447011 Epoch 4  	Train Loss = 3.58594 Val Loss = 3.37907
2023-04-10 10:10:47.320850 Epoch 5  	Train Loss = 3.58299 Val Loss = 3.35353
2023-04-10 10:11:23.795893 Epoch 6  	Train Loss = 3.55174 Val Loss = 3.33703
2023-04-10 10:12:00.586039 Epoch 7  	Train Loss = 3.54866 Val Loss = 3.35162
2023-04-10 10:12:37.728558 Epoch 8  	Train Loss = 3.54765 Val Loss = 3.36730
2023-04-10 10:13:15.255178 Epoch 9  	Train Loss = 3.53295 Val Loss = 3.33054
2023-04-10 10:13:53.352952 Epoch 10  	Train Loss = 3.52793 Val Loss = 3.34524
2023-04-10 10:14:31.536376 Epoch 11  	Train Loss = 3.48801 Val Loss = 3.29281
2023-04-10 10:15:09.772493 Epoch 12  	Train Loss = 3.48108 Val Loss = 3.28813
2023-04-10 10:15:48.005398 Epoch 13  	Train Loss = 3.47864 Val Loss = 3.28938
2023-04-10 10:16:26.020040 Epoch 14  	Train Loss = 3.47834 Val Loss = 3.28770
2023-04-10 10:17:03.162315 Epoch 15  	Train Loss = 3.47724 Val Loss = 3.28441
2023-04-10 10:17:39.370176 Epoch 16  	Train Loss = 3.47650 Val Loss = 3.28538
2023-04-10 10:18:15.121089 Epoch 17  	Train Loss = 3.47524 Val Loss = 3.28759
2023-04-10 10:18:50.946818 Epoch 18  	Train Loss = 3.47313 Val Loss = 3.28360
2023-04-10 10:19:26.842305 Epoch 19  	Train Loss = 3.47132 Val Loss = 3.29370
2023-04-10 10:20:02.917850 Epoch 20  	Train Loss = 3.47076 Val Loss = 3.28746
2023-04-10 10:20:39.236898 Epoch 21  	Train Loss = 3.46876 Val Loss = 3.28235
2023-04-10 10:21:16.285045 Epoch 22  	Train Loss = 3.46776 Val Loss = 3.28203
2023-04-10 10:21:53.794860 Epoch 23  	Train Loss = 3.48375 Val Loss = 3.34574
2023-04-10 10:22:31.492332 Epoch 24  	Train Loss = 3.50771 Val Loss = 3.31142
2023-04-10 10:23:09.274603 Epoch 25  	Train Loss = 3.48798 Val Loss = 3.29610
2023-04-10 10:23:47.116565 Epoch 26  	Train Loss = 3.47958 Val Loss = 3.29099
2023-04-10 10:24:25.144820 Epoch 27  	Train Loss = 3.47415 Val Loss = 3.28221
2023-04-10 10:25:03.200289 Epoch 28  	Train Loss = 3.46889 Val Loss = 3.27691
2023-04-10 10:25:41.490007 Epoch 29  	Train Loss = 3.46364 Val Loss = 3.27786
2023-04-10 10:26:19.830443 Epoch 30  	Train Loss = 3.46062 Val Loss = 3.26959
2023-04-10 10:26:58.432190 Epoch 31  	Train Loss = 3.45688 Val Loss = 3.26587
2023-04-10 10:27:37.380002 Epoch 32  	Train Loss = 3.45250 Val Loss = 3.26666
2023-04-10 10:28:16.837156 Epoch 33  	Train Loss = 3.45011 Val Loss = 3.26723
2023-04-10 10:28:56.562542 Epoch 34  	Train Loss = 3.44727 Val Loss = 3.25853
2023-04-10 10:29:36.250081 Epoch 35  	Train Loss = 3.44622 Val Loss = 3.27098
2023-04-10 10:30:15.990173 Epoch 36  	Train Loss = 3.44912 Val Loss = 3.25876
2023-04-10 10:30:55.258942 Epoch 37  	Train Loss = 3.44123 Val Loss = 3.25770
2023-04-10 10:31:33.516666 Epoch 38  	Train Loss = 3.43898 Val Loss = 3.26447
2023-04-10 10:32:11.196866 Epoch 39  	Train Loss = 3.43792 Val Loss = 3.26049
2023-04-10 10:32:48.601488 Epoch 40  	Train Loss = 3.43549 Val Loss = 3.25148
2023-04-10 10:33:26.168611 Epoch 41  	Train Loss = 3.43331 Val Loss = 3.25116
2023-04-10 10:34:03.838894 Epoch 42  	Train Loss = 3.43092 Val Loss = 3.25605
2023-04-10 10:34:41.508731 Epoch 43  	Train Loss = 3.43113 Val Loss = 3.25444
2023-04-10 10:35:19.286099 Epoch 44  	Train Loss = 3.42773 Val Loss = 3.25110
2023-04-10 10:35:57.101751 Epoch 45  	Train Loss = 3.42790 Val Loss = 3.24869
2023-04-10 10:36:34.846141 Epoch 46  	Train Loss = 3.42735 Val Loss = 3.25770
2023-04-10 10:37:12.619075 Epoch 47  	Train Loss = 3.42601 Val Loss = 3.25191
2023-04-10 10:37:50.422371 Epoch 48  	Train Loss = 3.42516 Val Loss = 3.24581
2023-04-10 10:38:28.293203 Epoch 49  	Train Loss = 3.42581 Val Loss = 3.25010
2023-04-10 10:39:06.211874 Epoch 50  	Train Loss = 3.42346 Val Loss = 3.24488
2023-04-10 10:39:44.309502 Epoch 51  	Train Loss = 3.42217 Val Loss = 3.24704
2023-04-10 10:40:22.383513 Epoch 52  	Train Loss = 3.42257 Val Loss = 3.24380
2023-04-10 10:41:00.596963 Epoch 53  	Train Loss = 3.42039 Val Loss = 3.24877
2023-04-10 10:41:39.046304 Epoch 54  	Train Loss = 3.42265 Val Loss = 3.26071
2023-04-10 10:42:17.793200 Epoch 55  	Train Loss = 3.41958 Val Loss = 3.24434
2023-04-10 10:42:57.024316 Epoch 56  	Train Loss = 3.41958 Val Loss = 3.23917
2023-04-10 10:43:36.397778 Epoch 57  	Train Loss = 3.41797 Val Loss = 3.24321
2023-04-10 10:44:15.927466 Epoch 58  	Train Loss = 3.41730 Val Loss = 3.23757
2023-04-10 10:44:55.496710 Epoch 59  	Train Loss = 3.41866 Val Loss = 3.24825
2023-04-10 10:45:34.561618 Epoch 60  	Train Loss = 3.41565 Val Loss = 3.24676
2023-04-10 10:46:12.451394 Epoch 61  	Train Loss = 3.40971 Val Loss = 3.23663
2023-04-10 10:46:49.475035 Epoch 62  	Train Loss = 3.40769 Val Loss = 3.23626
2023-04-10 10:47:25.985512 Epoch 63  	Train Loss = 3.40817 Val Loss = 3.23769
2023-04-10 10:48:02.432344 Epoch 64  	Train Loss = 3.40750 Val Loss = 3.23682
2023-04-10 10:48:38.868978 Epoch 65  	Train Loss = 3.40693 Val Loss = 3.23771
2023-04-10 10:49:15.264204 Epoch 66  	Train Loss = 3.40752 Val Loss = 3.23816
2023-04-10 10:49:51.690421 Epoch 67  	Train Loss = 3.40736 Val Loss = 3.23550
2023-04-10 10:50:28.092729 Epoch 68  	Train Loss = 3.40743 Val Loss = 3.23636
2023-04-10 10:51:04.490955 Epoch 69  	Train Loss = 3.40713 Val Loss = 3.23697
2023-04-10 10:51:40.981200 Epoch 70  	Train Loss = 3.40709 Val Loss = 3.23668
2023-04-10 10:52:17.461405 Epoch 71  	Train Loss = 3.40640 Val Loss = 3.23613
2023-04-10 10:52:53.981413 Epoch 72  	Train Loss = 3.40758 Val Loss = 3.23756
2023-04-10 10:53:30.585497 Epoch 73  	Train Loss = 3.40692 Val Loss = 3.23724
2023-04-10 10:54:07.284267 Epoch 74  	Train Loss = 3.40636 Val Loss = 3.23572
2023-04-10 10:54:44.087081 Epoch 75  	Train Loss = 3.40681 Val Loss = 3.23636
2023-04-10 10:55:20.940633 Epoch 76  	Train Loss = 3.40629 Val Loss = 3.23579
2023-04-10 10:55:58.043527 Epoch 77  	Train Loss = 3.40588 Val Loss = 3.23812
Early stopping at epoch: 77
Best at epoch 67:
Train Loss = 3.40736
Train RMSE = 6.96714, MAE = 3.40568, MAPE = 9.46944
Val Loss = 3.23550
Val RMSE = 6.88292, MAE = 3.27619, MAPE = 9.50683
--------- Test ---------
All Steps RMSE = 7.30168, MAE = 3.59305, MAPE = 10.36872
Step 1 RMSE = 4.30499, MAE = 2.41158, MAPE = 5.99965
Step 2 RMSE = 5.22961, MAE = 2.73724, MAPE = 7.08927
Step 3 RMSE = 5.91542, MAE = 2.99872, MAPE = 8.03170
Step 4 RMSE = 6.41474, MAE = 3.21284, MAPE = 8.84233
Step 5 RMSE = 6.86862, MAE = 3.40893, MAPE = 9.57716
Step 6 RMSE = 7.24149, MAE = 3.59539, MAPE = 10.29371
Step 7 RMSE = 7.60561, MAE = 3.76189, MAPE = 11.00281
Step 8 RMSE = 7.92144, MAE = 3.91756, MAPE = 11.60670
Step 9 RMSE = 8.20976, MAE = 4.06289, MAPE = 12.21304
Step 10 RMSE = 8.49350, MAE = 4.20164, MAPE = 12.77927
Step 11 RMSE = 8.75373, MAE = 4.33529, MAPE = 13.25680
Step 12 RMSE = 9.02090, MAE = 4.47279, MAPE = 13.73287
Inference time: 3.30 s
