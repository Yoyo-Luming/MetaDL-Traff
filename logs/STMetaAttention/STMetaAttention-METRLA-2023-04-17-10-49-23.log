METRLA
Trainset:	x-(23974, 12, 207, 3)	y-(23974, 12, 207, 1)
Valset:  	x-(3425, 12, 207, 3)  	y-(3425, 12, 207, 1)
Testset:	x-(6850, 12, 207, 3)	y-(6850, 12, 207, 1)

--------- STMetaAttention ---------
{
    "num_nodes": 207,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0,
    "milestones": [
        10,
        30
    ],
    "clip_grad": false,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "pass_device": true,
    "model_args": {
        "num_nodes": 207,
        "node_emb_file": "../data/METRLA/spatial_embeddings.npz",
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 32,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 7,
        "node_embedding_dim": 64,
        "z_dim": 32,
        "learner_hidden_dim": 64,
        "feed_forward_dim": 64,
        "num_heads": 4,
        "num_layers": 1,
        "dropout": 0,
        "with_spatial": true,
        "device": "cuda:0"
    }
}
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
STMetaAttention                               [64, 12, 207, 1]          13,248
├─Linear: 1-1                                 [64, 12, 207, 32]         64
├─Sequential: 1-2                             [64, 207, 32]             --
│    └─Linear: 2-1                            [64, 207, 32]             416
│    └─Tanh: 2-2                              [64, 207, 32]             --
│    └─Linear: 2-3                            [64, 207, 32]             1,056
│    └─Tanh: 2-4                              [64, 207, 32]             --
│    └─Linear: 2-5                            [64, 207, 32]             1,056
├─Sequential: 1-3                             [64, 207, 32]             --
│    └─Linear: 2-6                            [64, 207, 32]             416
│    └─Tanh: 2-7                              [64, 207, 32]             --
│    └─Linear: 2-8                            [64, 207, 32]             1,056
│    └─Tanh: 2-9                              [64, 207, 32]             --
│    └─Linear: 2-10                           [64, 207, 32]             1,056
├─ModuleList: 1-4                             --                        --
│    └─STMetaSelfAttentionLayer: 2-11         [64, 12, 207, 32]         --
│    │    └─Sequential: 3-1                   [64, 207, 3072]           207,872
│    │    └─Sequential: 3-2                   [64, 207, 96]             14,432
│    │    └─STMetaAttentionLayer: 3-3         [64, 207, 12, 32]         1,056
│    │    └─Dropout: 3-4                      [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-5                    [64, 207, 12, 32]         64
│    │    └─Sequential: 3-6                   [64, 207, 12, 32]         4,192
│    │    └─Dropout: 3-7                      [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-8                    [64, 207, 12, 32]         64
├─ModuleList: 1-5                             --                        --
│    └─STMetaSelfAttentionLayer: 2-12         [64, 12, 207, 32]         --
│    │    └─Sequential: 3-9                   [64, 207, 3072]           207,872
│    │    └─Sequential: 3-10                  [64, 207, 96]             14,432
│    │    └─STMetaAttentionLayer: 3-11        [64, 207, 12, 32]         1,056
│    │    └─Dropout: 3-12                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-13                   [64, 207, 12, 32]         64
│    │    └─Sequential: 3-14                  [64, 207, 12, 32]         4,192
│    │    └─Dropout: 3-15                     [64, 207, 12, 32]         --
│    │    └─LayerNorm: 3-16                   [64, 207, 12, 32]         64
├─Linear: 1-6                                 [64, 32, 207, 12]         156
├─Linear: 1-7                                 [64, 12, 207, 1]          33
===============================================================================================
Total params: 473,917
Trainable params: 473,917
Non-trainable params: 0
Total mult-adds (M): 29.48
===============================================================================================
Input size (MB): 1.91
Forward/backward pass size (MB): 1290.04
Params size (MB): 1.84
Estimated Total Size (MB): 1293.79
===============================================================================================

Loss: MaskedMAELoss

2023-04-17 10:49:53.898446 Epoch 1  	Train Loss = 5.04598 Val Loss = 3.48594
2023-04-17 10:50:22.832512 Epoch 2  	Train Loss = 3.53096 Val Loss = 3.28509
2023-04-17 10:50:52.043828 Epoch 3  	Train Loss = 3.41344 Val Loss = 3.19859
2023-04-17 10:51:22.743343 Epoch 4  	Train Loss = 3.33817 Val Loss = 3.16124
2023-04-17 10:51:54.425834 Epoch 5  	Train Loss = 3.26771 Val Loss = 3.11328
2023-04-17 10:52:26.709401 Epoch 6  	Train Loss = 3.20642 Val Loss = 3.05894
2023-04-17 10:52:59.189132 Epoch 7  	Train Loss = 3.12545 Val Loss = 3.00434
2023-04-17 10:53:31.874668 Epoch 8  	Train Loss = 3.05503 Val Loss = 2.94146
2023-04-17 10:54:04.597298 Epoch 9  	Train Loss = 3.00345 Val Loss = 2.94130
2023-04-17 10:54:37.536343 Epoch 10  	Train Loss = 2.96501 Val Loss = 2.93587
2023-04-17 10:55:10.705232 Epoch 11  	Train Loss = 2.88887 Val Loss = 2.88610
2023-04-17 10:55:43.886091 Epoch 12  	Train Loss = 2.87494 Val Loss = 2.88590
2023-04-17 10:56:17.075639 Epoch 13  	Train Loss = 2.86556 Val Loss = 2.88434
2023-04-17 10:56:50.228853 Epoch 14  	Train Loss = 2.85921 Val Loss = 2.88132
2023-04-17 10:57:23.280835 Epoch 15  	Train Loss = 2.85236 Val Loss = 2.88401
2023-04-17 10:57:56.119192 Epoch 16  	Train Loss = 2.84455 Val Loss = 2.87956
2023-04-17 10:58:28.619892 Epoch 17  	Train Loss = 2.83842 Val Loss = 2.87548
2023-04-17 10:59:00.628363 Epoch 18  	Train Loss = 2.83133 Val Loss = 2.87855
2023-04-17 10:59:32.278305 Epoch 19  	Train Loss = 2.82591 Val Loss = 2.87236
2023-04-17 11:00:03.828799 Epoch 20  	Train Loss = 2.81975 Val Loss = 2.87012
2023-04-17 11:00:35.278147 Epoch 21  	Train Loss = 2.81378 Val Loss = 2.87995
2023-04-17 11:01:06.672994 Epoch 22  	Train Loss = 2.80814 Val Loss = 2.87034
2023-04-17 11:01:38.229881 Epoch 23  	Train Loss = 2.80283 Val Loss = 2.86788
2023-04-17 11:02:09.762213 Epoch 24  	Train Loss = 2.79822 Val Loss = 2.87068
2023-04-17 11:02:41.422503 Epoch 25  	Train Loss = 2.79211 Val Loss = 2.87227
2023-04-17 11:03:13.187996 Epoch 26  	Train Loss = 2.78670 Val Loss = 2.87183
2023-04-17 11:03:45.016234 Epoch 27  	Train Loss = 2.78040 Val Loss = 2.86465
2023-04-17 11:04:17.013528 Epoch 28  	Train Loss = 2.77662 Val Loss = 2.86272
2023-04-17 11:04:49.150835 Epoch 29  	Train Loss = 2.77055 Val Loss = 2.86828
2023-04-17 11:05:21.391619 Epoch 30  	Train Loss = 2.76551 Val Loss = 2.86939
2023-04-17 11:05:53.746367 Epoch 31  	Train Loss = 2.75427 Val Loss = 2.86424
2023-04-17 11:06:26.175820 Epoch 32  	Train Loss = 2.75282 Val Loss = 2.86545
2023-04-17 11:06:58.544176 Epoch 33  	Train Loss = 2.75202 Val Loss = 2.86542
2023-04-17 11:07:30.905198 Epoch 34  	Train Loss = 2.75125 Val Loss = 2.86575
2023-04-17 11:08:03.115181 Epoch 35  	Train Loss = 2.75031 Val Loss = 2.86450
2023-04-17 11:08:35.336452 Epoch 36  	Train Loss = 2.75002 Val Loss = 2.86281
2023-04-17 11:09:07.571730 Epoch 37  	Train Loss = 2.74952 Val Loss = 2.86798
2023-04-17 11:09:40.071605 Epoch 38  	Train Loss = 2.74888 Val Loss = 2.86506
Early stopping at epoch: 38
Best at epoch 28:
Train Loss = 2.77662
Train RMSE = 5.42362, MAE = 2.74752, MAPE = 7.25247
Val Loss = 2.86272
Val RMSE = 6.01326, MAE = 2.89692, MAPE = 8.20638
--------- Test ---------
All Steps RMSE = 6.36985, MAE = 3.13778, MAPE = 8.76500
Step 1 RMSE = 4.54918, MAE = 2.48868, MAPE = 6.31843
Step 2 RMSE = 5.11505, MAE = 2.68225, MAPE = 6.96767
Step 3 RMSE = 5.52297, MAE = 2.83888, MAPE = 7.54877
Step 4 RMSE = 5.83953, MAE = 2.96007, MAPE = 8.02733
Step 5 RMSE = 6.15113, MAE = 3.06740, MAPE = 8.46413
Step 6 RMSE = 6.43732, MAE = 3.16865, MAPE = 8.88289
Step 7 RMSE = 6.59578, MAE = 3.25067, MAPE = 9.22612
Step 8 RMSE = 6.78683, MAE = 3.31831, MAPE = 9.51158
Step 9 RMSE = 6.95267, MAE = 3.37637, MAPE = 9.72599
Step 10 RMSE = 7.10072, MAE = 3.43551, MAPE = 9.93538
Step 11 RMSE = 7.24818, MAE = 3.49486, MAPE = 10.16308
Step 12 RMSE = 7.43660, MAE = 3.57185, MAPE = 10.40877
Inference time: 2.68 s
