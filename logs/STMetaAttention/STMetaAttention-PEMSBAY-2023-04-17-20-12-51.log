PEMSBAY
Trainset:	x-(36465, 12, 325, 3)	y-(36465, 12, 325, 1)
Valset:  	x-(5209, 12, 325, 3)  	y-(5209, 12, 325, 1)
Testset:	x-(10419, 12, 325, 3)	y-(10419, 12, 325, 1)

--------- STMetaAttention ---------
{
    "num_nodes": 325,
    "in_steps": 12,
    "out_steps": 12,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0,
    "milestones": [
        10,
        30
    ],
    "clip_grad": false,
    "batch_size": 64,
    "max_epochs": 200,
    "use_cl": false,
    "pass_device": true,
    "model_args": {
        "num_nodes": 325,
        "node_emb_file": "../data/PEMSBAY/spatial_embeddings.npz",
        "in_steps": 12,
        "out_steps": 12,
        "input_dim": 1,
        "output_dim": 1,
        "model_dim": 32,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 7,
        "node_embedding_dim": 64,
        "z_dim": 32,
        "learner_hidden_dim": 64,
        "feed_forward_dim": 64,
        "num_heads": 4,
        "num_layers": 1,
        "dropout": 0,
        "with_spatial": true,
        "device": "cuda:0"
    }
}
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
STMetaAttention                               [64, 12, 325, 1]          20,800
├─Linear: 1-1                                 [64, 12, 325, 32]         64
├─Sequential: 1-2                             [64, 325, 32]             --
│    └─Linear: 2-1                            [64, 325, 32]             416
│    └─Tanh: 2-2                              [64, 325, 32]             --
│    └─Linear: 2-3                            [64, 325, 32]             1,056
│    └─Tanh: 2-4                              [64, 325, 32]             --
│    └─Linear: 2-5                            [64, 325, 32]             1,056
├─Sequential: 1-3                             [64, 325, 32]             --
│    └─Linear: 2-6                            [64, 325, 32]             416
│    └─Tanh: 2-7                              [64, 325, 32]             --
│    └─Linear: 2-8                            [64, 325, 32]             1,056
│    └─Tanh: 2-9                              [64, 325, 32]             --
│    └─Linear: 2-10                           [64, 325, 32]             1,056
├─ModuleList: 1-4                             --                        --
│    └─STMetaSelfAttentionLayer: 2-11         [64, 12, 325, 32]         --
│    │    └─Sequential: 3-1                   [64, 325, 3072]           207,872
│    │    └─Sequential: 3-2                   [64, 325, 96]             14,432
│    │    └─STMetaAttentionLayer: 3-3         [64, 325, 12, 32]         1,056
│    │    └─Dropout: 3-4                      [64, 325, 12, 32]         --
│    │    └─LayerNorm: 3-5                    [64, 325, 12, 32]         64
│    │    └─Sequential: 3-6                   [64, 325, 12, 32]         4,192
│    │    └─Dropout: 3-7                      [64, 325, 12, 32]         --
│    │    └─LayerNorm: 3-8                    [64, 325, 12, 32]         64
├─ModuleList: 1-5                             --                        --
│    └─STMetaSelfAttentionLayer: 2-12         [64, 12, 325, 32]         --
│    │    └─Sequential: 3-9                   [64, 325, 3072]           207,872
│    │    └─Sequential: 3-10                  [64, 325, 96]             14,432
│    │    └─STMetaAttentionLayer: 3-11        [64, 325, 12, 32]         1,056
│    │    └─Dropout: 3-12                     [64, 325, 12, 32]         --
│    │    └─LayerNorm: 3-13                   [64, 325, 12, 32]         64
│    │    └─Sequential: 3-14                  [64, 325, 12, 32]         4,192
│    │    └─Dropout: 3-15                     [64, 325, 12, 32]         --
│    │    └─LayerNorm: 3-16                   [64, 325, 12, 32]         64
├─Linear: 1-6                                 [64, 32, 325, 12]         156
├─Linear: 1-7                                 [64, 12, 325, 1]          33
===============================================================================================
Total params: 481,469
Trainable params: 481,469
Non-trainable params: 0
Total mult-adds (M): 29.48
===============================================================================================
Input size (MB): 3.00
Forward/backward pass size (MB): 2025.42
Params size (MB): 1.84
Estimated Total Size (MB): 2030.26
===============================================================================================

Loss: MaskedMAELoss

2023-04-17 20:16:14.596582 Epoch 1  	Train Loss = 2.76024 Val Loss = 2.18901
2023-04-17 20:17:35.395188 Epoch 2  	Train Loss = 1.91791 Val Loss = 2.06946
2023-04-17 20:18:57.633324 Epoch 3  	Train Loss = 1.81431 Val Loss = 1.94742
2023-04-17 20:20:20.738951 Epoch 4  	Train Loss = 1.71600 Val Loss = 1.84331
2023-04-17 20:21:44.932089 Epoch 5  	Train Loss = 1.65904 Val Loss = 1.78537
2023-04-17 20:23:09.886208 Epoch 6  	Train Loss = 1.61865 Val Loss = 1.76383
2023-04-17 20:24:35.057433 Epoch 7  	Train Loss = 1.58982 Val Loss = 1.76892
2023-04-17 20:26:00.170797 Epoch 8  	Train Loss = 1.56432 Val Loss = 1.71423
2023-04-17 20:27:25.144280 Epoch 9  	Train Loss = 1.54797 Val Loss = 1.70555
2023-04-17 20:28:50.206740 Epoch 10  	Train Loss = 1.53032 Val Loss = 1.69195
2023-04-17 20:30:15.532655 Epoch 11  	Train Loss = 1.48875 Val Loss = 1.66438
2023-04-17 20:31:41.254607 Epoch 12  	Train Loss = 1.48131 Val Loss = 1.66077
2023-04-17 20:33:05.389259 Epoch 13  	Train Loss = 1.47768 Val Loss = 1.66202
2023-04-17 20:34:29.092301 Epoch 14  	Train Loss = 1.47494 Val Loss = 1.65497
2023-04-17 20:35:53.872404 Epoch 15  	Train Loss = 1.47169 Val Loss = 1.65606
2023-04-17 20:37:18.866832 Epoch 16  	Train Loss = 1.46873 Val Loss = 1.65223
2023-04-17 20:38:44.048468 Epoch 17  	Train Loss = 1.46615 Val Loss = 1.65526
2023-04-17 20:40:09.333070 Epoch 18  	Train Loss = 1.46377 Val Loss = 1.65430
2023-04-17 20:41:34.436431 Epoch 19  	Train Loss = 1.46101 Val Loss = 1.65192
2023-04-17 20:42:59.780318 Epoch 20  	Train Loss = 1.45834 Val Loss = 1.65427
2023-04-17 20:44:26.015475 Epoch 21  	Train Loss = 1.45626 Val Loss = 1.64924
2023-04-17 20:45:51.373371 Epoch 22  	Train Loss = 1.45373 Val Loss = 1.64483
2023-04-17 20:47:15.309829 Epoch 23  	Train Loss = 1.45160 Val Loss = 1.64491
2023-04-17 20:48:38.096749 Epoch 24  	Train Loss = 1.44937 Val Loss = 1.64591
2023-04-17 20:50:01.843516 Epoch 25  	Train Loss = 1.44766 Val Loss = 1.64565
2023-04-17 20:51:26.400007 Epoch 26  	Train Loss = 1.44501 Val Loss = 1.64670
2023-04-17 20:52:51.409399 Epoch 27  	Train Loss = 1.44353 Val Loss = 1.65278
2023-04-17 20:54:16.503501 Epoch 28  	Train Loss = 1.44146 Val Loss = 1.64883
2023-04-17 20:55:41.502075 Epoch 29  	Train Loss = 1.43942 Val Loss = 1.64515
2023-04-17 20:57:06.473134 Epoch 30  	Train Loss = 1.43758 Val Loss = 1.64471
2023-04-17 20:58:31.597830 Epoch 31  	Train Loss = 1.43120 Val Loss = 1.63950
2023-04-17 20:59:56.674518 Epoch 32  	Train Loss = 1.43061 Val Loss = 1.63747
2023-04-17 21:01:21.097102 Epoch 33  	Train Loss = 1.43021 Val Loss = 1.64091
2023-04-17 21:02:43.938882 Epoch 34  	Train Loss = 1.42989 Val Loss = 1.64125
2023-04-17 21:04:07.528998 Epoch 35  	Train Loss = 1.42944 Val Loss = 1.63981
2023-04-17 21:05:31.895736 Epoch 36  	Train Loss = 1.42940 Val Loss = 1.64008
2023-04-17 21:06:56.854469 Epoch 37  	Train Loss = 1.42909 Val Loss = 1.63864
2023-04-17 21:08:22.018784 Epoch 38  	Train Loss = 1.42885 Val Loss = 1.63838
2023-04-17 21:09:47.060246 Epoch 39  	Train Loss = 1.42863 Val Loss = 1.64095
2023-04-17 21:11:12.028706 Epoch 40  	Train Loss = 1.42834 Val Loss = 1.64133
2023-04-17 21:12:37.145682 Epoch 41  	Train Loss = 1.42784 Val Loss = 1.63904
2023-04-17 21:14:02.336373 Epoch 42  	Train Loss = 1.42797 Val Loss = 1.63797
Early stopping at epoch: 42
Best at epoch 32:
Train Loss = 1.43061
Train RMSE = 3.12110, MAE = 1.42718, MAPE = 3.05626
Val Loss = 1.63747
Val RMSE = 3.74215, MAE = 1.62677, MAPE = 3.74035
--------- Test ---------
All Steps RMSE = 3.65604, MAE = 1.59962, MAPE = 3.62325
Step 1 RMSE = 1.75191, MAE = 0.93405, MAPE = 1.87487
Step 2 RMSE = 2.36448, MAE = 1.17493, MAPE = 2.43331
Step 3 RMSE = 2.86484, MAE = 1.35089, MAPE = 2.88348
Step 4 RMSE = 3.24559, MAE = 1.48248, MAPE = 3.24532
Step 5 RMSE = 3.52935, MAE = 1.58230, MAPE = 3.53672
Step 6 RMSE = 3.74426, MAE = 1.65965, MAPE = 3.76472
Step 7 RMSE = 3.91740, MAE = 1.72340, MAPE = 3.96286
Step 8 RMSE = 4.04307, MAE = 1.77254, MAPE = 4.11562
Step 9 RMSE = 4.15703, MAE = 1.81570, MAPE = 4.24620
Step 10 RMSE = 4.26677, MAE = 1.85606, MAPE = 4.35927
Step 11 RMSE = 4.37645, MAE = 1.89812, MAPE = 4.46915
Step 12 RMSE = 4.49337, MAE = 1.94529, MAPE = 4.58750
Inference time: 7.37 s
